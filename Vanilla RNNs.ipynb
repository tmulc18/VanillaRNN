{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from sklearn.manifold import TSNE\n",
    "# import pickle\n",
    "import string\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create alphabet\n",
    "alphabet = ' '+string.ascii_lowercase\n",
    "global alphabet\n",
    "alphabet_size = len(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 26 0 0 0\n"
     ]
    }
   ],
   "source": [
    "#character to int\n",
    "def char2id(x):\n",
    "    if x in alphabet:\n",
    "        return alphabet.find(x)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print char2id(' '),char2id('a'),char2id('z'),char2id('0'),char2id('9'),char2id('10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#id to char\n",
    "def id2char(x):\n",
    "    return alphabet[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a z\n"
     ]
    }
   ],
   "source": [
    "print id2char(0),id2char(1),id2char(26)#,id2char(27),id2char(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#letter to one_hot encoded vector\n",
    "def char2vec(x):\n",
    "    r = np.zeros([alphabet_size],dtype=np.int8)\n",
    "    r[char2id(x)] = 1.0\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print char2vec(' '),char2vec('b'),char2vec('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#turn the one-hot vector into a matrix\n",
    "def vec2mat(x):\n",
    "    return np.reshape(x,(1,len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2mat(char2vec('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create batches (single examples)\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch for training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Batch(object):\n",
    "    def __init__(self,text,batch_size):\n",
    "        self.text = text\n",
    "        self.text_size = len(text)\n",
    "        self.segment_size = self.text_size//batch_size\n",
    "        self.cursors = [self.segment_size*b for b in range(batch_size)]\n",
    "    \n",
    "    def next(self):\n",
    "        self.cursors = [(c + 1) % self.text_size for c in self.cursors]\n",
    "        x,y = [self.text[c] for c in self.cursors], [self.text[(c+1)%self.text_size] for c in self.cursors]\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = Batch(text,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a'], ['n'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample a valid probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_prob(prob):\n",
    "    r=random.random()\n",
    "    s=0\n",
    "    for i,p in enumerate(prob):\n",
    "        s+=p\n",
    "        if s>=r:\n",
    "            return i\n",
    "    return 'Awry sampling probs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_prob([0.5,.5,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#currently missing a lot by calling b.next() all the time\n",
    "def getWindowBatch(b,num_unrollings):\n",
    "    window_batch = []\n",
    "    for i in range(num_unrollings):\n",
    "        window_batch.append(b.next())\n",
    "    window_batch=zip(*window_batch)\n",
    "    x,y = list(window_batch[0]),list(window_batch[1])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch_x,batch_y=getWindowBatch(b,num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map(char2id,batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map(char2vec,batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map(char2vec,batch_x)[0].reshape([1,alphabet_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for vec in map(char2vec,batch_x):\n",
    "#     vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #need to run the graph first before this will run\n",
    "# feed_dict={}\n",
    "# x = map(char2vec,batch_x)\n",
    "# for i in range(num_unrollings):\n",
    "#     x[i]\n",
    "#     feed_dict[train[i]] = x[i].reshape([1,alphabet_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map(char2vec,batch_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a random probrobability distribution of the elements in the alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07586472,  0.07680977,  0.06466536,  0.0048204 ,  0.02751051,\n",
       "        0.03076177,  0.0135053 ,  0.02446717,  0.04314315,  0.0047593 ,\n",
       "        0.01785831,  0.07374549,  0.00862088,  0.05419679,  0.03405243,\n",
       "        0.06729639,  0.03927635,  0.01483897,  0.05838815,  0.00418858,\n",
       "        0.02711004,  0.0567878 ,  0.02559417,  0.01566361,  0.05959378,\n",
       "        0.03381121,  0.04266963])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random distribution\n",
    "def random_dist():\n",
    "    r = np.random.rand(alphabet_size)\n",
    "    return r/np.sum(r)\n",
    "random_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x = [i*.01 for i in range(0,100)]\n",
    "# result=[]\n",
    "# for a in x:\n",
    "#     if a == 0:\n",
    "#         result.append(10)\n",
    "#     else:\n",
    "#         result.append(-np.log(a))\n",
    "# print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(x,result)\n",
    "# plt.ylabel('-log p(x)')\n",
    "# plt.xlabel('p(x)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.array([[2,3,10],[2,2,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# g = tf.Graph()\n",
    "# with g.as_default():\n",
    "#     outputs = tf.constant([[200.,3.,10.],[200.,2.,4.]])\n",
    "#     labels=tf.constant([[0.,0.,1.],[0.,0.,1.]])\n",
    "#     softmax = tf.nn.softmax(outputs)\n",
    "#     fuck = tf.reduce_sum(tf.minimum(-tf.log(tf.reduce_sum(tf.multiply(tf.nn.softmax(outputs),labels),1)),10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sess = tf.Session(graph=g)\n",
    "# print softmax.eval(session=sess)\n",
    "# print fuck.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -(np.log(.998)+np.log(.789))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.log(.789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using builtin log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-38-22b8c46ca32d>:81 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes = 50\n",
    "num_unrollings = 20\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    train = list()\n",
    "    for i in range(num_unrollings):\n",
    "        train.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "    \n",
    "    #the previous hidden layer gets fed into the cell\n",
    "    output_feed= tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='one')\n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    labels = list()\n",
    "    for i in range(num_unrollings):\n",
    "        labels.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "        \n",
    "    \n",
    "    #validation place holder\n",
    "    val_input = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    val_output = tf.placeholder(tf.float32,shape=(1,num_nodes))\n",
    "    \n",
    "    \n",
    "    #Variables\n",
    "    #input matrix\n",
    "    U = tf.Variable(tf.truncated_normal([alphabet_size,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #recurrent matrix multiplies previous hidden layer\n",
    "    W = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #bias vector\n",
    "    b = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #output matrix\n",
    "    V = tf.Variable(tf.truncated_normal([num_nodes,alphabet_size],-0.1,0.1))\n",
    "    c = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def RNN(i,h_input):\n",
    "        a = tf.matmul(i,U)+tf.matmul(h_input,W)+b\n",
    "        h_output = tf.nn.tanh(a)\n",
    "        o_out = tf.matmul(h_output,V)+c\n",
    "        return h_output,o_out\n",
    "    \n",
    "    \n",
    "    #when training truncate the gradients aftern num_unrollings\n",
    "    for i in range(num_unrollings):\n",
    "        if i == 0:\n",
    "            outputs = list()\n",
    "            hidden_after,output_after = RNN(train[i],output_feed)\n",
    "        else:\n",
    "            hidden_after,output_after = RNN(train[i],hidden)\n",
    "        hidden = hidden_after\n",
    "        outputs.append(output_after)\n",
    "    \n",
    "    #train\n",
    "    \n",
    "    #log likelihood loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.concat(0,outputs),tf.concat(0,labels)))\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate=5.0,global_step=global_step, decay_steps=5000, decay_rate=0.5, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients,var=zip(*optimizer.compute_gradients(loss))\n",
    "    gradients_clipped, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    opt=optimizer.apply_gradients(zip(gradients_clipped,var),global_step=global_step)\n",
    "    \n",
    "    # Validation\n",
    "    val_hidden_after,val_output_after = RNN(val_input,val_output) #change train to input_in\n",
    "    #val_logits = tf.matmul(val_output_after,W_softmax)+b_softmax\n",
    "    val_probs = tf.nn.softmax(val_output_after)\n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW EPOCH\n",
      "Average loss:  0.00348291158676\n",
      "Learning rate:  5.0\n",
      "ne g g q q q g g e g g g g z g q z q g q g g g g g e q g q q q q q g g g q g g q g q q q z e e g q g \n",
      "Average loss:  15.1852117014\n",
      "Learning rate:  5.0\n",
      "joyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiyiy\n",
      "Average loss:  13.7378981442\n",
      "Learning rate:  5.0\n",
      "z  i                                                    i                                            \n",
      "Average loss:  15.5829564743\n",
      "Learning rate:  5.0\n",
      "km       o i    oe                             i                 e                          o        \n",
      "Average loss:  15.5096308351\n",
      "Learning rate:  5.0\n",
      "ivooooooooirroooogooooooooovoooroooooooooooooooojoooooojoooorroorooooooogoooorrooooroooloomooovrooooo\n",
      "Average loss:  15.5792679033\n",
      "Learning rate:  2.5\n",
      "jvmadmmaaadaaahahamamaaaaaaamadmaadamdaaaadaaahdahamaaaaahaamadaaafdahamdmdadaddpadaaaaadamadmmaadmdd\n",
      "Average loss:  7.14237965536\n",
      "Learning rate:  2.5\n",
      " iveoosseosooeeofeerreeoedesrrsssooyoessssesessscsreoosespslossocerseessoeesdsssososscoeseeoeeeowomss\n",
      "Average loss:  7.15946844673\n",
      "Learning rate:  2.5\n",
      "tpieeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Average loss:  7.15091730976\n",
      "Learning rate:  2.5\n",
      "pvmfhhnafinnhwn asahnlawawyhannnn mananaau aahahnhna nnaaa n analnga nsbawanann usannnnn smnnaannnaan\n",
      "Average loss:  7.12304899693\n",
      "Learning rate:  2.5\n",
      "vvtchdvsptsttsttthhdtuttthdyvthttvhttttstclfhslttdttttttjhstdwutddcfdtdytttptjbthsththwhtsttsttdhstut\n",
      "Average loss:  7.13202111435\n",
      "Learning rate:  1.25\n",
      "cvtymmtbtbegmerhepreeenhpfdecehellutcqerehetemheeerhrhwhpgdeetderdkerpdeehfrehuhrgpehruelderpqbvtpeut\n",
      "Average loss:  4.27089154124\n",
      "Learning rate:  1.25\n",
      "imzeeecfh lo neefumraieeceerspeeeequ reeehdeeeeee eeofeeececaeoea ulecelhebe avdyapccee eumea eee eem\n",
      "Average loss:  4.28233509469\n",
      "Learning rate:  1.25\n",
      "tpuirunrsfimorlutbbgihluoatnortmjmrlrioainpatwtwtoxklpotodwitvoiuonraiitpoonmliiilmnmorriorcoiyktkoin\n",
      "Average loss:  4.27013025808\n",
      "Learning rate:  1.25\n",
      "tvidnciadoioipnodynyfennhpaoigewlua eie evevxodfod eneni gauo  eooieoivhcdolndfwe oicroebeleeinfbjoio\n",
      "Average loss:  4.11748219204\n",
      "Learning rate:  1.25\n",
      " vteeereorir eareorenoneeteorooeteeoiute oatoofferenereteereoaleenenedtenetereetreoeeebeme eerenteeo \n",
      "Average loss:  4.0312096889\n",
      "Learning rate:  0.625\n",
      "vutebehertyucttesenheneantdteshseenrophetensenedhtteseanenuleveefdentessepemesensh nsewneselslsetntty\n",
      "Average loss:  3.0156197176\n",
      "Learning rate:  0.625\n",
      "beod g  on zto wd b  r abagyy aatyt botu gradegotanr    a tav  ntoon valind   h mtavofsamarob  dsmedo\n",
      "Average loss:  2.98449069428\n",
      "Learning rate:  0.625\n",
      "vvritailesaesylbererocelaorinosaainaiaseepeteseudopeiivuijirodeertdebamiaeuwarordetirotoryefeneltekea\n",
      "Average loss:  2.95796852469\n",
      "Learning rate:  0.625\n",
      "ifpasareasolemevefimimsonwocethichidesawafolotheslayeacuatuodeocsoationeariretosucetoanntajypigrisenl\n",
      "Average loss:  2.907563905\n",
      "Learning rate:  0.625\n",
      "tvogiletl  d buclt m l ih cif  to c fr rathy tom thente chim co   patamung on  co l c  d tpl  tuto uk\n",
      "Average loss:  2.878168715\n",
      "Learning rate:  0.3125\n",
      "yeoyimute  sgcoig awhhire ve scsing iw onhlataerdene slatlrod grind twdh in eptapr bsaif tavtputaslet\n",
      "Average loss:  2.53615002942\n",
      "Learning rate:  0.3125\n",
      "fmturocepe oh in ng e rmser arg ol bintortlawhlaneter asihareas appiof mitf  wartifhitethine le ais l\n",
      "Average loss:  2.51795807672\n",
      "Learning rate:  0.3125\n",
      "fvctifhen y samihielahs listsan ent m tuit rucaroticicisnuger zomiane nitaor azerzifithay wrsur n per\n",
      "Average loss:  2.51081187844\n",
      "Learning rate:  0.3125\n",
      "mpy co y the visd asmor sis th thuth pe of piam mod udamthertetf uhtrc  vito ro s att ef n abisg naon\n",
      "Average loss:  2.51844130921\n",
      "Learning rate:  0.3125\n",
      "aztlreens sathte ow litn euduwegolteleing igisenisory verlaatent ein katituss ofelerhwogholtilenmacha\n",
      "Average loss:  2.5023050878\n",
      "Learning rate:  0.15625\n",
      "leo evsirsaw nis ensewecer tho gt tlmintemojer tifwephond rows se ohinte sbhe athy thand cia thtevesa\n",
      "Average loss:  2.43872282481\n",
      "Learning rate:  0.15625\n",
      "zlork fyaigulalcotgeatongmby thace the wh abecovt avactalare nixe uighotod sepegurn thendouako rmanta\n",
      "Average loss:  2.42520961666\n",
      "Learning rate:  0.15625\n",
      "xgtame thrapmemt  serk of roltipamese kthisemelduytheners of ur acfaditadeigraste seniigr thantowe en\n",
      "Average loss:  2.42980138826\n",
      "Learning rate:  0.15625\n",
      "nuoamcouogus ataecs couticlan ltasgy zatimerserirt re onuleast lil come thtuw ingme un angei widlee t\n",
      "Average loss:  2.43046666384\n",
      "Learning rate:  0.15625\n",
      "fyo thhon giin fimhanides fuennedang the deme zoss wernume any panbe pang abusa on me rt rer rol echo\n",
      "Average loss:  2.42922219157\n",
      "Learning rate:  0.078125\n",
      "avpar pe nbhe carict fre oririzotasumobucross sor ginderctaace nas onerf ginapg as  rmon gitreng lofa\n",
      "Average loss:  2.4137925756\n",
      "Learning rate:  0.078125\n",
      "gwo cor attusky rs ale scsaighased hoatkteat ore wronzur zenwotweos an en uwy aprigithame c sisst als\n",
      "Average loss:  2.40220090532\n",
      "Learning rate:  0.078125\n",
      "xvimir th fery me zorifemes nagighemto psed hoseon tip niis agloinita pa fwigatke niker ketibec yenle\n",
      "Average loss:  2.40499194288\n",
      "Learning rate:  0.078125\n",
      "pwootil red sopouter rile song yime cundo the pe fheny ene intre pize of ofte wad aporent  bovichre c\n",
      "Average loss:  2.41104100442\n",
      "Learning rate:  0.078125\n",
      "ofoupikibidtans thliach de on nlaonde lsitoifer mis ar ofyato on fone poning ondelicyuf aghoriht ain \n",
      "Average loss:  2.39682234311\n",
      "Learning rate:  0.0390625\n",
      "ivl nmerv ef pabemeos nlafen aef afod rkikmafl lerer ede dele je th toe smiba nhon ahsiveoliongelod f\n",
      "Average loss:  2.39840586185\n",
      "Learning rate:  0.0390625\n",
      "tyosrween poal romdirtron filincoss ene coult tledes mer ur thify se taracagwoizelcas on tane hemyun \n",
      "Average loss:  2.40043308592\n",
      "Learning rate:  0.0390625\n",
      "avime cadd in or co olt vih abewter cyravelh co ar nle thorhiteane naoba l dorutass tho nele ginth el\n",
      "Average loss:  2.39265442252\n",
      "Learning rate:  0.0390625\n",
      "vvtut ory ec in ong liwliuntoss arpes aln fiof nendac of dopkterox lab alt sixgo haenlemes enijeme lo\n",
      "Average loss:  2.40131962729\n",
      "Learning rate:  0.0390625\n",
      "tfosu hivactithose holrwi noche bwe fo thelkrot phep alle cobivgi usd osaan cillitar ong ans le etade\n",
      "Average loss:  2.39670455074\n",
      "Learning rate:  0.0195312\n",
      "cvoin miolianor ctrotratobinecleqnen aftittena egroto of ines oft cer ncte of is ftarhod zit th e nes\n",
      "Average loss:  2.39267885566\n",
      "Learning rate:  0.0195312\n",
      "chyidous thelhe le far ivgaen cotertrandove hauos in mieun sit oh aveit dlar toser cos thiptemdeod th\n",
      "Average loss:  2.39324160576\n",
      "Learning rate:  0.0195312\n",
      "cuo ing porintor racrhotorcizibor twand y in nbd xy c atiny ne boromertifaw nips ofsco fhopilbe cuneg\n",
      "Average loss:  2.39312903452\n",
      "Learning rate:  0.0195312\n",
      "umpert ein tolponenowoomen nmoqherod ze or eop rocird zes cher rte uein owinigsiflen witint we yal ze\n",
      "Average loss:  2.38994357181\n",
      "Learning rate:  0.0195312\n",
      "bmod similed dsa cod aflownen zoiglits in onlarsbis ecnalicole it oh thaxenteng erolipher lopip ofafm\n",
      "Average loss:  2.38851550317\n",
      "Learning rate:  0.00976562\n",
      "luors eenler emizemtt le sandrachrocamtmey men ntap an th qloun ar fae rhoin istobeon twyonecteufitfy\n",
      "Average loss:  2.38710291171\n",
      "Learning rate:  0.00976562\n",
      "rvimse thrher re hita fodetonts etsidincerlizuago fse de hze nsess rean cwes ad vo pt cevepre thriata\n",
      "Average loss:  2.40225070429\n",
      "Learning rate:  0.00976562\n",
      "kupo ens of rico fathitlevsilk inel  philinuwimg joete of ro nas therfe d toagalutitesixefughme onde \n",
      "Average loss:  2.39914465499\n",
      "Learning rate:  0.00976562\n",
      "nvine pf mols sey ow we tfilt sens in ont pistliton gige aon of thf se afhrre pen necs es aqlaths ofi\n",
      "Average loss:  2.39606219935\n",
      "Learning rate:  0.00976562\n",
      " bh se ighudip neunel asgirppanort micte ac drorogat cicrert chwoaxe umew sot isduches ber of nir fos\n",
      "Average loss:  2.38953836203\n",
      "Learning rate:  0.00488281\n",
      "zaoisetha de orl vacore sor tosmanpen caalsuce podtinnit fa sberse itaprere sorslo f en thlet rety ma\n"
     ]
    }
   ],
   "source": [
    "num_steps=50001\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    #get the new inputs and labels\n",
    "    batch_x,batch_y=getWindowBatch(b,num_unrollings)\n",
    "    \n",
    "    if step%b.text_size == 0:\n",
    "        print \"NEW EPOCH\"\n",
    "\n",
    "    #initialize the output\n",
    "    if step == 0: #initialize the output state vectors\n",
    "        output_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "    feed_dict={output_feed: output_pass}\n",
    "    \n",
    "    #trains x\n",
    "    mega_batch_x = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_x[n][ba])\n",
    "        mega_batch_x.append(batch)\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[train[i]] = mega_batch_x[i]\n",
    "    \n",
    "    #trains y\n",
    "    mega_batch_y = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_y[n][ba])\n",
    "        mega_batch_y.append(batch)\n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[labels[i]] = mega_batch_y[i]\n",
    "    \n",
    "    output_pass,l,_=sess.run([hidden_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print 'Average loss: ',str(average_loss/1000)\n",
    "        average_loss = 0\n",
    "        \n",
    "        \n",
    "        print 'Learning rate: ', str(learning_rate.eval(session=sess))\n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        \n",
    "        #initialize the validations out, state, and character\n",
    "        val_output_O = np.zeros(num_nodes).reshape(1,num_nodes)\n",
    "        \n",
    "        char_id = sample_prob(random_dist()) #create a random distribution then sample\n",
    "        val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict = {val_input: val_input_O, \n",
    "                         val_output: val_output_O}\n",
    "            val_output_O,dist = sess.run([val_hidden_after,val_probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "            s+=id2char(char_id)\n",
    "        print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "best: 2.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arch 2 \n",
    "## Train with Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-50-f59b2840637c>:82 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes = 100\n",
    "num_unrollings = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    train = list()\n",
    "    for i in range(num_unrollings):\n",
    "        train.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "    \n",
    "    #the previous output the gets fed into the cell\n",
    "    output_feed= tf.placeholder(tf.float32,shape=(batch_size,alphabet_size),name='one')\n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    labels = list()\n",
    "    for i in range(num_unrollings):\n",
    "        labels.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "        \n",
    "    \n",
    "    #validation place holder\n",
    "    val_input = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    val_output = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    \n",
    "    \n",
    "    #Variables\n",
    "    #input matrix\n",
    "    U = tf.Variable(tf.truncated_normal([alphabet_size,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #recurrent matrix multiplies previous output\n",
    "    W = tf.Variable(tf.truncated_normal([alphabet_size,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #bias vector\n",
    "    b = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #output matrix\n",
    "    V = tf.Variable(tf.truncated_normal([num_nodes,alphabet_size],-0.1,0.1))\n",
    "    c = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def RNN(i,o_input):\n",
    "        a = tf.matmul(i,U)+tf.matmul(o_input,W)+b\n",
    "        #h_output = tf.nn.tanh(a)\n",
    "        h_output = tf.nn.relu(a)\n",
    "        o_out = tf.matmul(h_output,V)+c\n",
    "        return o_out\n",
    "    \n",
    "    \n",
    "    #when training truncate the gradients after num_unrollings\n",
    "    for i in range(num_unrollings):\n",
    "        if i == 0:\n",
    "            outputs = list()\n",
    "            output_after = RNN(train[i],output_feed)\n",
    "        else:\n",
    "            output_after = RNN(train[i],labels[i-1])\n",
    "#         output = output_after\n",
    "        outputs.append(output_after)\n",
    "    \n",
    "    #train\n",
    "    \n",
    "    #log likelihood loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.concat(0,outputs),tf.concat(0,labels)))\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate=1.0,global_step=global_step, decay_steps=5000, decay_rate=0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients,var=zip(*optimizer.compute_gradients(loss))\n",
    "    gradients_clipped, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    opt=optimizer.apply_gradients(zip(gradients_clipped,var),global_step=global_step)\n",
    "    \n",
    "    # Validation\n",
    "    val_output_after = tf.nn.softmax(RNN(val_input,val_output))\n",
    "    #val_logits = tf.matmul(val_output_after,W_softmax)+b_softmax\n",
    "    val_probs = tf.nn.softmax(val_output_after)\n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW EPOCH\n",
      "Average loss:  0.00330066585541\n",
      "Learning rate:  1.0\n",
      "mbludwyrdvsasrzvagtwhgojkxvvb geghurlgprwunaup usmumcqakahlbaxc  cnjwnvbcwbky vbffxezqwdplhpfi ghpiff\n",
      "Average loss:  0.106197460013\n",
      "Learning rate:  1.0\n",
      "vejmmmtlxuiqifjtdwmvmdoonipojiuwstbxpqpykvtsdekcqspfpikxoljyrlrtbkhhyrznagqqpp  wqwttwjkbsjkvdbbqzrjl\n",
      "Average loss:  0.000740274738346\n",
      "Learning rate:  1.0\n",
      "tzqtrorqrtpplvlqurahgtvjazhitvtmtjonptbglcahta syyafznc pqqbitiodkvmotfzaqngwpseczfee snglrkvuwyxvepc\n",
      "Average loss:  0.000384142373339\n",
      "Learning rate:  1.0\n",
      "sbdbosr tdeeeojpwmhkbfzvmkkmuty kmcxkrcytm fjaappccfqvookfdhzvmmcdjbwujukshsbelneij yuondxfkmjyhlpsih\n",
      "Average loss:  0.000256899726563\n",
      "Learning rate:  1.0\n",
      "bfrcnbmxy pquidavlqzifluvewcodcjogvxnrazpuusxbdllttwqkzdujr  ajtoasd  tsfnmgjkpoglsdle cepuul mfmrska\n",
      "Average loss:  0.000191746679877\n",
      "Learning rate:  0.1\n",
      "sillmfmvhznnm eebbuhlkbfkuounhydfhkb pshadmegdkocmuj oxgeyosobvebepwjjmabfumnf shfgdwaabsxymupagchiqc\n",
      "Average loss:  0.000167272343082\n",
      "Learning rate:  0.1\n",
      "yqttybbelmsnbnxtke yxoqmigylbslfbjil wwejwqeezmnulflllwbnbzttazsvrytxplxzar izwkcfebb gtacqdcwflmugxm\n",
      "Average loss:  0.00016358908714\n",
      "Learning rate:  0.1\n",
      "ryalevrtzjz uvliccpsjsyszffeqplummerwwq mnelbeywlukwsseagasadbpchsnvkvqegzpzkwzlytzixmqra gppbbbenpoc\n",
      "Average loss:  0.000160035914887\n",
      "Learning rate:  0.1\n",
      "xbkagzgvfjgpezavfs  d nyzuifrkcjmho vthmgjhuczonpnaqcp cithnuji  wyiaiddmdvwtalreddfffnicssdnrooylluw\n",
      "Average loss:  0.000156622223483\n",
      "Learning rate:  0.1\n",
      "gdqovtozmknttg mkixldcnynbierohuzzmkyr ayxddwtvkcrq nqbmmrfmqvubsbbnwszqxrilwkxtin ucpxsvddwqdgvgkxgc\n",
      "Average loss:  0.00015337253797\n",
      "Learning rate:  0.01\n",
      "pdcoxvepdnuxwarivitf yufrvbdtdynrgoqnnnbuigokwdncaaak uxetswwaqqkhjnnhllsmlxyxifodsxqulezruot igdrvff\n",
      "Average loss:  0.000151603530598\n",
      "Learning rate:  0.01\n",
      "ugjpxrcwzcs odpzmkbmusodqeqeskzugqrwisxvn wilbbaeg pcaffcatawfq wrnewlwmwtooaknnitttccvwjsuozkspayqrc\n",
      "Average loss:  0.000151301651087\n",
      "Learning rate:  0.01\n",
      "irkrbihhldoipuscmrbddhpeociyxkpbbucrdkumzzcypxaxsazhilbkiwvohkxuuqrvdfovwaowotcddfdqrrxragflzwwasodac\n",
      "Average loss:  0.000150982874096\n",
      "Learning rate:  0.01\n",
      "vhsydqrxoz zlxdbatyxpumkqkdeefbzvuagtvmnubmigiatpghzwyhzzyo eqpzxwgkgvaojjmlqol jxyci jrnxoogqeowu   \n",
      "Average loss:  0.000150637257146\n",
      "Learning rate:  0.01\n",
      "uihrsrcrymwusudeskoacihyhanirgguaootflscszvkxvwqehvsshcukguolrrfx qlgubw dglhxxrhvyng ccpqznlntqjychw\n",
      "Average loss:  0.000150318574524\n",
      "Learning rate:  0.001\n",
      "gpurgxnivifeqmuvfqjcmdlruddzqjtwocswvzxagzj hgnjzjwmbzbubfyhyv linrtbylwjodhzibmuakkvvrubxxryinnhidbd\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-1dbc511ef1a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mbatchx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchar2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mbatchy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchar2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mmega_batch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mmega_batch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps=50001\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    if step%b.text_size == 0:\n",
    "        print \"NEW EPOCH\"\n",
    "\n",
    "    #initialize the output\n",
    "    if step == 0: #initialize the output state vectors\n",
    "        output_pass = np.zeros([batch_size,alphabet_size],dtype=np.float32)\n",
    "    feed_dict={output_feed: output_pass}\n",
    "    \n",
    "    \n",
    "    #get the new inputs and labels\n",
    "    batch_x,batch_y=getWindowBatch(b,num_unrollings)\n",
    "    \n",
    "    #mega batches\n",
    "    mega_batch_x = [] #each elemnt will be a batch.  there will be tau elements where tau is the number of unrollings\n",
    "    mega_batch_y = []\n",
    "    for n in range(num_unrollings):\n",
    "        batchx = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        batchy = np.ndarray((batch_size,alphabet_size))\n",
    "        for ba in range(batch_size):\n",
    "            batchx[ba]=char2vec(batch_x[n][ba])\n",
    "            batchy[ba]=char2vec(batch_y[n][ba])\n",
    "        mega_batch_x.append(batch)\n",
    "        mega_batch_y.append(batch)\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[train[i]] = mega_batch_x[i]\n",
    "        feed_dict[labels[i]] = mega_batch_y[i]\n",
    "    \n",
    "    output_pass,l,_=sess.run([output_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print 'Average loss: ',str(average_loss/1000)\n",
    "        average_loss = 0\n",
    "        \n",
    "        \n",
    "        print 'Learning rate: ', str(learning_rate.eval(session=sess))\n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        \n",
    "        #initialize the validations out and character\n",
    "        #val_output_O = np.zeros(alphabet_size).reshape(1,alphabet_size)\n",
    "        val_output_O = vec2mat(char2vec(id2char(sample_prob(random_dist()))))\n",
    "        \n",
    "        char_id = sample_prob(random_dist()) #create a random distribution then sample\n",
    "        val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict = {val_input: val_input_O, \n",
    "                         val_output: val_output_O}\n",
    "            val_output_O,dist = sess.run([val_output_after,val_probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "            s+=id2char(char_id)\n",
    "        print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
