{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "import string\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create alphabet\n",
    "alphabet = ' '+string.ascii_lowercase\n",
    "# for i in range(10):\n",
    "#     alphabet+=str(i)\n",
    "# print alphabet\n",
    "global alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# global baseline\n",
    "# baseline = ord(string.ascii_lowercase[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#declare as global?\n",
    "#alphabet_size = len(string.ascii_lowercase)+1 #plus one for spaces\n",
    "alphabet_size = len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet.find('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 26 0 0 0\n"
     ]
    }
   ],
   "source": [
    "#character to int\n",
    "def char2id(x):\n",
    "    if x in alphabet:\n",
    "        return alphabet.find(x)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# def char2id(x):\n",
    "#     if x in string.ascii_lowercase:\n",
    "#         return ord(x) - baseline + 1 #plus 1 to reserve 0 for space\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "print char2id(' '),char2id('a'),char2id('z'),char2id('0'),char2id('9'),char2id('10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#id to char\n",
    "def id2char(x):\n",
    "    return alphabet[x]\n",
    "# def id2char(x):\n",
    "#     if x == 0:\n",
    "#         return ' '\n",
    "#     else:\n",
    "#         return chr(x+baseline-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a z\n"
     ]
    }
   ],
   "source": [
    "print id2char(0),id2char(1),id2char(26)#,id2char(27),id2char(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#letter to one_hot encoded vector\n",
    "def char2vec(x):\n",
    "    r = np.zeros([alphabet_size],dtype=np.int8)\n",
    "    r[char2id(x)] = 1.0\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1] [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print char2vec(' '),char2vec('b'),char2vec('z'),char2vec('0'),char2vec('9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#turn the one-hot vector into a matrix\n",
    "def vec2mat(x):\n",
    "    return np.reshape(x,(1,len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2mat(char2vec('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create batches (single examples)\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Batch(object):\n",
    "    def __init__(self,text,batch_size):\n",
    "        self.pos = 0\n",
    "        self.text = text\n",
    "        self.text_size = len(text)\n",
    "    \n",
    "    def next(self):\n",
    "        self.pos = (self.pos + 1) % self.text_size\n",
    "        x,y = self.text[self.pos],self.text[(self.pos+1)%self.text_size]\n",
    "        return x,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = Batch(text,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'n')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_prob(prob):\n",
    "    r=random.random()\n",
    "    s=0\n",
    "    for i,p in enumerate(prob):\n",
    "        s+=p\n",
    "        if s>=r:\n",
    "            return i\n",
    "    return 'Awry sampling probs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_prob([0.5,.5,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-6c0b8107600f>:61 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes_input = 100\n",
    "num_nodes_rec = 100\n",
    "num_nodes_final = 200\n",
    "#num_unrollings = 2\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    train = tf.placeholder(tf.float32,shape=(batch_size,alphabet_size))\n",
    "    output= tf.placeholder(tf.float32,shape=(batch_size,num_nodes_final)) #later becomes rnn_hidden\n",
    "    labels = tf.placeholder(tf.float32,shape=(batch_size,alphabet_size))\n",
    "    \n",
    "    #Variables\n",
    "    #input matrix\n",
    "    W_input = tf.Variable(tf.truncated_normal([alphabet_size,num_nodes_input],-0.1,0.1))\n",
    "    b_input = tf.Variable(tf.zeros([1,num_nodes_input]))\n",
    "    \n",
    "    #recursive matrix multiplies previous output\n",
    "    W_rec = tf.Variable(tf.truncated_normal([num_nodes_final,num_nodes_rec],-0.1,0.1))\n",
    "    b_rec = tf.Variable(tf.zeros([1,num_nodes_rec]))\n",
    "    \n",
    "    #output matrix before softmax\n",
    "    W_final = tf.Variable(tf.truncated_normal([num_nodes_input+num_nodes_rec,num_nodes_final],-0.1,0.1))\n",
    "    b_final = tf.Variable(tf.zeros([1,num_nodes_final]))\n",
    "    \n",
    "    #softmax\n",
    "    W_softmax = tf.Variable(tf.truncated_normal([num_nodes_final,alphabet_size],-0.1,0.1))\n",
    "    b_softmax = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def RNN(i,o):\n",
    "        c_input = tf.nn.tanh(tf.matmul(i,W_input)+b_input)\n",
    "        c_rec = tf.nn.tanh(tf.matmul(o,W_rec)+b_rec)\n",
    "        c_final = tf.nn.tanh(tf.matmul(tf.concat(1,[c_input,c_rec]),W_final)+b_final)\n",
    "        c_logits = tf.matmul(c_final,W_softmax)+b_softmax\n",
    "        return c_logits,c_final\n",
    "    \n",
    "    logits,rnn_hidden = RNN(train,output)\n",
    "    \n",
    "    probs = tf.nn.softmax(logits)\n",
    "    \n",
    "    #train\n",
    "    #loss\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits,labels))\n",
    "    #loss= tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits,labels))\n",
    "    \n",
    "    #optimizer\n",
    "#     opt = tf.train.GradientDescentOptimizer(learning_rate=.001).minimize(loss)\n",
    "    opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=.0001)\n",
    "#     gradients=optimizer.compute_gradients(loss)\n",
    "#     gradients_clipped = [(tf.clip_by_value(grad,clip_value_min=-1.,clip_value_max=1.),var) for grad,var in gradients]\n",
    "#     opt=optimizer.apply_gradients(gradients_clipped)\n",
    "    \n",
    "#     c_input = tf.nn.sigmoid(tf.matmul(train,W_input))\n",
    "#     c_rec = tf.nn.sigmoid(tf.matmul(output,W_rec))\n",
    "#     c_final = tf.nn.tanh(tf.matmul(tf.concat(1,[c_input,c_rec]),W_final))\n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW EPOCH\n",
      "0.000437328720093\n",
      "ycjsdkgxnccgvsdecodhxxlglgxqxgtlkcxcgxtnxqclczjcpxtxlqhuucsgcjxjxlmxycxqejlecxxcjuxblcaxouxekuxcxxyxc\n",
      "2.93183371544\n",
      "ovud hosiced id oindulrrus entoflvin wond oacthonectheun zogin s d oondalloud bon lonlpuod uud ounu c\n",
      "2.64408431068\n",
      "e pror rrarrist t urped ne ine aupatonluppis w b oredtibthcoredy tonrn pe t ndond  fpocos fis pareuiy\n",
      "2.57149393977\n",
      "rt lte anarcon s ot ururfs iho s ge tust fd chamers t pefarl ss f arm binorcis irr m hakornal s mtori\n",
      "2.59453962439\n",
      " rucen t utsthefb wrunthithenthe on tenan tinin e chintrertr nund chond d fofy sisthofonand s peth rt\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "num_steps=40001\n",
    "batch_size=1\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    batch_x,batch_y = b.next()\n",
    "    if step%b.text_size == 0:\n",
    "        print \"NEW EPOCH\"\n",
    "    if step == 0:\n",
    "        output_pass = np.zeros([1,num_nodes_final])\n",
    "    feed_dict={train: vec2mat(char2vec(batch_x)),labels:vec2mat(char2vec(batch_y)),output: output_pass}\n",
    "    output_pass,l,_=sess.run([rnn_hidden,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 10000 == 0:\n",
    "        print average_loss/10000\n",
    "        average_loss = 0\n",
    "        \n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        val_output,dist = sess.run([rnn_hidden,probs],feed_dict=feed_dict) #initialize the validations better\n",
    "        char_id = sample_prob(dist[0])\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict={train: vec2mat(char2vec(id2char(char_id))),output: val_output}\n",
    "            val_outpout,dist = sess.run([rnn_hidden,probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            s+=id2char(char_id)\n",
    "        print s\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-38db31b3d680>:78 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes=200\n",
    "#num_unrollings = 2\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    train = tf.placeholder(tf.float32,shape=(batch_size,alphabet_size),name='zero')\n",
    "    \n",
    "    #the previous output the gets fed into the cell\n",
    "    output= tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='one')\n",
    "    \n",
    "    #the previous state that gets fed into the cell\n",
    "    state = tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='two')\n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    labels = tf.placeholder(tf.float32,shape=(batch_size,alphabet_size),name='three')\n",
    "    \n",
    "    \n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    \n",
    "#     #output matrix before softmax\n",
    "#     W_final = tf.Variable(tf.truncated_normal([num_nodes_input+num_nodes_rec,num_nodes_final],-1.0,1.0))\n",
    "#     b_final = tf.Variable(tf.zeros([num_nodes_final]))\n",
    "    \n",
    "    #softmax\n",
    "    W_softmax = tf.Variable(tf.truncated_normal([num_nodes,alphabet_size],-0.1,0.1))\n",
    "    b_softmax = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    #model\n",
    "    # Definition of the cell computation.\n",
    "    def LSTM(i, o, state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    output_after,state_after = LSTM(train,output,state)\n",
    "    logits = tf.matmul(output_after,W_softmax)+b_softmax\n",
    "    \n",
    "    #calculating the softmax probabilities for an input\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    \n",
    "    #train\n",
    "    #loss\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits,labels))\n",
    "    #loss= tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits,labels))\n",
    "    \n",
    "    #optimizer\n",
    "#     opt = tf.train.GradientDescentOptimizer(learning_rate=.1).minimize(loss)\n",
    "#     opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)\n",
    "    gradients=optimizer.compute_gradients(loss)\n",
    "    gradients_clipped = [(tf.clip_by_value(grad,clip_value_min=-1.,clip_value_max=1.),var) for grad,var in gradients]\n",
    "    opt=optimizer.apply_gradients(gradients_clipped)\n",
    "    \n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW EPOCH\n",
      "0.00328375148773\n",
      "mmahjwjnjjapzuqxslyclajxd niwdkspv qqabtbbtxcjv jtvptqsoo moxutxw bnpud fphvbkyoxpifzhcotgfpueuyzlusn\n",
      "2.94763534886\n",
      "de iwieh nilztaioriroccorcaabad  bimcriesnaegbtiiearunevtsls eaeoomtctitkcrs afrebneat jemhdvdnailbae\n",
      "2.91269771791\n",
      " citapvd  bthurnnsyroetirt rh idoay nanlkccnniint ii tsrfnlristiko uelaxd e bo d a ls s f hoktl igsc \n",
      "2.85474477287\n",
      "rrrdm rsergan ness d sslegt a mtisla n  nroyihlds  n  ilher om onhoearnremten hseeictgey sinnwls s ry\n",
      "2.82076149687\n",
      "ofsoofhanoxyo roderteon resehereeoeckamrasee asrolnpadys famodcyienunouisian ireerf foeht aecotth thg\n",
      "2.85940232995\n",
      "phohiou aettlarspos ooute oeovinogiou oanonuugpo eo oohd lilwaciciince tt whagono honios edteeromisso\n",
      "2.74991314648\n",
      "ecn itemialan kenpnlinrfecammr iondnzdh uaretetnonrisfre aag ioed erer  inhinr fan rhovthwsjaedhthhse\n",
      "2.84983351021\n",
      "ooooocooponyho nie ohanano kunouarta aatonenaorsorothiao ast iaacor ostosoen oooeeo oicoositt osmolbs\n",
      "2.92542555242\n",
      "ndanj s an b induand sn pndumdannnasrrsedanandeuldrlepedtulrity wphed nleuyty sairers befaanh ev isqo\n",
      "2.59895157136\n",
      "r jivurd eint fiserbe alogurthy wuseraris niro arte firk and radisin woifumtun irutre cant bervond  a\n",
      "2.42400593413\n",
      "hed coenrhercermed nomemen s cone anem ens onpl owugrey ced odhe onegiasm som berliswed efed thionps \n",
      "2.45745385352\n",
      "enir darsamapr ane thist droninen the ane as misto fyeved sisw hanm anjtsorre coy edamt is orckecism \n",
      "2.40986661595\n",
      "ebo thickuchilkikasbeatev ofo he ady tomx anatguchistled ab ed okghe andokidererouthts ofetato th ofo\n",
      "2.39661094232\n",
      "se cosvactinte mich ann ts an canarlyt th fotlrxiathie bercoas ofm veg sm nerces patin medgg ss thel \n",
      "2.32116248199\n",
      "teig ront s of vir an the thed ion thior ph ileceige ce torevoren popn owo ary m onl ikge ilaed on sr\n",
      "2.24736291651\n",
      " ininan we rolzaip y spne zanin svee zaed tht pocaxo jho lerilroun inq ghe wis lakistiflicme fine the\n",
      "2.18332059593\n",
      "in ferionsionditio doechride unchi ancanarithel anfiint enybcera daict mostiin inst ferco nalithalya \n",
      "2.23289616244\n",
      "inasir wenorancicithmich rewher woferprio s thiong trtinina kened juqnee reralpticn  and rover are ze\n",
      "2.2713578483\n",
      "alonacs owmoeporo arn ad abeistsors andinand ow tover laxevor teremshon ef inas ancola n oforf chanis\n",
      "2.13101529067\n",
      "the fonsacte ianis ty rin calescond the an efo fhe de be kus thist de this ve ne tount ivis pictot ar\n",
      "2.22165613048\n",
      "te cine rudizee t arcpict ane pratixis piolede f the rato ctacustig and of orte trtiof carps und ther\n",
      "2.07884366972\n",
      " the sed ce thee nipgorshist wunavists and the vanocroin alanchillallifinived thruce vormmetgen ann c\n",
      "2.14440077648\n",
      "ury diheord worctirk in ricluthas fofgoze reprcist d bellctios polhy caust torulisterigiand wious the\n",
      "2.19982683498\n",
      "ear archin treden ante derus and eaptealy anbe n raridistlan octrogey galanch ofy ancte herxiand cend\n",
      "2.25407839193\n",
      "aly ferigrmearch extone aism kaands er syist ecitfeocalnaren ars of ouch enopid hend rete ana ferocex\n",
      "2.04143926131\n",
      "eururch anx eimifedlesg veng ansofdj rithm cond fitiasly cald retthr oectortapasm crist rancdoxlely h\n",
      "2.09239522271\n",
      " fop nive mostaiti s aparchist aed levini fole bithour anad hihlost whemmin of of merexp peouriall wo\n",
      "2.12192121488\n",
      "bro fers cd the calism sith manpicy min hpsed un somen cane scevincle fucital moparis amarm nrelsts v\n",
      "2.01273507828\n",
      "ects ar ghats rike tare anar homped thirt ilitonty io ant onmtreo car arthact histeor apse bce krover\n",
      "2.08475461209\n",
      "he boreal high acany on minar hathee stare coleonrch isteothgerourar che peiti alfuen treje urte anar\n",
      "2.15871146137\n",
      " in hakist in herthers mianc arto een tror vanchist cavivel pomther in enth olarian ofs covellen trma\n",
      "2.17774062313\n",
      "octiche incgd pone sotsivind fowim ss pre pas tare ofts a lecpaty eouraticluestubg of al se sedes bec\n",
      "2.08787259958\n",
      "ith al mar watiougarl anar hassj a anthe atanicnos aled and diclanat auss anarceiszbing ef insupudina\n",
      "2.05129181815\n",
      "s an atedtorey anis ive bite tevo resision oug on ang the thos deverpin wousutice the theles asi asm \n",
      "2.07202910024\n",
      "w of to can dory anarchinancs waund of a con orbaists creh cosukg anctorcarcerk bospo is akshikh coe \n",
      "2.04943436362\n",
      "vomacys ees anarchion angationtionsy mate monvonise te opelufur secusece ofgate in onetans stice ats \n",
      "2.10688076759\n",
      "t veocuces coupiexs the zerupseront and worem noed grevent worm cechice povuterents veeen neercal kor\n",
      "2.04381390075\n",
      "g hutok nouls anertatadico is anarchi sustems eanthe shaichist pading gais go dognko in entias  anarc\n",
      "1.99337842131\n",
      "of brent audhy in the eose to blopol as og ophbterm anireagibut erms bermen erthev or hah varutiand o\n",
      "2.10851621705\n",
      "n be antacearshy bolle teat nons chat a l edsuate adlilf mans whoug asbsicarthe anacles if lleot ruce\n",
      "2.0243253253\n",
      "e pres telt nenist in wionets thasen of suceists indit terten thoplaracy ansewhism bersts the pone pr\n",
      "2.03626001173\n",
      "ally kooterrdy akd anists adestianted was ix lilucieley aldedicly anantiels thivelyists wereved mevpo\n",
      "2.12414213094\n",
      "atertamfosing prialaridy pormitun the in in fatudy the rernnia nye ind and hes rath torg the dovures \n",
      "1.70413354391\n",
      " otplist bine woun ans ant urcelory enoc mucothomiamarian les fanizenc mmsithation one anarchism ghe \n",
      "2.04907076763\n",
      "g the an enhre anarction ocrinis whersclatidein cestataolaturice site unhcenaricos og huvicoatism cha\n",
      "2.09825359802\n",
      "enatilitfors mheersent muve of clysermnres of kely ysmpykred ofdenms noeds wonle atixl luserres relts\n",
      "2.12396200585\n",
      "mdinusicy asiglabcist vingaron andivesopical op manftiou bominist on alormavot and actiliralize aunta\n",
      "2.08971176627\n",
      "bo condior co at id thos horus chitee smund wonting eswors aduth wirf hate ly tuatitiagism and the wh\n",
      "2.16273937357\n",
      "gare axpre sos descreriand in therbal sresreall to pergreotism the ras cre ascy thil at nistrcgurkers\n",
      "2.14024279734\n",
      " of abitilize vire oref to tred helaropbe reviniosceria ferm onect atestery mine ored ary als uterm y\n",
      "1.95851025578\n",
      " to mnyen and neleenteagro s with nover leent eneceristic sminter cwisted leeowithel the ske one dist\n",
      "2.03828709836\n",
      " the tont other rether com doldred intertort tromiwtintrout dooms as hist op onorovem abten at por or\n",
      "2.06335397353\n",
      " indrawifitentrearspous ugss ild ofde cudiand thibys tare forchist a paciets in placoloper the p andf\n",
      "2.07888909826\n",
      "usinsom ingler butrhreveutis is ida miretorlfl prophtorimas csution opr eealclndeor pren soxm incgrin\n",
      "2.0198957992\n",
      " to sorusis i proutor the forgention isgnor compsticy te hhares prouthre pestcaltist in ofdo frouple \n",
      "2.07455663608\n",
      "oa terperications anh arme authilgme itell inas owent autism pars on can losomirabl boh as ling mablo\n",
      "1.97436956528\n",
      "g on vived to nar chily roev the in ertay a cartuty oth maves of to lang ot of comers grougs wilungue\n",
      "1.85408573569\n",
      "ednal lest with uaut reverspoveted and comeond moverilugitidys sech voa meak on ve as op lube ther so\n",
      "1.75762580044\n",
      "moter thilfilal autism artion chal chaldly to nimaly socle auth autistis waple auplor aive asoplomeag\n",
      "2.08183091099\n",
      "ebing doly ocht exter formougitaling techustreacange cretaals thee leragkent authens deabel or thepou\n",
      "1.98667306915\n",
      "mundies ning meystery andeds of thender that seved tat thesuaing ypal mom tasiadent soam ardever soms\n",
      "1.86689216225\n",
      " acaild to the reach teibe ocmeopitare almontave one tiact po aratory that boig tikd toro manoo alto \n",
      "1.87494024918\n",
      "econd this extrecan and offechicar s fetcicial nutuatiand to chile hove paing andornt cand wimpleitio\n",
      "2.04982002613\n",
      "ntpracing apcaitior entorlets int foy rows tpoint prsopitity stwigirtivits haciol meirhe ty chime oro\n",
      "1.95350343282\n",
      "trestreacted peintnente fdoprtolebs ind bysire or formens unf toply tourakiel spdparding preperos and\n",
      "1.98781444532\n",
      "th almowiares in aspifia tho fovely thre divenoribed pree aned and cial and soxaintopreal into intual\n",
      "1.87718403757\n",
      " sominize a theni gory medicl of autism bed stuals con muserents infisie chive thino nite ther a the \n",
      "1.89882902546\n",
      "e wromick that the indes one of socienfocnavs delay the lasintact the ofterent betcctions nactyplats \n",
      "1.81035933493\n",
      " neconcin to zuberam puce thay they wing of as put thes hactrout sokwer the pela resanut od that exth\n",
      "1.91280474496\n",
      "ofbel dypel at detoautism hav anxas the snct darf masing meatt othuly disour trably s a that ic lexe \n",
      "2.04180056254\n",
      "geung main assowe in to in reices autism se marnenc enaents a chia cautism in a dnonn the zeanssuas a\n",
      "1.99363037876\n",
      "nched in ponented to meand in tharic chibe who sen ancely wisone the the bet parled who instro envifr\n",
      "1.87239013211\n",
      "ith sizce of peepliof to didss bo mphtour stuul symeroty es aar reont blifictdost a pribhh lo afnhati\n",
      "1.81284380633\n",
      " u the bose in the othe in feneg eve auti whiride day whim stivo whis  aulity and of of to deve in th\n",
      "1.88131505296\n",
      "ntity anal encensistic dispindrity aresest crilitabuted nowhas giowt indionaticm a autistic cace indi\n",
      "1.72364971392\n",
      "har autistics in the caliters pain to the upely daroart of the wozautistics meosper the portions auti\n",
      "1.88048788973\n",
      "astior the mor asse mappities us ver somerevent diefrieg candife morukest or dich it mortien pare han\n",
      "1.82643435918\n",
      "tic laccinut autistics of suke the of searg tomelter overome of otheds mak to mor tensile s socade fo\n",
      "1.7993453274\n",
      "ciely dasmantasementaltal promevatione of siventer syndental pemporjentind the itore the farverders s\n",
      "1.87249722444\n",
      "istod pe the of the re can autisticals chol ozer an tho hiration a peett iads as engy autism th onper\n",
      "1.83542994018\n",
      " conding hossotsurettual hore fisorded cith is autisy of sive wod man cdurate divasins suvite chich d\n",
      "1.69865823175\n",
      "al actional assm autistic autist ctsps in thal diagnosts anl syntr of autist cauped a pect ant speiva\n",
      "1.95857959344\n",
      " deso zever de for s peries to autism not bespers wristrup trum us sig thesic und appikes fwo nouder \n",
      "1.72892597796\n",
      "g man on than by form but of runch ord of autism spind aupers autits a catority autism ration instics\n",
      "1.77129166804\n",
      "on arcial atiatism vilervaction tidy intery troal spectriancres to reatiqg diwitrourous cldntal dizat\n",
      "2.02608688495\n",
      " aloog in seloure of o nesone tho ffom any ituly morlo fur hast withith disordizif foro to s wity inc\n",
      "2.01283733313\n",
      "freriatiost of a chrage bevactercs othe grbecliss leceationg rer sorling creal har liolcom cont noceg\n",
      "1.97726873709\n",
      "ic ne mea spun thesupe in dick anceridests in the unls the fhes bowht chact whilly highs of theole wi\n",
      "1.85642698234\n",
      "of five have oun far zeadny wande dive hepen wan autilg oou five autrune envalsul enverous bfeauns by\n",
      "1.83160315723\n",
      "dany ass froulse sforby noso nare duased for corled boor fores cobinhial counds duast albedo ninbella\n",
      "1.88231485906\n",
      "ighted alonesing canle whivintiovereds and ince iffertolmplically s withent nows culdooutive thcilse \n",
      "1.94752275684\n",
      "pmer mard bepf tweare troves snecterents to tert surue havect ar mow als alage woret filld were delat\n",
      "1.93947994394\n",
      "wt sarturice eay ligh the one thir e to on ua cynme sopitist twiturcage a sof if fine c ror of for th\n",
      "1.92359509759\n",
      "tud cs of than abea ababocy one its abee or  sunging dae cand the seltectand nowent haby nesticern ca\n",
      "2.07691225404\n",
      "oulthin tertati of the arechtud axd turn whaval proin habute basts oralding ohe nopat the tco ba juta\n",
      "1.97009165181\n",
      "alds of the illy neminy abi kis harge dhebaand of rend andy shemetent the unita ze morks and ala the \n",
      "1.76960099542\n",
      " on aust one suve folfidw one wera famh onfal ham cultould wegnturde arage to aution and wain eight o\n",
      "1.84963058941\n",
      "ticlenter othe one seven beses pexpreeacs dett comper twested tree this one as the as sad ix the buts\n",
      "1.76107272406\n",
      "praidhduck of one six two nal wale oes five zeria waling aver and compred wan iss withled bricas drag\n",
      "2.00836730536\n",
      "ar tott mis nat main one ner fir habigh dhistatio truas wat ins eough pat litlic foul the the bercy a\n",
      "1.89221063712\n",
      " eived in larkaty one uloroparre rial tructer truts buk trearobitees in to sal the ai aunsoorghayi ma\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "num_steps=100001\n",
    "batch_size=1\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    batch_x,batch_y = b.next()\n",
    "    if step%b.text_size == 0:\n",
    "        print \"NEW EPOCH\"\n",
    "    if step == 0:\n",
    "        output_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "        state_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "    feed_dict={train: vec2mat(char2vec(batch_x)),labels:vec2mat(char2vec(batch_y)),\n",
    "               output: output_pass, state: state_pass}\n",
    "    output_pass,state_pass,l,_=sess.run([output_after,state_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print average_loss/1000\n",
    "        average_loss = 0\n",
    "        \n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        val_output,val_state,dist = sess.run([output_after,state_after,probs],feed_dict=feed_dict) #initialize the validations better\n",
    "        char_id = sample_prob(dist[0])\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict={train: vec2mat(char2vec(id2char(char_id))),output: val_output, state: val_state}\n",
    "            val_output,val_state,dist = sess.run([output_after,state_after,probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            s+=id2char(char_id)\n",
    "        print s\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unrolling LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_unrollings = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 'c'),\n",
       " ('c', 'o'),\n",
       " ('o', 'a'),\n",
       " ('a', 's'),\n",
       " ('s', 't'),\n",
       " ('t', 'a'),\n",
       " ('a', 'l'),\n",
       " ('l', ' '),\n",
       " (' ', 'c'),\n",
       " ('c', 'i'),\n",
       " ('i', 't'),\n",
       " ('t', 'i'),\n",
       " ('i', 'e'),\n",
       " ('e', 's'),\n",
       " ('s', ' '),\n",
       " (' ', 'p')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_batch = []\n",
    "for i in range(num_unrollings):\n",
    "    window_batch.append(b.next())\n",
    "window_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ',\n",
       "  'c',\n",
       "  'o',\n",
       "  'a',\n",
       "  's',\n",
       "  't',\n",
       "  'a',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'i',\n",
       "  't',\n",
       "  'i',\n",
       "  'e',\n",
       "  's',\n",
       "  ' '),\n",
       " ('c',\n",
       "  'o',\n",
       "  'a',\n",
       "  's',\n",
       "  't',\n",
       "  'a',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'i',\n",
       "  't',\n",
       "  'i',\n",
       "  'e',\n",
       "  's',\n",
       "  ' ',\n",
       "  'p')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(*window_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = zip(*window_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'a',\n",
       " 's',\n",
       " 't',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 'c',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'e',\n",
       " 's',\n",
       " ' ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#currently missing a lot by calling b.next() all the time\n",
    "def getWindowBatch(b,num_unrollings):\n",
    "    window_batch = []\n",
    "    for i in range(num_unrollings):\n",
    "        window_batch.append(b.next())\n",
    "    window_batch=zip(*window_batch)\n",
    "    x,y = list(window_batch[0]),list(window_batch[1])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_x,batch_y=getWindowBatch(b,num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p',\n",
       " 'h',\n",
       " 'i',\n",
       " 'l',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " 'l',\n",
       " 'y',\n",
       " ' ',\n",
       " 'b',\n",
       " 'y',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'u']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 8, 9, 12, 1, 20, 5, 12, 25, 0, 2, 25, 0, 3, 15, 21]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(char2id,batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0], dtype=int8),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0], dtype=int8),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0], dtype=int8)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(char2vec,batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(char2vec,batch_x)[0].reshape([1,alphabet_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for vec in map(char2vec,batch_x):\n",
    "    vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "slice index 1 of dimension 0 out of bounds. for 'strided_slice_1' (op: 'StridedSlice') with input shapes: [1,27], [1], [1], [1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-70dea6941533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_unrollings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malphabet_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36m_SliceHelper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m    588\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   3501\u001b[0m                                 \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3502\u001b[0m                                 \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3503\u001b[0;31m                                 shrink_axis_mask=shrink_axis_mask, name=name)\n\u001b[0m\u001b[1;32m   3504\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    757\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    758\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1615\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: slice index 1 of dimension 0 out of bounds. for 'strided_slice_1' (op: 'StridedSlice') with input shapes: [1,27], [1], [1], [1]."
     ]
    }
   ],
   "source": [
    "#need to run the graph first before this will run\n",
    "feed_dict={}\n",
    "x = map(char2vec,batch_x)\n",
    "for i in range(num_unrollings):\n",
    "    x[i]\n",
    "    feed_dict[train[i]] = x[i].reshape([1,alphabet_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0], dtype=int8),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0], dtype=int8),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0], dtype=int8)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(char2vec,batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02904298,  0.02703609,  0.03863471,  0.00728035,  0.06218987,\n",
       "        0.01272476,  0.03239124,  0.02270598,  0.05485696,  0.02607722,\n",
       "        0.06948997,  0.00536511,  0.04520778,  0.03975837,  0.03899329,\n",
       "        0.07407319,  0.06580299,  0.07135798,  0.03104809,  0.0409943 ,\n",
       "        0.00701552,  0.03624875,  0.01014228,  0.03924026,  0.00197938,\n",
       "        0.03648808,  0.07385452])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random distribution\n",
    "def random_dist():\n",
    "    r = np.random.rand(alphabet_size)\n",
    "    return r/np.sum(r)\n",
    "random_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0054"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "for i in range(10000):\n",
    "    a.append(sample_prob(random_dist()))\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-46-3c21e874be9a>:91 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes=200\n",
    "num_unrollings = 2\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    #train = tf.placeholder(tf.float32,shape=(batch_size,alphabet_size),name='zero')\n",
    "    train = list()\n",
    "    for i in range(num_unrollings):\n",
    "        train.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "    \n",
    "    #the previous output the gets fed into the cell\n",
    "    output_feed= tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='one')\n",
    "    \n",
    "    #the previous state that gets fed into the cell\n",
    "    state_feed = tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='two')\n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    #labels = tf.placeholder(tf.float32,shape=(batch_size,alphabet_size),name='three')\n",
    "    labels = list()\n",
    "    for i in range(num_unrollings):\n",
    "        labels.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "        \n",
    "    \n",
    "    #validation place holder\n",
    "    val_input = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    val_output = tf.placeholder(tf.float32,shape=(1,num_nodes))\n",
    "    val_state = tf.placeholder(tf.float32,shape=(1,num_nodes))\n",
    "    \n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    #softmax\n",
    "    W_softmax = tf.Variable(tf.truncated_normal([num_nodes,alphabet_size],-0.1,0.1))\n",
    "    b_softmax = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    #model\n",
    "    # Definition of the cell computation.\n",
    "    def LSTM(i, o, state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    #when training\n",
    "    for i in range(num_unrollings):\n",
    "        if i == 0:\n",
    "            output_after,state_after = LSTM(train[i],output_feed,state_feed)\n",
    "        else:\n",
    "            output_after,state_after = LSTM(train[i],output,state)\n",
    "        logits = tf.matmul(output_after,W_softmax)+b_softmax\n",
    "        output,state = output_after,state_after\n",
    "    \n",
    "    #calculating the softmax probabilities for an input\n",
    "    val_output_after,val_state_after = LSTM(val_input,val_output,val_state) #change train to input_in\n",
    "    val_logits = tf.matmul(val_output_after,W_softmax)+b_softmax\n",
    "    val_probs = tf.nn.softmax(val_logits)\n",
    "    \n",
    "    #train\n",
    "    #loss\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits,labels[-1]))\n",
    "    \n",
    "    #optimizer\n",
    "#     opt = tf.train.GradientDescentOptimizer(learning_rate=.1).minimize(loss)\n",
    "#     opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)\n",
    "    gradients=optimizer.compute_gradients(loss)\n",
    "    gradients_clipped = [(tf.clip_by_value(grad,clip_value_min=-1.,clip_value_max=1.),var) for grad,var in gradients]\n",
    "    opt=optimizer.apply_gradients(gradients_clipped)\n",
    "    \n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW EPOCH\n",
      "0.00315153813362\n",
      "oz dausbkpm tfdawzktghpy wgdbezfwqnlzad hsjsrdjxjylygaqidavhr sbxbrritqvwqdbsvvqqguty nhiiengdvohiajt\n",
      "2.96640299222\n",
      "hnr n ltiialiot cs aolcaci  binn rdugesors ratta gce vsdasarmton sgt oeqsaozottc eo  kacteurina dihon\n",
      "2.86862867916\n",
      "s tenissl yt ap n goiitnd dve nndorrsi dhrhgrr hesrn m rrr  ijkh h onoran  refisg pfertxnccesrdrlhodf\n",
      "2.81186389065\n",
      "jkyaae pdhe uhlyoun fefar  aafilhdtd ralanr laie  errenydeet g g heyael rwba yfi ipya y oaclrtlisyas \n",
      "2.73009972018\n",
      "kodenlt owejen eae anans andfuluegooilhs m oe ui estonk ml ihandodesgseeu sdihe ho ddiceo e paiscscdu\n",
      "2.64976170111\n",
      "ds  anermce indars doacdg podcordadtinhsbunintyorcoainsm hemenors ninud hosadirtheirkoniedestusit the\n",
      "2.7037856518\n",
      "glaole enzaall a o roond cascled bbteathh th othem at h aelu fecbownroaory oeavascqod ovr c onteo cou\n",
      "2.6205986399\n",
      "zin henovaruarsl w avh s an ngnardineokuriadin w bcinnrstso nnumistay ste wthh ism ut onh noale endic\n",
      "2.5387362823\n",
      "fy sorthl or sesoueelmcaoner cbseuarearis ed moharesian  swetr adedilt in n sdaqerthanevenalici onald\n",
      "2.52891299095\n",
      "t fostary ald agatit ohmnaxosaem sued al th mur on voinn to pamanhd e muan fhed efn an th amumgmssthi\n",
      "2.44055909152\n",
      "fscale of e lhiis ne ct k tist thr ofliluls th ofaunig th econ rieruill ph almmr utt zovith che t uin\n",
      "2.28798066099\n",
      "kal ofe chind cinc azpne vovipes whorein herrhedig oe anchasiou fuonid nnu arichtyherry ty th ctuwhar\n",
      "2.44443800365\n",
      "xred  as d torel hetr be arc archics toal a su aedancy tye alocy nam as bo of mthar hsy chsin ppabern\n",
      "2.21788902862\n",
      "gh ang smend an qeramissinina whiicalar hers behuichimt finmigw anirihis wigers diall indmararaha teo\n",
      "2.22466020702\n",
      "jqtray thosnity and erathe of it art n ecoutak eororchits af mocthe d her as bit anien da emte ed te \n",
      "2.25022331118\n",
      "zchens the thery inl andiofis dopst antiwy tgond vioncery fssml antre chist momtists ladthhe tons chi\n",
      "2.22843936118\n",
      "astisu uns nl holicl anfoche atsiove thins rhe seet atinound of lulicuglsecuns lcendyt sacauing resme\n",
      "2.209071837\n",
      "tocingontur f anacation anand unacthe fusr awendecane alarchishujrga inats anrerth ce s carns ancho s\n",
      "2.21927376133\n",
      "vist ente ancal ur momla timmerrhty alita somne the nnt phal of thevolem anormtigw mwte bra pricion g\n",
      "2.16603624916\n",
      "h nopoeceinmoph lereith soconn ce ethe allibal in buwush thon inovizwand itatooung th othaiuw theoh e\n",
      "2.17601977173\n",
      "vun firlfzend custoly cilses thonarlins oforys ptecminaks vanarisy uguntticismk enaucinnt onarchism a\n",
      "2.13977691037\n",
      "my phe to sgarist ars pent lyrof gumarisg woy hont forty doty ugand motion con a dory to cent rof apl\n",
      "1.98080189237\n",
      "zont s lithist in on eropotatre in pproddport hopted wyrmingestion ren erneve one enin paringeidest l\n",
      "2.26961013957\n",
      "qcing tolitil foulaatco mstismevroj wmarm a mere chiticl vaginmert of final mubhitives xent autilltio\n",
      "2.19618115613\n",
      "lt serelatism arhis aystram dactox chd daugd thets s cherre hers porcenngict lecy ilsyict prsictert a\n",
      "2.22255745894\n",
      "finilic hisc to tpeennsts d fousened inite thes seandito msam andigulannstistenles inalsedtedtapkes d\n",
      "2.1913674956\n",
      " try scecle in arism antittic a of pang on ackicuv toss a yuts astion theus colivlo medilmleal of int\n",
      "2.18639629877\n",
      "btyy rath pwoutinde meermlaels yepyrth th thes woksrac ou thy seenplor efad poed sism erhallaut rotwa\n",
      "2.15414334029\n",
      "xe urfingard twe naug goel and encole thargaraly tive d asiin pisay tho in antantuth evliotisy cer ga\n",
      "1.96985460451\n",
      "kmpiny theatale of mowe autistion whomy ars on aumin to whdeic to clam uescautic at ow efpronl uther \n",
      "2.19299212912\n",
      "h winc thee alree bors or paes aul thoularls abde sos thmes menar acusten ovebver hated bat or chaven\n",
      "2.05899486634\n",
      "pror to ibe ist ngtion woica vealdem wibion hat of a civings sthiccudent ape stily obech tociges toud\n",
      "2.18885970943\n",
      "gates andmenss iy orparment ib axeress of or to or fipbec ett onchret of trerencor astand ontire in s\n",
      "2.10236440554\n",
      "nstexys conte tire chinely ans stise hepor is a torce a aicilics autict or sical is stha so saed fork\n",
      "1.99494230377\n",
      "jtutrem abler usoplllach other fraue in auticl dhesakmat lyr hemomees a teased nandions ander as lafi\n",
      "2.14329419326\n",
      "ty inuldues butif the beches chabunder bew soy them bopioshal metreald reda of sears hedotor uncaus o\n",
      "2.06753165169\n",
      "xury atler to keliyntic ip profddrot no efpereges seal ust of sustred cutclibe is ewhimples in the in\n",
      "2.02130254934\n",
      "edsindibing it buzistrar clat ther pithiged vinediristaded ows ald autrists ondes ardioglity mane sim\n",
      "1.9526642786\n",
      "lalies fearchul ofsedran agthe nige the iag peree aturism be mal meve fre sevevien sitwate ev ony suy\n",
      "1.93719084551\n",
      "wevinith koth ired ordispromers a disent to the wenctorm dist is is eeturmtiny forpe the d  jundive t\n",
      "2.053897082\n",
      "senclesirgalisted shiverlanming riaperofthresine is antsindian somand ares nosere remarcultmoms fotfo\n",
      "1.98298494649\n",
      "m wiine inter relldamectism scech autism stroudism worm wisactrop untpl ne tbetsoum ment twesic nersu\n",
      "1.95772062882\n",
      "zerliggs indirk autismis homo in in clizor for tom s auty ris ecby aucism sper atarise sixdivetzrutit\n",
      "2.10477856807\n",
      "inges falgisilaration in ads thing mon jingllie the zeof as nith those son dility ausis aning th of w\n",
      "2.05827505501\n",
      "wurcoulig elfoore ine albend douater gene the uwy bes anderglsuric mace sincen theenses ratien reymon\n",
      "1.97518230154\n",
      "foco veavenas the hallandif warmincring norol sowzwrales ale tho mavelanbly rabbame gall atpexullings\n",
      "2.08675357333\n",
      "al abte factirate as the undan of efter atic eat als a divest dat thit cloa to as a ols dad hative in\n",
      "2.13591636623\n",
      "ver froy ereg sidench nehakil guen trousrookered inhay aled veoglopn the solsed thin al in noledal al\n",
      "2.03845513843\n",
      "fromuce twenin eeg wouttulag cwinic alo and nommone lwrop can nanduraal ing in tregist one forn one t\n",
      "1.94601221151\n",
      "ker ind indrag is of bts ara prai noded ledy wero the meribitury the neini iin las thdo has a mix wat\n",
      "2.06808065058\n",
      " band everuvimloot the stam yhed omero aucs one ration boc sinction harues ands the ails bnitcss abu \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "num_steps=50001\n",
    "batch_size=1\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    batch_x,batch_y=getWindowBatch(b,num_unrollings) \n",
    "    if step%b.text_size == 0:\n",
    "        print \"NEW EPOCH\"\n",
    "    if step == 0: #initialize the output state vectors\n",
    "        output_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "        state_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "    feed_dict={output_feed: output_pass, state_feed: state_pass}\n",
    "    \n",
    "    #trains x\n",
    "    x = map(char2vec,batch_x)\n",
    "    for i in range(num_unrollings):\n",
    "        x[i]\n",
    "        feed_dict[train[i]] = x[i].reshape([1,alphabet_size])\n",
    "    \n",
    "    #trains y\n",
    "    y = map(char2vec,batch_y)\n",
    "    for i in range(num_unrollings):\n",
    "        y[i]\n",
    "        feed_dict[labels[i]] = y[i].reshape([1,alphabet_size])\n",
    "    \n",
    "#     feed_dict={train: vec2mat(char2vec(batch_x)),labels:vec2mat(char2vec(batch_y)),\n",
    "#                output: output_pass, state: state_pass}\n",
    "    output_pass,state_pass,l,_=sess.run([output_after,state_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print average_loss/1000\n",
    "        average_loss = 0\n",
    "        \n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        \n",
    "        #initialize the validations out, state, and character\n",
    "        val_state_O = np.zeros(num_nodes).reshape(1,num_nodes)\n",
    "        val_output_O = np.zeros(num_nodes).reshape(1,num_nodes)\n",
    "        \n",
    "        char_id = sample_prob(random_dist()) #create a random distribution then sample\n",
    "        #val_input_O = char2vec(id2char(char_id)).reshape(1,alphabet_size)\n",
    "        val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict = {val_input: val_input_O, \n",
    "                         val_output: val_output_O, \n",
    "                         val_state: val_state_O}\n",
    "            val_output_O,val_state_O,dist = sess.run([val_output_after,val_state_after,val_probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "            s+=id2char(char_id)\n",
    "        print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train over all unrollings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-48-5288370d834b>:95 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes=200\n",
    "num_unrollings = 16\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    #train = tf.placeholder(tf.float32,shape=(batch_size,alphabet_size),name='zero')\n",
    "    train = list()\n",
    "    for i in range(num_unrollings):\n",
    "        train.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "    \n",
    "    #the previous output the gets fed into the cell\n",
    "    output_feed= tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='one')\n",
    "    \n",
    "    #the previous state that gets fed into the cell\n",
    "    state_feed = tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='two')\n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    #labels = tf.placeholder(tf.float32,shape=(batch_size,alphabet_size),name='three')\n",
    "    labels = list()\n",
    "    for i in range(num_unrollings):\n",
    "        labels.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "        \n",
    "    \n",
    "    #validation place holder\n",
    "    val_input = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    val_output = tf.placeholder(tf.float32,shape=(1,num_nodes))\n",
    "    val_state = tf.placeholder(tf.float32,shape=(1,num_nodes))\n",
    "    \n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([alphabet_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    #softmax\n",
    "    W_softmax = tf.Variable(tf.truncated_normal([num_nodes,alphabet_size],-0.1,0.1))\n",
    "    b_softmax = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    #model\n",
    "    # Definition of the cell computation.\n",
    "    def LSTM(i, o, state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    #when training\n",
    "    for i in range(num_unrollings):\n",
    "        if i == 0:\n",
    "            outputs = list()\n",
    "            output_after,state_after = LSTM(train[i],output_feed,state_feed)\n",
    "        else:\n",
    "            output_after,state_after = LSTM(train[i],output,state)\n",
    "#         logits = tf.matmul(output_after,W_softmax)+b_softmax\n",
    "        output,state = output_after,state_after\n",
    "        outputs.append(output)\n",
    "    \n",
    "    #train\n",
    "    #loss\n",
    "    logits = tf.matmul(tf.concat(0,outputs),W_softmax)+b_softmax\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,tf.concat(0,labels)))\n",
    "    \n",
    "    #optimizer\n",
    "#     opt = tf.train.GradientDescentOptimizer(learning_rate=.1).minimize(loss)\n",
    "#     opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)\n",
    "    gradients=optimizer.compute_gradients(loss)\n",
    "    gradients_clipped = [(tf.clip_by_value(grad,clip_value_min=-1.,clip_value_max=1.),var) for grad,var in gradients]\n",
    "    opt=optimizer.apply_gradients(gradients_clipped)\n",
    "    \n",
    "    #validation\n",
    "    #calculating the softmax probabilities for an input\n",
    "    val_output_after,val_state_after = LSTM(val_input,val_output,val_state) #change train to input_in\n",
    "    val_logits = tf.matmul(val_output_after,W_softmax)+b_softmax\n",
    "    val_probs = tf.nn.softmax(val_logits)\n",
    "    \n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW EPOCH\n",
      "0.00326658940315\n",
      "itfelebwmy rgixnxcfcveseeypra vlsrnyqdtyywvytbvespgmzvzfsjjawhlvfetdbny  znxhikwrxnkuspaojkbffzvwpaxy\n",
      "2.92798019862\n",
      "ye i zieeio e  onehaienethojl z rj ieihuzenylmvebooatt s c  kozrqe sckrenrernyo fi snrmneevadgtaadi n\n",
      "2.81759448504\n",
      "mo jnsyicyctstpthmnhcshiy gaornasx uastnfreeleori snmrgyre gtoc h aed msofrnsexafinekry trdtans handn\n",
      "2.74227179027\n",
      "dqfd afsioes neinksrsthaeinrsinantthugcztindrd uoerecewstee disaisthziuhd o nienhittotrdwh idpn ne ar\n",
      "2.71927181697\n",
      "here nmi eoneonnefhceip err tsipe or  poanetes t elepenu al an oo ruh ta tt de cnaylanponrntiis unuln\n",
      "2.66247247028\n",
      "y aentesrcay meuep pead dd fnenesvanpoizrbtd aocsimenorpacnu  enguf uvs plr p a iudtels ay ool erciti\n",
      "2.73523379993\n",
      "e snod  nnnap ssnne oyk sre madh se bh ned a naianlx nno ndz anoonea  wohu and nun aes crr tht unanua\n",
      "2.47201531029\n",
      "q fenbuis obels thrtr ate zsephas ating lar ros set a aral serae wiobt e af hewt woan amhumtie losthh\n",
      "2.34971864533\n",
      "xe uaut amin to tho weace le as andil th w ynhelmirey wedon gap ald cukelldom hs il ar thislorom enid\n",
      "2.33508347428\n",
      "ke se loru rs lherdesteed ro fedibe swek wins ursoced hintile bolne trofry anuceingofe duruidoreut ac\n",
      "2.24107826126\n",
      "e smtyegin the miro ob ole sebeled sibe los a ay oferyoin is mow oucion bacand axd the ateengusc aaw \n",
      "2.20996541739\n",
      "z morp uustote y bowcong the folcorernd chotatur wer enedlind on of nca counton hint sht six popegsoo\n",
      "2.16850378013\n",
      "lre n lentine fkersis w chominlins f reroront h prot hal cinl tol at ofed bind jebaren ent one ang wi\n",
      "2.14183234376\n",
      "thig b this cictid futhe hus or hoks war alin ow ies his atint ame garcotho g he tho the wewhstit s q\n",
      "2.09287572241\n",
      "ag rotam nof his ore p as s dercaty he hetoon the wergol iy soecale b onse eater e belins ot slothauy\n",
      "2.08982238913\n",
      "at de pint int ad hine tuor wereus are sing sine the prerer bess threa ond stincion s arto press cy o\n",
      "2.15505663002\n",
      "pa marcat and id teopuiedtoorm is non poreplabe feathass foistreic he heriinlibio of eder stantlicolt\n",
      "2.07644128317\n",
      "s iae uverione s ant einet the ninernely ane me isine a oasgsssis nor nen nine to ole nomekt albiene \n",
      "2.094771873\n",
      "vosne is the kas rewlar thefertion eavpre teop u ingen foth ary gaist the wor her the hinin as co mpr\n",
      "2.06875200313\n",
      "ze menice adig of a hino secsanily zeraptinith inst une bwibne peronore of pumune nitwvio in eight go\n",
      "1.94542909448\n",
      "rifebe fiallign e onelvern and of offerainche of feries in to suines folllouethinss ann otstliiged of\n",
      "2.04797946006\n",
      "iastwor the priatiins wabr gomed ot erolmate in thaned the liwge the wigrist rivetep rats scins prani\n",
      "1.99003349501\n",
      "ctat allle broun and in twon spe fotrea one foun this dy in the kode thorat son in hes whe s is ore d\n",
      "1.973972875\n",
      "jo nine one one uy roorne of thueon the six to goenlato ie fousnmen wyy to is ess bentradkat fruats r\n",
      "1.96671408433\n",
      "hisnlace to morl rald to tarde fiendaties fromen in the for has to his one covzer the forl fork word \n",
      "1.93496804261\n",
      "qe uskey dext the deriskest is oplerion atsovens soarn deselt the staty the clancents all toptersann \n",
      "2.01444604164\n",
      "bicnt scterencans lat tunties in crots nas cilducuebeition in ming anthropich one forseseding the jor\n",
      "1.91158019203\n",
      "ne deantird the d camtucal dyea huvey wit ree is thouniest all nitthy all wat patnculluy difoy colt a\n",
      "1.9141220578\n",
      "at writosoplly ealt metiolly for boilllongtirad surony mnsers betonsicsainm intrans afling wyud manpl\n",
      "1.88952014309\n",
      "unsmagation kurd orvingstifro as arzaeind on e nemper calded reale ariatt with two ragule exprecuets \n",
      "1.86378138494\n",
      "ats of ycterl anc minate wro the schisis my inmiphly sifoun the onj prich as indorite threp to ol to \n",
      "1.92246926445\n",
      "y aromotian aribued is dures adimn and in not brous thery altomint breor untriqids alcelus and chatin\n",
      "1.8773218435\n",
      "k an hustuliar barles poralling is our be kal arciens as innisoly was sithize the plomed goferrest in\n",
      "1.96346108663\n",
      "h intenn mefically treans ads the ainemend idd the fine zeroppey courad with its cophive the ant hive\n",
      "1.86491648525\n",
      "vien meran arsatent whi hoek corboust iesate of qimk resoud revertes anc homemains of atsmeme zero me\n",
      "1.86408931911\n",
      "citme sigria zero fith of eight on nities olag of the eaded the to th is of aistraced tie jamover tha\n",
      "1.75321974745\n",
      "lo y hus rele three wrila rastival a re lucters humintreds of larticialo wish surc the untrimial the \n",
      "1.76956975538\n",
      "cs of the two zero zero ob s zero one the hzertel himzurte unoly ning and hive be coono nemerth is ak\n",
      "1.91891979152\n",
      "y calactellitilisrad and necnoptapias gian and the amoefals orza wora the effaridy ans jefecas oblato\n",
      "1.80767055091\n",
      "me vornsbiciosders the nomper wer bro which for assitm of each sex nawnsentur contort astration desen\n",
      "1.82010163103\n",
      "je ai hame africe wour the eabal gacther maske beish hom a fald conth subed the beigne eftuthes ol bu\n",
      "1.78573210794\n",
      "we lated conning relicab megbtersed csinel of apridion tudearem of opesdinzt juritheric on othera pal\n",
      "1.86459821278\n",
      "aps thop uperphy to bleas nat worlo poustatal soppean ardics ongut of aldegsol the peononted shimoram\n",
      "1.92289677364\n",
      "poudenderentsone difes thuse prica ber nem huspes wropwalle dertholy onf the greegh the grombesiny hu\n",
      "1.80978192642\n",
      " the it would forlud tadse fine three sefer nok extions to linsh disen syshames plot defetion on the \n",
      "1.59370944737\n",
      "d metiver nated sarner sef swaclery finar prhacics open hiars cetters in layn te the mary fival prast\n",
      "1.87094142911\n",
      "h of verq which one timul hisnadian listian wiren poinate vernica ex chrip beinded exceded char condo\n",
      "1.83238348097\n",
      "ling one the lage toped firster and morobely corrsictry geee in ninen to seart goonopoge exirational \n",
      "1.8175564753\n",
      "atoonagion the parthredritwingree waroghes ill a dattraluise avensuentuecthotion vero uastdanation of\n",
      "1.81059281459\n",
      "m afterne const him sublister two zero zero those gour oan they ol musicon gior on the innopration th\n",
      "1.77328261214\n",
      " zero lime preishes pogoll renth kootely tra diesial by the fause the anrebond of of manks culnimity \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "num_steps=50001\n",
    "batch_size=1\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    batch_x,batch_y=getWindowBatch(b,num_unrollings) \n",
    "    if step%b.text_size == 0:\n",
    "        print \"NEW EPOCH\"\n",
    "    if step == 0: #initialize the output state vectors\n",
    "        output_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "        state_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "    feed_dict={output_feed: output_pass, state_feed: state_pass}\n",
    "    \n",
    "    #trains x\n",
    "    x = map(char2vec,batch_x)\n",
    "    for i in range(num_unrollings):\n",
    "        x[i]\n",
    "        feed_dict[train[i]] = x[i].reshape([1,alphabet_size])\n",
    "    \n",
    "    #trains y\n",
    "    y = map(char2vec,batch_y)\n",
    "    for i in range(num_unrollings):\n",
    "        y[i]\n",
    "        feed_dict[labels[i]] = y[i].reshape([1,alphabet_size])\n",
    "    \n",
    "#     feed_dict={train: vec2mat(char2vec(batch_x)),labels:vec2mat(char2vec(batch_y)),\n",
    "#                output: output_pass, state: state_pass}\n",
    "    output_pass,state_pass,l,_=sess.run([output_after,state_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print average_loss/1000\n",
    "        average_loss = 0\n",
    "        \n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        \n",
    "        #initialize the validations out, state, and character\n",
    "        val_state_O = np.zeros(num_nodes).reshape(1,num_nodes)\n",
    "        val_output_O = np.zeros(num_nodes).reshape(1,num_nodes)\n",
    "        \n",
    "        char_id = sample_prob(random_dist()) #create a random distribution then sample\n",
    "        #val_input_O = char2vec(id2char(char_id)).reshape(1,alphabet_size)\n",
    "        val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict = {val_input: val_input_O, \n",
    "                         val_output: val_output_O, \n",
    "                         val_state: val_state_O}\n",
    "            val_output_O,val_state_O,dist = sess.run([val_output_after,val_state_after,val_probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "            s+=id2char(char_id)\n",
    "        print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-71-f95a037314c8>:85 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes_input = 50\n",
    "num_nodes_rec = 50\n",
    "num_nodes_final = 100\n",
    "num_unrollings = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    train = list()\n",
    "    for i in range(num_unrollings):\n",
    "        train.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "    \n",
    "    #the previous output the gets fed into the cell\n",
    "    output_feed= tf.placeholder(tf.float32,shape=(batch_size,num_nodes_final),name='one')\n",
    "    \n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    labels = list()\n",
    "    for i in range(num_unrollings):\n",
    "        labels.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "        \n",
    "    \n",
    "    #validation place holder\n",
    "    val_input = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    val_output = tf.placeholder(tf.float32,shape=(1,num_nodes_final))\n",
    "    \n",
    "    \n",
    "    #Variables\n",
    "    #input matrix\n",
    "    W_input = tf.Variable(tf.truncated_normal([alphabet_size,num_nodes_input],-0.1,0.1))\n",
    "    b_input = tf.Variable(tf.zeros([1,num_nodes_input]))\n",
    "    \n",
    "    #recursive matrix multiplies previous output\n",
    "    W_rec = tf.Variable(tf.truncated_normal([num_nodes_final,num_nodes_rec],-0.1,0.1))\n",
    "    b_rec = tf.Variable(tf.zeros([1,num_nodes_rec]))\n",
    "    \n",
    "    #output matrix before softmax\n",
    "    W_final = tf.Variable(tf.truncated_normal([num_nodes_input+num_nodes_rec,num_nodes_final],-0.1,0.1))\n",
    "    b_final = tf.Variable(tf.zeros([1,num_nodes_final]))\n",
    "    \n",
    "    #softmax\n",
    "    W_softmax = tf.Variable(tf.truncated_normal([num_nodes_final,alphabet_size],-0.1,0.1))\n",
    "    b_softmax = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def RNN(i,o):\n",
    "        c_input = tf.nn.sigmoid(tf.matmul(i,W_input)+b_input)\n",
    "        c_rec = tf.nn.sigmoid(tf.matmul(o,W_rec)+b_rec)\n",
    "        c_final = tf.nn.tanh(tf.matmul(tf.concat(1,[c_input,c_rec]),W_final)+b_final)\n",
    "        return c_final\n",
    "    \n",
    "    \n",
    "    #when training truncate the gradients\n",
    "    for i in range(num_unrollings):\n",
    "        if i == 0:\n",
    "            outputs = list()\n",
    "            output_after = RNN(train[i],output_feed)\n",
    "        else:\n",
    "            output_after = RNN(train[i],output)\n",
    "        output = output_after\n",
    "        outputs.append(output)\n",
    "    \n",
    "    #train\n",
    "    #loss\n",
    "    logits = tf.matmul(tf.concat(0,outputs),W_softmax)+b_softmax\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,tf.concat(0,labels)))\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "#     opt = tf.train.GradientDescentOptimizer(learning_rate=.001).minimize(loss)\n",
    "#     opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\n",
    "    gradients=optimizer.compute_gradients(loss)\n",
    "    gradients_clipped = [(tf.clip_by_value(grad,clip_value_min=-1.,clip_value_max=1.),var) for grad,var in gradients]\n",
    "    opt=optimizer.apply_gradients(gradients_clipped)\n",
    "    \n",
    "    # Validation\n",
    "    val_output_after = RNN(val_input,val_output) #change train to input_in\n",
    "    val_logits = tf.matmul(val_output_after,W_softmax)+b_softmax\n",
    "    val_probs = tf.nn.softmax(val_logits)\n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW EPOCH\n",
      "0.00378063726425\n",
      "jrppvgimymmlzdmmddnhnpbbpiiyvifudspdiefpzpmaxvepzuw rfdlpjbcobdoabvpvrzmwirhvrahhmpeyaanxbmdrgiumgoaq\n",
      "2.99322602367\n",
      "xmne  thzhysuoyckfidi  nl iaiijoylnplgtape lncoapnois vkeplavnoipaarasapad  ta plceeaetkapsr sestsokn\n",
      "2.86842941189\n",
      " irrtane l etp sshnmitsi beielnnno xlssnarjr earnrrwr ton  ahfndeem uaimhdu vwcnruuihzsehkeoo utu lft\n",
      "2.85040369606\n",
      "csdloihecsaupdtmpdtadfctn masa th nno navi rbwnvao cchilreeossou  neen chh  ftao rwccfamp lolaechithc\n",
      "2.84867376828\n",
      "xoqon  aedndaahopimwtoeehnea nttppvcnm aeur h eena aruime ci eiasg aufixedcitnsn ioaacnarhanslpdfpauq\n",
      "2.85351779914\n",
      "dtyfoap tors aa  vemseu ewaut  jse  mrea mittastrliea z oeut tbyro tvohmo i estdclueigdforhnk p tiih \n",
      "2.86648564625\n",
      "c paefnag gstcelgohsuhg cse eegrkeaen ser ni    genomcsacdnienpaealrmn lfnesokalasspe sineeorhtlr    \n",
      "2.84553366756\n",
      "d wad ttgsodnciief l g etsmayuthcssnp s  lns dt phuot  sbncah  coavirsianiffed il mrenfpirhhyebtanhwn\n",
      "2.84390960336\n",
      "dvnwasl wlpiod rcsti ielttdvn ht lrgwh eanhtvalasbutleidtseoalein s o ancraahcmtialnl oi tdeo i t ssd\n",
      "2.84986441827\n",
      "ipdee tt ilefi eaesfcyss  nlnndnn  rovhh elnmseuar tt gt srya er lnltsn lh nedlceo    eorew osulunric\n",
      "2.84433007646\n",
      "jislg vssnthtsu tsnreiamoaehlibs hc auehaar qrth rot oetpe gmoaaunlwyeaealhobn rnda qbits  aitdihcr h\n",
      "2.81413958859\n",
      "b ttaa leot cceotdi dmdrhs eden e tsaidzettbo nonnxi trut litv nei  al dnc yent  csetsttuemnttgbliuix\n",
      "2.84801300502\n",
      "cen erf  o ahnds  ttaobl wlcle  ycttdaiuwoe nhli  eehdonaombe aoke lelreuuw  afzb tulof rnralhr awocl\n",
      "2.85424426317\n",
      "m oscenhpddsgspbt ree sghhfityncns li  el o  owocccbm rog voi ruu fnois sealnvih oiaiieastieurdcorns \n",
      "2.85491948009\n",
      "foncnehiithnsnce santghiedsan  nrcst  ehosni otha dalkrhtdl iadibae ysl snhibrnhnifcbeaicwdia pfen   \n",
      "2.85111884475\n",
      "msaewedacaa fw nlnii etoeifitwrhcsn n d iefim es iu eeearosetg nlad  yseooh daatr iattnpsbcdndt mghft\n",
      "2.84023721862\n",
      "hghyee eys seg aseesn nesa oh eranu  ns   yhesseicat ssnint v   nx hlp cerlgexnn io rho thpenhandrlea\n",
      "2.8372109015\n",
      "usobreuenlsileh mcceaeer dgvwin iooe laehnl  hhllohneul  doatnstnemh nt arasesl hjo n ad eh   u llne \n",
      "2.85383971786\n",
      "fsd  dpdr ikaoeshno srdiianatenrnnitdgrscoptt lo tnyfn dtothnoreir g issaywoseteggehi sf  wf cnr oefr\n",
      "2.84309356332\n",
      "loeerds r bilpt  omgtrinle hgelr ro norprh etor saeisb oi orrihra itnfossu frmana shmaetaoecnrmass ha\n",
      "2.85940803599\n",
      "ytes fere dservh tnmnnsifeoiddnasgnasb taise yhl  ato asoun theddhriost sriinre estrrbrtlatitwh s irs\n",
      "2.85114702702\n",
      "wotaeaotoithe otaoet usdo   tievsinetyceieiaeemdlwdn  asv os ostvrru tr iwentleeonrr ll eisxy ne m pn\n",
      "2.83348437905\n",
      "jfolse cstlsooaettk tser eueoiiit oataef i rn dbt  ejt hstedts  alteadsursn boce cfgt ans  afmsoga sa\n",
      "2.83235422635\n",
      "eeowussmes  u  eiswh   raa  seho yefisa  pasnly  idde hrsilitda t o ello o  sg tntp ios rn i lue w  e\n",
      "2.81659138012\n",
      "cswro l ens e  sacel rd nefeeviotwt lone   sneoenmrmot om ia t s rnf n  e meetmtsnsecvprd atf  esas o\n",
      "2.84338764763\n",
      "thih    olrhinhf nriynr fo toa arnnedohue mle rvrlupdu cd  pa vdcis hgtnnnn y rx s he ehaneum b a l e\n",
      "2.8435768714\n",
      "ye waahwi escaer h eaeot  eeruri ontiue r baos o  as  lhtueai otjshsc fode eeaelieeatds gjiihtla tnrf\n",
      "2.85392257428\n",
      "itr tngecuebi mra aswniumreu irsydttuwtrth hmtleeriheaewth ttirattool   yy amcemaowraa apleakpanteeaa\n",
      "2.83985976458\n",
      "s noattids  adnflervlmtyiou csaytiwrmy  ciww geefnaie  esnhdmovsthn bheseur ttutt naa he  h crvu o la\n",
      "2.85870722103\n",
      "m  cwslh lnan  ishhss slanaiao  tvm eeerad nso spiamfsepcecoallakouo aylirle w u yvlpi ifnlmebli ugsa\n",
      "2.87105358219\n",
      "vnc   o oseyxefuurswode p vnhpsan tc n anntaorizionnlsanascacn   tfe oelr usf betogyutglpaaonne lrfsg\n",
      "2.84284036589\n",
      " vohggdaacotrteostegeywtird a  dtt rofsg  cxrwpf reeuwt r doto d va prbidilforg e tviytl tt nnrme aro\n",
      "2.8528415041\n",
      "llcnrar    aafleemnn alin laiusm n aasebobaesrpbngp c fttamrneael ite puaorplpmerlcoaen kreenotrdbrnc\n",
      "2.84370697117\n",
      "tdd fwoo e i g olsdb cz phfe   nnr  i hlilreee e ih ata  enscn afooonolnrfr hsstfooeoltefnatmd st noh\n",
      "2.84164684987\n",
      "o iimiim iaconse odtteisfbronifvca czea e dmy  mmd a  utnevycrsoeswtntniyorsoetieo aci  nh tshf i  wd\n",
      "2.83256310153\n",
      "ni piiggt dyp nddeeeyph eaje o ei tt ref andctfoett lt ms ersdhheeedaioahat maalrl oeefalud ctliaos a\n",
      "2.82911073661\n",
      "pvrht lk lnelksc i lneo o hfd taetotf htf tcnh  asliose alahrmgt oodetgi  adr ienrlt  untlmphrhnipapi\n",
      "2.82705251122\n",
      "ytssnrasg sadnfkhy adttehnh otheer dirplne oweimenei hsetei eerntn yaet fustseuipcraarydn an g hss ad\n",
      "2.85379183292\n",
      "kunuod gowehdanlsb t  yia  sdepmn svil  sggr twrhs hrgptmelao ao eju  fnneas n vsiklfppohld  cmest lt\n",
      "2.8174858489\n",
      "upltatrocrne nj a srot  ndlsne rerriaahttn hr lochihpienri oct lisaieotu neheh lnadidnrla mtta aoetot\n",
      "2.83694489384\n",
      "dsne   shm  dc ulones atloe icaa elcoanasgrci oo eogh hd e hdl  u va r erwtte issdwn c  cr vciigeenth\n",
      "2.8810424633\n",
      "eorc tunniho rcaiontsietonebo cgrn aichnfxmies kf rneeli hrta crak euoco chgt ide a   dnsmltnfkns n u\n",
      "2.87353798056\n",
      "rlohnpefeeztc ehepio o amooe fffe  egnclsghiceneggt saap hgcrsehpc iefa taieopgaeee soa patloitisiiin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-0b9b5b9959aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malphabet_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0moutput_pass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_after\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/floptical/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps=50001\n",
    "batch_size=1\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    batch_x,batch_y=getWindowBatch(b,num_unrollings) \n",
    "    if step%b.text_size == 0:\n",
    "        print \"NEW EPOCH\"\n",
    "    if step == 0: #initialize the output state vectors\n",
    "        output_pass = np.zeros([batch_size,num_nodes_final],dtype=np.float32)\n",
    "    feed_dict={output_feed: output_pass}\n",
    "    \n",
    "    #trains x\n",
    "    x = map(char2vec,batch_x)\n",
    "    for i in range(num_unrollings):\n",
    "        x[i]\n",
    "        feed_dict[train[i]] = x[i].reshape([1,alphabet_size])\n",
    "    \n",
    "    #trains y\n",
    "    y = map(char2vec,batch_y)\n",
    "    for i in range(num_unrollings):\n",
    "        y[i]\n",
    "        feed_dict[labels[i]] = y[i].reshape([1,alphabet_size])\n",
    "    \n",
    "    output_pass,l,_=sess.run([output_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print average_loss/1000\n",
    "        average_loss = 0\n",
    "        \n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        \n",
    "        #initialize the validations out, state, and character\n",
    "        val_output_O = np.zeros(num_nodes_final).reshape(1,num_nodes_final)\n",
    "        \n",
    "        char_id = sample_prob(random_dist()) #create a random distribution then sample\n",
    "        val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict = {val_input: val_input_O, \n",
    "                         val_output: val_output_O}\n",
    "            val_output_O,dist = sess.run([val_output_after,val_probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "            s+=id2char(char_id)\n",
    "        print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed up learning by using larger batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Batch(object):\n",
    "    def __init__(self,text,batch_size):\n",
    "        self.text = text\n",
    "        self.text_size = len(text)\n",
    "        self.segment_size = self.text_size//batch_size\n",
    "        self.cursors = [self.segment_size*b for b in range(batch_size)]\n",
    "    \n",
    "    def next(self):\n",
    "        self.cursors = [(c + 1) % self.text_size for c in self.cursors]\n",
    "        x,y = [self.text[c] for c in self.cursors], [self.text[(c+1)%self.text_size] for c in self.cursors]\n",
    "        return x,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=3\n",
    "b = Batch(text,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['a', 'a', 'v'], ['n', 'r', 'a'])\n",
      "(['n', 'r', 'a'], ['a', 'i', 'l'])\n",
      "(['a', 'i', 'l'], ['r', 'o', 'u'])\n",
      "(['r', 'o', 'u'], ['c', 'u', 'a'])\n",
      "(['c', 'u', 'a'], ['h', 's', 'b'])\n",
      "(['h', 's', 'b'], ['i', ' ', 'l'])\n",
      "(['i', ' ', 'l'], ['s', 'i', 'e'])\n",
      "(['s', 'i', 'e'], ['m', 'n', ' '])\n",
      "(['m', 'n', ' '], [' ', 't', 'c'])\n",
      "([' ', 't', 'c'], ['o', 'e', 'o'])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print b.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = b.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['o', 'e', 'o'], ['r', 'r', 'm'])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'e', 'o']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x,batch_y=getWindowBatch(b,num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'f', 'm'],\n",
       " ['s', 'e', 'a'],\n",
       " [' ', 'c', 'r'],\n",
       " ['a', 't', 'e'],\n",
       " [' ', 's', ' '],\n",
       " ['t', ' ', 'o'],\n",
       " ['e', 'o', 'u'],\n",
       " ['r', 'n', 't'],\n",
       " ['m', ' ', ' '],\n",
       " [' ', 't', 'o']]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'e', 'p'],\n",
       " ['g', 's', 'e'],\n",
       " ['i', 't', 't'],\n",
       " ['n', 'i', 'i'],\n",
       " ['a', 'n', 't'],\n",
       " ['t', 'g', 'i'],\n",
       " ['e', ' ', 'o'],\n",
       " ['d', 'e', 'n'],\n",
       " [' ', 'f', ' '],\n",
       " ['a', 'f', 'm']]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ba in range(batch_size):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mega_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0], dtype=int8)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2vec(batch_x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unrollings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mega_batch_x = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "for n in range(num_unrollings):\n",
    "    batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "    for ba in range(batch_size):\n",
    "        batch[ba]=char2vec(batch_x[n][ba])\n",
    "    mega_batch_x.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mega_batch_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mega_batch_y = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "for n in range(num_unrollings):\n",
    "    batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "    for ba in range(batch_size):\n",
    "        batch[ba]=char2vec(batch_y[n][ba])\n",
    "    mega_batch_y.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mega_batch_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mega_batch_x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they match which is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-156-9940e2c7f03d>:93 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes_input = 50\n",
    "num_nodes_rec = 50\n",
    "num_nodes_final = 100\n",
    "num_unrollings = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    train = list()\n",
    "    for i in range(num_unrollings):\n",
    "        train.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "    \n",
    "    #the previous output the gets fed into the cell\n",
    "    output_feed= tf.placeholder(tf.float32,shape=(batch_size,num_nodes_final),name='one')\n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    labels = list()\n",
    "    for i in range(num_unrollings):\n",
    "        labels.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "        \n",
    "    \n",
    "    #validation place holder\n",
    "    val_input = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    val_output = tf.placeholder(tf.float32,shape=(1,num_nodes_final))\n",
    "    \n",
    "    \n",
    "    #Variables\n",
    "    #input matrix\n",
    "    W_input = tf.Variable(tf.truncated_normal([alphabet_size,num_nodes_input],-0.1,0.1))\n",
    "    b_input = tf.Variable(tf.zeros([1,num_nodes_input]))\n",
    "    \n",
    "    #recursive matrix multiplies previous output\n",
    "    W_rec = tf.Variable(tf.truncated_normal([num_nodes_final,num_nodes_rec],-0.1,0.1))\n",
    "    b_rec = tf.Variable(tf.zeros([1,num_nodes_rec]))\n",
    "    \n",
    "    #output matrix before softmax\n",
    "    W_final = tf.Variable(tf.truncated_normal([num_nodes_input+num_nodes_rec,num_nodes_final],-0.1,0.1))\n",
    "    b_final = tf.Variable(tf.zeros([1,num_nodes_final]))\n",
    "    \n",
    "    #softmax\n",
    "    W_softmax = tf.Variable(tf.truncated_normal([num_nodes_final,alphabet_size],-0.1,0.1))\n",
    "    b_softmax = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def RNN(i,o):\n",
    "#         c_input = tf.nn.sigmoid(tf.matmul(i,W_input)+b_input)\n",
    "#         c_rec = tf.nn.sigmoid(tf.matmul(o,W_rec)+b_rec)\n",
    "        c_input = tf.matmul(i,W_input)+b_input\n",
    "        c_rec = tf.matmul(o,W_rec)+b_rec\n",
    "        c_final = tf.nn.tanh(tf.matmul(tf.concat(1,[c_input,c_rec]),W_final)+b_final)\n",
    "        return c_final\n",
    "    \n",
    "    \n",
    "    #when training truncate the gradients\n",
    "    for i in range(num_unrollings):\n",
    "        if i == 0:\n",
    "            outputs = list()\n",
    "            output_after = RNN(train[i],output_feed)\n",
    "        else:\n",
    "            output_after = RNN(train[i],output)\n",
    "        output = output_after\n",
    "        outputs.append(output)\n",
    "    \n",
    "    #train\n",
    "    #loss\n",
    "    logits = tf.matmul(tf.concat(0,outputs),W_softmax)+b_softmax\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,tf.concat(0,labels)))\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "#     opt = tf.train.GradientDescentOptimizer(learning_rate=.001).minimize(loss)\n",
    "#     opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\n",
    "    gradients,var=zip(*optimizer.compute_gradients(loss))\n",
    "#     gradients_clipped = [(tf.clip_by_value(grad,clip_value_min=-1.,clip_value_max=1.),var) for grad,var in gradients]\n",
    "    gradients_clipped, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "#     opt=optimizer.apply_gradients(gradients_clipped)\n",
    "    opt=optimizer.apply_gradients(zip(gradients_clipped,var),global_step=global_step)\n",
    "    \n",
    "    # Validation\n",
    "    val_output_after = RNN(val_input,val_output) #change train to input_in\n",
    "    val_logits = tf.matmul(val_output_after,W_softmax)+b_softmax\n",
    "    val_probs = tf.nn.softmax(val_logits)\n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss:  0.00401857757568\n",
      "Learning rate:  10.0\n",
      "k                                                                                                    \n",
      "Average loss:  55.0183110218\n",
      "Learning rate:  10.0\n",
      "o                                                                                                    \n",
      "Average loss:  54.6480228424\n",
      "Learning rate:  10.0\n",
      "j                                                                                                    \n",
      "Average loss:  54.6805970325\n",
      "Learning rate:  10.0\n",
      "ylllllllllallalllllllllllllllllllllllllllllllllllllallllllalllllllllllllllllllllllllallllllllllllllla\n",
      "Average loss:  54.535797039\n",
      "Learning rate:  10.0\n",
      "fhmhhhmmnnmmnmhmnnnhmnhmmnmnhnmhmnnhhmmmhhnnnmnhhmhhmmnhnmmmmnnmngnnmemnmnhmnnnhhhnhnnmmnmmmnnhnhhnmn\n",
      "Average loss:  54.5390685921\n",
      "Learning rate:  1.0\n",
      "legegngeeeggeeeblnneeegeneeenegenbegeegelgeeeeegnenenengeeelngeeeeegegngeggngeebeeggneeegenegegenneee\n",
      "Average loss:  5.38481104255\n",
      "Learning rate:  1.0\n",
      "mbinndilegeseeineeeeeeaeyniieeeecieeideeihweeeewec eeeeeeeeebseeeieennobeeneueeeeennep naniehumsciess\n",
      "Average loss:  4.69182119918\n",
      "Learning rate:  1.0\n",
      "neseeesesneseufeeeislssskeeeesnsilerieneerireiysiseeueseygeeinereainessreieersrsesrnsueryieseuelenese\n",
      "Average loss:  4.69795924044\n",
      "Learning rate:  1.0\n",
      "vp      r      fjd    n       s         d      g           ns   s  n              n f   n   b s    f \n",
      "Average loss:  4.727632231\n",
      "Learning rate:  1.0\n",
      "wyeeveeytteeegtheeeaeeurelereempeetuiimrzfresieeebtgeeetzeeeeeeeieesertmdsemtierveeeeeutrrtseeeeeiree\n",
      "Average loss:  4.69711676645\n",
      "Learning rate:  0.1\n",
      "fiannnatgmvattnphthbktbiiacigmypabatatiitsaatihbtmtuuaiueddtaiyuhmtaninihaohtlniiaifetanttgxcandgnfyn\n",
      "Average loss:  2.87248352647\n",
      "Learning rate:  0.1\n",
      "mh a hvotbreocc d egewiztndeyveurh slnhenbhrriseaahe aeoebonnmbr iivdc direr  yeah iveeo ivhoefsg lie\n",
      "Average loss:  2.86703247046\n",
      "Learning rate:  0.1\n",
      "nipee dnstuaoscstwtosewbsnrnaam  nsaashasatsaeeoepif  ltkan mrcfpyaomn nei y chgrnimnhcl n deilehviow\n",
      "Average loss:  2.86594680715\n",
      "Learning rate:  0.1\n",
      "one nenrget yknsdts  tne osneiiae delehieexe w  zmowa tibs   ioeamyapnlnawh otcosnl unegnisrrn eeunii\n",
      "Average loss:  2.8632589891\n",
      "Learning rate:  0.1\n",
      "n  i em iaamelonooi dypibr pi tbir aerltrnttb hrimewuenul no  szsr  ga  au  o rsrdrv fstom  iochotoih\n",
      "Average loss:  2.86277505565\n",
      "Learning rate:  0.01\n",
      "t resrhli tnacheenhf gisuhootuny  eainsh hgibhoginmtntshhamytnr lc nrfdiige rccu soooeiiee de n eutik\n",
      "Average loss:  2.85882177019\n",
      "Learning rate:  0.01\n",
      " bi  rrwcd htsaodadnarirle  hhds nrr o nirtesldor oi el ta ima tf lmn nh i aht  lems  hp  et rtoe e  \n",
      "Average loss:  2.85922462225\n",
      "Learning rate:  0.01\n",
      "vzbm enscsee pom th aesm epe oihe w opr oee l aletcahast r f dr eecr ccteipmsshemjiyany  he  eltoib o\n",
      "Average loss:  2.8576789453\n",
      "Learning rate:  0.01\n",
      "yrlmphoneontrrn to anieaeonulgwswzmlagoo rlpt olrihrscceeyde crc re ctyu e ednni  a a ideuhtou neeadw\n",
      "Average loss:  2.85573905754\n",
      "Learning rate:  0.01\n",
      "es od eef ogwefnohaarapcnyt awyolfmtav rnrlssr i a rd wh sigaoahdtoad gzw enas ucehhrk  iittheifabmil\n",
      "Average loss:  2.85848614264\n",
      "Learning rate:  0.001\n",
      "vgiee wds  sd  frtsltare eo dwa s tsae  pn elwftarlt uhp nfoet an dseanfieosd sds redc almtandgfnaeeh\n",
      "Average loss:  2.8562712841\n",
      "Learning rate:  0.001\n",
      "jsrsoerltorsactaee s b v uetcnen g tlnlfs ntns n   eletlmo  idanphipwst tenttedetebea   twr  ailhci t\n",
      "Average loss:  2.8549373126\n",
      "Learning rate:  0.001\n",
      "q eiidmpeggpanmdo krgdeneuihtmi onff da  hnlaeb ltctoial orit fvflacncst rwna attblm nptmadoeihbhfebe\n",
      "Average loss:  2.85718628502\n",
      "Learning rate:  0.001\n",
      " c nr dlxersc nu arnwpt rwtnepiia yisinothmeriahothknhafneefmltipin steotbclhfeeai rtigsla es oli  ni\n",
      "Average loss:  2.85651082802\n",
      "Learning rate:  0.001\n",
      "xjilehzytfrn r arewn rlibtfmtbgieiarno traeh arllean n n hit  ifssntooh aaoidhaaistagnewte  ie isjuen\n",
      "Average loss:  2.85644932389\n",
      "Learning rate:  0.0001\n",
      "roncoensaeenco wt rlshie bao fitn i gufsffo t   aqpmefiarchodaadvssaahrnoa ylroyoo o teeo eu cnfaobeo\n",
      "Average loss:  2.85719205236\n",
      "Learning rate:  0.0001\n",
      "owwapaahts o  esahrs ehc anwifyatsaeew enlnlohcd o uiattrl opnesaeetncc t  eoayoloseiw tnoaelioeiabhf\n",
      "Average loss:  2.85733001566\n",
      "Learning rate:  0.0001\n",
      "yo cleosyrgaoiddiennabevoi oepd  o oichkacoritoes ltlkathaninl iwcneavnrseoriu uii i dcn nr  toe oceo\n",
      "Average loss:  2.85503441095\n",
      "Learning rate:  0.0001\n",
      "o eh du e ryr rgwa osoehehex h u eaaacteeehoapnldhe    hrnscoareot afidiu o enpntito   n eecmuvfldf e\n",
      "Average loss:  2.8537906673\n",
      "Learning rate:  0.0001\n",
      "c  eyuaasnobssce rppl i hrtchnee t ethsoo itrovmrceynucopeledopl asatle l ufgeiri fef   rtafc useheoo\n",
      "Average loss:  2.85615442896\n",
      "Learning rate:  1e-05\n",
      "istee roeen vhi r doatr falghe aheoo el  ta e ngihs dtneea ecwdt  gelyeefed  e gefl e stwrnzznfmsn li\n",
      "Average loss:  2.85477080345\n",
      "Learning rate:  1e-05\n",
      "jwrsy oya saveman te ve  cnifina uaie hi aabhit setlvrnee oe smroti   rjeaiera gretdwmeritan ipeaawrf\n",
      "Average loss:  2.85482525063\n",
      "Learning rate:  1e-05\n",
      "ci etneaairescdaoritoiaimotr mtozis h s ceets nrhinii  rdodca ae eg ta sngoc  liiadltiiesmeedean  ird\n",
      "Average loss:  2.85410851526\n",
      "Learning rate:  1e-05\n",
      "htocneiisctehsr io i atet  oaoti rcisle  vfoaitusiftavx n senarp  mrs lgrhnrallm sah eehs  r moe aiwe\n",
      "Average loss:  2.85205314255\n",
      "Learning rate:  1e-05\n",
      "tvsfhwgblls  e  r cbic w nenownt  eevraeswdtlooenojhop cttih daoociiot it douzinhmnnoe  eeu tsaehemsf\n",
      "Average loss:  2.85673786545\n",
      "Learning rate:  1e-06\n",
      "dr igkda oeaijrnf ytnumvouneicsoos vhooxialetfnuhrts nssfa aleiafrhtt fwssrhdam a oe  e sbdo rt tsprg\n",
      "Average loss:  2.85668739057\n",
      "Learning rate:  1e-06\n",
      "gf tenistnerii lheaidyel aihscetogf noiashdmr  iut  deoieh itisnrves ontoanioaeuh  nekblcdetsf tazatt\n",
      "Average loss:  2.85706393003\n",
      "Learning rate:  1e-06\n",
      "cqnpm  rfinl oux eaii h  lgotenraaer snecea osts e u s oaiaphmgi hnotirt teipdhlsos  eoa  rzannpyam  \n",
      "Average loss:  2.85595782042\n",
      "Learning rate:  1e-06\n",
      "xuoc tbvm e utciasaeoaanitrunreceia ewsrzp go ei  jesp rn  zuiatio a   rwdeacfethuisoacteeopide  a a \n",
      "Average loss:  2.8579226203\n",
      "Learning rate:  1e-06\n",
      "itp enflxriooadletf rsoi deayifrmie  lrse wa inn oo i mnoagn cga nfi adrh me afar o ac avc ilnt i rcc\n",
      "Average loss:  2.85469387746\n",
      "Learning rate:  1e-07\n",
      "xzt mca   ni erictsshvsdgo snhjmdl   e   nlcttlrin blh t p msb  rradnecaihatna g dtdahett akiom  hb  \n",
      "Average loss:  2.8585257256\n",
      "Learning rate:  1e-07\n",
      "oy mo niarlenadtnnserwniesoo rsb hui p  t ibconiiur   fpe yshhovsaoe hatc  sslt  itdioelz pesemsn red\n",
      "Average loss:  2.85811604548\n",
      "Learning rate:  1e-07\n",
      "gt tlr htilrybsotsfritosai  rye tissntw  bttt  tshd inaeavicfs dnsnnl snlrucdbtsilnouoah esrg rilnnpl\n",
      "Average loss:  2.85608854651\n",
      "Learning rate:  1e-07\n",
      "wlubynriebdlh onosnwe eurrod ohs ts edih ehehuehoieaaeirtco lrif naypu lndnelinbtm wpcntbalegsstt i u\n",
      "Average loss:  2.85646507359\n",
      "Learning rate:  1e-07\n",
      "leteetirieethmoie emmfaemfitehh vy pgraltoiv gria e fasaenr a cf the tdidisnd aenii arcbn oodn up rtm\n",
      "Average loss:  2.85599145365\n",
      "Learning rate:  1e-08\n",
      "gs oa kqtlear h l d  t le  s esuw lr led etedsahsasttotm rmhfes oy enrl pnos crowfslniloboosa zfnirto\n",
      "Average loss:  2.85598709249\n",
      "Learning rate:  1e-08\n",
      "xilauiz e   c e  oiaheyt   g endren m uii ea twd ov ueghtteo m invrac s  rjlwa ztse noee etsan hneni \n",
      "Average loss:  2.85628132772\n",
      "Learning rate:  1e-08\n",
      "amsg cht roectos ivaahm pcaehlhy tnh i grihh uigrt paori hnriampuph tl tbtmdt r  hup ee igt fien etro\n",
      "Average loss:  2.8566867795\n",
      "Learning rate:  1e-08\n",
      "wzt   esoerorsiiebkn n rrsniw crn rimtea  aroeon nnieaicisorlrlpnitrm itn tiicnswifhl egalmipeakayl  \n",
      "Average loss:  2.85777712369\n",
      "Learning rate:  1e-08\n",
      "mllvr c ilt dr   i nionuoaeein aruush tl esgaarsf tmflfjawr iabt  ie dn atgiin  ocps nnson ieeoosgueg\n",
      "Average loss:  2.8611014955\n",
      "Learning rate:  1e-09\n",
      "xlginr tlt   wsesrssidt re don anayioacert lneceeftgolta yhtuh rsvc t syoie  eo eeucd dol  bl ei nnea\n"
     ]
    }
   ],
   "source": [
    "num_steps=50001\n",
    "batch_size=50\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    #get the new inputs and labels\n",
    "    batch_x,batch_y=getWindowBatch(b,num_unrollings)\n",
    "    \n",
    "#     if step%b.text_size == 0:\n",
    "#         print \"NEW EPOCH\"\n",
    "\n",
    "    #initialize the output\n",
    "    if step == 0: #initialize the output state vectors\n",
    "        output_pass = np.zeros([batch_size,num_nodes_final],dtype=np.float32)\n",
    "    feed_dict={output_feed: output_pass}\n",
    "    \n",
    "    #trains x\n",
    "    mega_batch_x = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_x[n][ba])\n",
    "        mega_batch_x.append(batch)\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[train[i]] = mega_batch_x[i]\n",
    "    \n",
    "    #trains y\n",
    "    mega_batch_y = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_y[n][ba])\n",
    "        mega_batch_y.append(batch)\n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[labels[i]] = mega_batch_y[i]\n",
    "    \n",
    "    output_pass,l,_=sess.run([output_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print 'Average loss: ',str(average_loss/1000)\n",
    "        average_loss = 0\n",
    "        \n",
    "        \n",
    "        print 'Learning rate: ', str(learning_rate.eval(session=sess))\n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        \n",
    "        #initialize the validations out, state, and character\n",
    "        val_output_O = np.zeros(num_nodes_final).reshape(1,num_nodes_final)\n",
    "        \n",
    "        char_id = sample_prob(random_dist()) #create a random distribution then sample\n",
    "        val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict = {val_input: val_input_O, \n",
    "                         val_output: val_output_O}\n",
    "            val_output_O,dist = sess.run([val_output_after,val_probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "            s+=id2char(char_id)\n",
    "        print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Goodfellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-246-cf14e3c7232e>:96 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes = 50\n",
    "num_unrollings = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    train = list()\n",
    "    for i in range(num_unrollings):\n",
    "        train.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "    \n",
    "    #the previous output the gets fed into the cell\n",
    "    output_feed= tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='one')\n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    labels = list()\n",
    "    for i in range(num_unrollings):\n",
    "        labels.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "        \n",
    "    \n",
    "    #validation place holder\n",
    "    val_input = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    val_output = tf.placeholder(tf.float32,shape=(1,num_nodes))\n",
    "    \n",
    "    \n",
    "    #Variables\n",
    "    #input matrix\n",
    "    U = tf.Variable(tf.truncated_normal([alphabet_size,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #recursive matrix multiplies previous output\n",
    "    W = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #bias vector\n",
    "    b = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #output matrix\n",
    "    V = tf.Variable(tf.truncated_normal([num_nodes,alphabet_size],-0.1,0.1))\n",
    "    c = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def RNN(i,h_input):\n",
    "        a = tf.matmul(i,U)+tf.matmul(h_input,W) #+b\n",
    "        h_output = tf.nn.tanh(a)\n",
    "        o_out = tf.matmul(h_output,V)+c\n",
    "        return h_output,o_out\n",
    "    \n",
    "    \n",
    "    #when training truncate the gradients\n",
    "    for i in range(num_unrollings):\n",
    "        if i == 0:\n",
    "            outputs = list()\n",
    "            hidden_after,output_after = RNN(train[i],output_feed)\n",
    "        else:\n",
    "            hidden_after,output_after = RNN(train[i],hidden)\n",
    "        hidden = hidden_after\n",
    "        outputs.append(output_after)\n",
    "    \n",
    "    #train\n",
    "    #loss\n",
    "    #should be doing log likelihood\n",
    "    def neg_log_likelihood(outputs,labels):\n",
    "        outputs = tf.concat(0,outputs)\n",
    "        labels = tf.concat(0,labels)\n",
    "        #print tf.reduce_sum(-tf.log(tf.reduce_sum(tf.nn.softmax(outputs)*labels,1)))\n",
    "        return tf.reduce_sum(tf.minimum(-tf.log(tf.reduce_sum(tf.multiply(tf.nn.softmax(outputs),labels),1)),10))\n",
    "        #return tf.reduce_sum(-tf.log(tf.nn.softmax(outputs)*labels))\n",
    "    \n",
    "    loss = neg_log_likelihood(outputs,labels)\n",
    "    \n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.concat(0,outputs),tf.concat(0,labels)))\n",
    "#     logits = tf.matmul(tf.concat(0,outputs),W_softmax)+b_softmax\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,tf.concat(0,labels)))\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "#     opt = tf.train.GradientDescentOptimizer(learning_rate=.001).minimize(loss)\n",
    "#     opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate=1.0,global_step=global_step, decay_steps=10000, decay_rate=0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\n",
    "    gradients,var=zip(*optimizer.compute_gradients(loss))\n",
    "    gradients_clipped, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "#     opt=optimizer.apply_gradients(gradients_clipped)\n",
    "    opt=optimizer.apply_gradients(zip(gradients_clipped,var),global_step=global_step)\n",
    "    \n",
    "    # Validation\n",
    "    val_hidden_after,val_output_after = RNN(val_input,val_output) #change train to input_in\n",
    "    #val_logits = tf.matmul(val_output_after,W_softmax)+b_softmax\n",
    "    val_probs = tf.nn.softmax(val_output_after)\n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss:  1.74181860352\n",
      "Learning rate:  1.0\n",
      "xwysifefon f hvtqrgegb d ncwid f kop lgliuit fivihci t notal fatjfrn d qefivil eafcf fcdofcj t f fjfm\n",
      "Average loss:  1755.38800317\n",
      "Learning rate:  1.0\n",
      "voratw ouge wateo san sabses necaithen of anes gabed licl zfor sus caiu k ghor tste pomeces oasadwoin\n",
      "Average loss:  1073.46149817\n",
      "Learning rate:  1.0\n",
      "goncy hoso a stdulk morneng ann to the isione aralous ave noy bonchaling tho nemed the anaws bulce on\n",
      "Average loss:  1018.07973169\n",
      "Learning rate:  1.0\n",
      "jeaties com two poluigasioded crmation ver beladivilatic for hof the eightat coc poth the actral in g\n",
      "Average loss:  1003.77085699\n",
      "Learning rate:  1.0\n",
      "hot ssom one corpded in poosent at ona peosed neg beaden frument cared world of ensues is the cerven \n",
      "Average loss:  1000.02314819\n",
      "Learning rate:  1.0\n",
      "be a formed phypurth a herseret the worme thail nall splepal an lalilan aspolf theres artistans roman\n",
      "Average loss:  993.702313538\n",
      "Learning rate:  1.0\n",
      "x nine six ti place avr confemblal he cent whoctingung th is lir two tin tearian six cameremonk recui\n",
      "Average loss:  985.234312988\n",
      "Learning rate:  1.0\n",
      "n intersting wornbome ans niticabled accardiwar ats and tore is hendicatries not arukord gyt the nric\n",
      "Average loss:  980.243496094\n",
      "Learning rate:  1.0\n",
      "th ylstanly fais one one faimsos furdar rusmbace in kphild loptored nugn the tabutine obcate in hnve \n",
      "Average loss:  969.139664795\n",
      "Learning rate:  1.0\n",
      "vet and the russrought there epeicn as as terees and subbary freashs dethem ind arth be the sedeverde\n",
      "Average loss:  975.869427612\n",
      "Learning rate:  0.1\n",
      "vers modlates decos bysting ind cams vees tow in tore and companosing iritutingto stmen anrahtimet an\n",
      "Average loss:  902.372052979\n",
      "Learning rate:  0.1\n",
      "ver expersest mantion from mimor sictelater was leks chalary the phas revens to the mox peates one ei\n",
      "Average loss:  909.747643921\n",
      "Learning rate:  0.1\n",
      "quagratas was mumbes that magiis to bangubler for bapusebal revoc mostated the earrard modentarsts at\n",
      "Average loss:  911.657155396\n",
      "Learning rate:  0.1\n",
      "eribe paratige maptantsiatip knorination of kistnolovired sikalitiors strinds s nubjed playliome notr\n",
      "Average loss:  914.067802185\n",
      "Learning rate:  0.1\n",
      "ed in sn can defferigh a can faid one seven cout puard in the ming tritinic of zero undie eneaviside \n",
      "Average loss:  907.340288818\n",
      "Learning rate:  0.1\n",
      "or lnuns foleme sfrring one one threen severy onaligs who ongarmanusce counagnovity interving covuana\n",
      "Average loss:  899.513652222\n",
      "Learning rate:  0.1\n",
      "silation forled one foloon acconehero name bisine chormed toduciritime the two diti chew to to part w\n",
      "Average loss:  904.641938782\n",
      "Learning rate:  0.1\n",
      "kinis cossuscute s wikes erarguing the ssive issicamely prangic frraptix the orian and smal mottern a\n",
      "Average loss:  906.317715637\n",
      "Learning rate:  0.1\n",
      "jonudes the dovernards and newabluak the germadi lert tep as or as they s and brila of siman of jalon\n",
      "Average loss:  898.233400024\n",
      "Learning rate:  0.1\n",
      "quciding stially usurow a pare mele afta call dis kat or hancent absents nerghard arding nent d commo\n",
      "Average loss:  905.556837708\n",
      "Learning rate:  0.01\n",
      "cone also three pool mosheminan reperar and on logwn ameriex the mainillands there its of expecictory\n",
      "Average loss:  913.738755188\n",
      "Learning rate:  0.01\n",
      "mish six five gucropthy one fesice fuitory of walatacter carned congynention proherstion state one na\n",
      "Average loss:  899.691818726\n",
      "Learning rate:  0.01\n",
      "y fricqy the arled si butded twboond s the end noter analionak could the featlatews rusprealle l posi\n",
      "Average loss:  908.515995728\n",
      "Learning rate:  0.01\n",
      "quouet sial and therymale s reven ouchodo rats trithiness wore uses a fend syure on two zero zero min\n",
      "Average loss:  909.832747498\n",
      "Learning rate:  0.01\n",
      "kes and the countailed natilatiluas id the contruata are to monz compine progeavider axthat after tar\n",
      "Average loss:  905.647872437\n",
      "Learning rate:  0.01\n",
      "holy altwhichrathody if the two threight of the collect deceases olyng the rcecturentory a moded dami\n",
      "Average loss:  909.831513855\n",
      "Learning rate:  0.01\n",
      "rejster tamnitn the defic seroflegress bb there escreen three benferuttoutsity eop trave tilnly speci\n",
      "Average loss:  911.610009155\n",
      "Learning rate:  0.01\n",
      "mosoment emms they enime hin witcon by two of wars jutil actublic make g lake woulds the ceres quast \n",
      "Average loss:  904.079336731\n",
      "Learning rate:  0.01\n",
      "vistunation by oup siment cank of the some carde b gnotmorication and al rogn politler of hrinch conf\n",
      "Average loss:  894.163990417\n",
      "Learning rate:  0.01\n",
      "could which a d arrictincistor in the the alpuistrates useincan the known a soon brering see u land r\n",
      "Average loss:  911.684607849\n",
      "Learning rate:  0.001\n",
      "h any the phan in the spevilal of dusicon is cans of raic yeari moscempea in the rablip esion the pos\n",
      "Average loss:  908.269167786\n",
      "Learning rate:  0.001\n",
      "b to tron demoringout the constely way infrictert the twhhoretion of the gniortion herh agent ho for \n",
      "Average loss:  900.055040161\n",
      "Learning rate:  0.001\n",
      "ning and r locto intent fattients prove the one which in one nine fancuti four nine yormaistiness oni\n",
      "Average loss:  899.366785278\n",
      "Learning rate:  0.001\n",
      "que of the new beeninelreday in bairt of one thosy custicines lirmbrred the mystith banitaen discliou\n",
      "Average loss:  899.752015625\n",
      "Learning rate:  0.001\n",
      "fseal whother here with stacluis lpicce are call ca eutay histants ditish by gende stact six sazish r\n",
      "Average loss:  903.746667847\n",
      "Learning rate:  0.001\n",
      "que d palles of onyolli nanal amero arducculiestripules advies vally in coners and work show nutinal \n",
      "Average loss:  904.426499756\n",
      "Learning rate:  0.001\n",
      "jerical a basant ot natiam engle bearch and old myolail had meric wevendaglicidentically make case th\n",
      "Average loss:  902.677435669\n",
      "Learning rate:  0.001\n",
      "t the emertake on the ligns that s from the lawmialiop afto an are he pocaned the workn to the greegs\n",
      "Average loss:  895.390015686\n",
      "Learning rate:  0.001\n",
      "rrods dets rariemer s lpelferer borly of played numently eston is zero three zero zero zero nine of c\n",
      "Average loss:  897.615647583\n",
      "Learning rate:  0.001\n",
      "uc it obconans eqome tudenqusb one zero nright denered folart betwo two five fhimustive set fir geald\n",
      "Average loss:  899.427069458\n",
      "Learning rate:  0.0001\n",
      "ncan scation also socnine and reilthera speping termary as paysisn the efhen conjudelf a two sime exp\n",
      "Average loss:  900.392390686\n",
      "Learning rate:  0.0001\n",
      "cition but the layuately harted eraditidge four zero three for almanar subys guplabe mitnatlo asterna\n",
      "Average loss:  905.645970825\n",
      "Learning rate:  0.0001\n",
      " a relarmen natme of the bytec and the decould for protrenty e wnoproe ny the stisharand twostrucim t\n",
      "Average loss:  893.927180237\n",
      "Learning rate:  0.0001\n",
      "ants the tom by a pemitich is sea krogneverdiologitannh reterger sysomed cuers uasticion the eet the \n",
      "Average loss:  896.966827026\n",
      "Learning rate:  0.0001\n",
      "ke presental e herward of to the koovemenfa copth ragies emaded by somes other he sittanologlize engl\n",
      "Average loss:  903.610714172\n",
      "Learning rate:  0.0001\n",
      "dey the one threl and yormenc be teries acon s that and of the ven bopore wife to weatance of hiessiv\n",
      "Average loss:  903.905823181\n",
      "Learning rate:  0.0001\n",
      "xanclime othel all deft ajar of knose conit heading op the conokerwite when furfectill works and cuan\n",
      "Average loss:  912.848898926\n",
      "Learning rate:  0.0001\n",
      "jonesar conce beening miever to the from and eahtution ledce are pogrile becases flifair the his and \n",
      "Average loss:  907.149965637\n",
      "Learning rate:  0.0001\n",
      "ja stusi three twatrment five derimeas dad the been but may e basition reaniation porn weith dowever \n",
      "Average loss:  907.804831604\n",
      "Learning rate:  0.0001\n",
      "ical formad whited histe fintod a luting to molates s mmulow rerynitate on copshovered netg therracce\n",
      "Average loss:  905.816061829\n",
      "Learning rate:  1e-05\n",
      "ate helsume ented being grbeth othess achard call his in probjects four ovichfeiln as exfilstus co fi\n"
     ]
    }
   ],
   "source": [
    "num_steps=50001\n",
    "batch_size=50\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    #get the new inputs and labels\n",
    "    batch_x,batch_y=getWindowBatch(b,num_unrollings)\n",
    "    \n",
    "#     if step%b.text_size == 0:\n",
    "#         print \"NEW EPOCH\"\n",
    "\n",
    "    #initialize the output\n",
    "    if step == 0: #initialize the output state vectors\n",
    "        output_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "    feed_dict={output_feed: output_pass}\n",
    "    \n",
    "    #trains x\n",
    "    mega_batch_x = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_x[n][ba])\n",
    "        mega_batch_x.append(batch)\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[train[i]] = mega_batch_x[i]\n",
    "    \n",
    "    #trains y\n",
    "    mega_batch_y = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_y[n][ba])\n",
    "        mega_batch_y.append(batch)\n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[labels[i]] = mega_batch_y[i]\n",
    "    \n",
    "    output_pass,l,_=sess.run([hidden_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print 'Average loss: ',str(average_loss/1000)\n",
    "        average_loss = 0\n",
    "        \n",
    "        \n",
    "        print 'Learning rate: ', str(learning_rate.eval(session=sess))\n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        \n",
    "        #initialize the validations out, state, and character\n",
    "        val_output_O = np.zeros(num_nodes).reshape(1,num_nodes)\n",
    "        \n",
    "        char_id = sample_prob(random_dist()) #create a random distribution then sample\n",
    "        val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict = {val_input: val_input_O, \n",
    "                         val_output: val_output_O}\n",
    "            val_output_O,dist = sess.run([val_hidden_after,val_probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "            s+=id2char(char_id)\n",
    "        print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 4.6051701859880909, 3.912023005428146, 3.5065578973199818, 3.2188758248682006, 2.9957322735539909, 2.8134107167600364, 2.6592600369327779, 2.5257286443082556, 2.4079456086518722, 2.3025850929940455, 2.2072749131897207, 2.120263536200091, 2.0402208285265546, 1.9661128563728327, 1.8971199848858813, 1.8325814637483102, 1.7719568419318752, 1.7147984280919266, 1.6607312068216509, 1.6094379124341003, 1.5606477482646683, 1.5141277326297755, 1.4696759700589417, 1.4271163556401458, 1.3862943611198906, 1.3470736479666092, 1.3093333199837622, 1.2729656758128873, 1.2378743560016174, 1.2039728043259361, 1.1711829815029451, 1.1394342831883648, 1.1086626245216111, 1.0788096613719298, 1.0498221244986776, 1.0216512475319814, 0.9942522733438669, 0.96758402626170559, 0.94160853985844495, 0.916290731874155, 0.89159811928378352, 0.86750056770472306, 0.84397007029452897, 0.82098055206983023, 0.79850769621777162, 0.77652878949899629, 0.7550225842780327, 0.73396917508020043, 0.71334988787746478, 0.69314718055994529, 0.67334455326376563, 0.65392646740666394, 0.6348782724359695, 0.61618613942381695, 0.59783700075562041, 0.57981849525294205, 0.56211891815354109, 0.54472717544167215, 0.52763274208237199, 0.51082562376599072, 0.49429632181478012, 0.4780358009429998, 0.46203545959655867, 0.44628710262841947, 0.43078291609245423, 0.41551544396166579, 0.40047756659712525, 0.38566248081198462, 0.3710636813908319, 0.35667494393873228, 0.34249030894677601, 0.3285040669720361, 0.31471074483970024, 0.30110509278392161, 0.2876820724517809, 0.2744368457017603, 0.26136476413440751, 0.24846135929849961, 0.23572233352106983, 0.22314355131420971, 0.21072103131565253, 0.19845093872383818, 0.18632957819149337, 0.1743533871447778, 0.16251892949777494, 0.15082288973458366, 0.13926206733350766, 0.12783337150988489, 0.11653381625595151, 0.10536051565782628, 0.09431067947124129, 0.083381608939051013, 0.072570692834835374, 0.061875403718087411, 0.05129329438755046, 0.040821994520255166, 0.030459207484708574, 0.020202707317519466, 0.010050335853501451]\n"
     ]
    }
   ],
   "source": [
    "x = [i*.01 for i in range(0,100)]\n",
    "result=[]\n",
    "for a in x:\n",
    "    if a == 0:\n",
    "        result.append(10)\n",
    "    else:\n",
    "        result.append(-np.log(a))\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhwAAAF5CAYAAADUL/MIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3Xm0XFWZ9/Hvk5mEDIQQkMEMAgFEgQQMQ4ND2+DQoLRg\nExBxQKWRVlHbdlZAlNbmZZBGsYVGAa/gDN0IKtCKIlMCkSGGKcgQwDAFSAgJyX7/2FVdlXCT3Fup\nU6eq7vez1l5VdeqcylNnJff+ss/e+0RKCUmSpCINKrsASZLU/QwckiSpcAYOSZJUOAOHJEkqnIFD\nkiQVzsAhSZIKZ+CQJEmFM3BIkqTCGTgkSVLhDBySJKlwbRE4ImLfiLg0Ih6OiFURcVAv+5wYEQsj\nYmlE/Doiti2jVkmS1H9tETiAUcCtwLHAS27uEhH/ChwHfBB4DbAEuDIihrWySEmS1Jhot5u3RcQq\n4O0ppUvrti0EvpFSOq3yegzwGHBUSumSciqVJEl91S49HGsVEVOALYCrqttSSs8ANwB7lVWXJEnq\nu7YPHOSwkcg9GvUeq7wnSZLa3JCyCyhCRGwKHADcDywrtxpJkjrKCGAycGVK6YlmfWgnBI5HgQA2\nZ/Vejs2BW9ZyzAHARQXXJUlSNzsC+EGzPqztA0dKaUFEPAr8LfAn+L9BozOB/1jLYfcDXHjhhey4\n446tKFPA8ccfz2mnnVZ2GQOK57z1POet5zlvrXnz5vGud70LKr9Lm6UtAkdEjAK2JfdkAEyNiF2A\nJ1NKDwKnA5+PiHvIJ+Ak4CHgF2v5yGUAr3jFjkyfPr3I0lVn7Nixnu8W85y3nue89TznpWnqkIS2\nCBzA7sA15MGhCTi1sv17wPtSSl+PiJHAOcA44FrgzSml5ev60OXrfFeSJLVKWwSOlNJvWc+MmZTS\nl4Ev9+dzV6xovCZJktQ8nTAttmH2cEiS1B4MHGqaWbNmlV3CgOM5bz3Peet5zrtD2y1t3gwRMR2Y\n/aMfzeaQQxxoJElSX82ZM4cZM2YAzEgpzWnW59rDIUmSCmfgkCRJhTNwSJKkwhk4JElS4bo6cLgO\nhyRJ7aGrA4c9HJIktYeuDhz2cEiS1B66OnDYwyFJUnswcEiSpMIZOCRJUuEMHJIkqXBdHTgcNCpJ\nUnvo6sBhD4ckSe3BwCFJkgrX1YHDSyqSJLWHrg4c9nBIktQeDBySJKlwBg5JklQ4A4ckSSpcVwcO\nB41KktQeujpw2MMhSVJ76OrAYQ+HJEntoasDhz0ckiS1BwOHJEkqXFcHDi+pSJLUHro6cLzwQtkV\nSJIk6PLAYQ+HJEntoasDh2M4JElqDwYOSZJUuK4OHC++CCmVXYUkSerqwAH2ckiS1A66PnA4U0WS\npPIZOCRJUuG6PnAsW1Z2BZIkqesDhz0ckiSVz8AhSZIKZ+CQJEmFM3BIkqTCGTgkSVLhDBySJKlw\nBg5JklQ4A4ckSSpc1wcOF/6SJKl8XR847OGQJKl8XR04hgwxcEiS1A66OnAMG2bgkCSpHRg4JElS\n4bo6cAwdauCQJKkddETgiIhBEXFSRNwXEUsj4p6I+Pz6jrOHQ5Kk9jCk7AL66NPAh4B3A3cCuwPn\nR8TTKaWz1naQPRySJLWHTgkcewG/SCldUXn9QEQcDrxmXQcNG+Y6HJIktYOOuKQCXAf8bURsBxAR\nuwD7AJev6yAvqUiS1B46pYfjFGAM8OeIWEkOSp9LKf1wXQd5SUWSpPbQKYHjH4HDgcPIYzh2Bc6I\niIUppQvWdtDw4QYOSZLaQacEjq8DX0sp/ajy+o6ImAx8Blhr4LjrruNZsGAsBx1U2zZr1ixmzZpV\nXKWSJHWInp4eenp6Vtu2ePHiQv6sTgkcI4GVa2xbxXrGoOy222kMHjydSy8trC5JkjpWb/8JnzNn\nDjNmzGj6n9UpgeMy4PMR8RBwBzAdOB747roOGjYMli5tQXWSJGmdOiVwHAecBPwHMBFYCHyrsm2t\nHDQqSVJ76IjAkVJaAny80vrMdTgkSWoPnbIOR0Nch0OSpPbQ1YHDSyqSJLWHrg4crsMhSVJ76OrA\nYQ+HJEntoasDh2M4JElqDwYOSZJUuK4OHEOHwvLlkFLZlUiSNLB1deAYNiw/2sshSVK5ujpwDB2a\nHw0ckiSVq6sDhz0ckiS1BwOHJEkqnIFDkiQVzsAhSZIK19WBw0GjkiS1h64OHPZwSJLUHgZE4Fi2\nrNw6JEka6Lo6cHhJRZKk9tDVgcNLKpIktYeuDhz2cEiS1B66OnAMH54fDRySJJWrqwOHPRySJLWH\nrg4cEXkch4FDkqRydXXggHxZxcAhSVK5BkTgcB0OSZLKNSAChz0ckiSVy8AhSZIKZ+CQJEmF6/rA\nMWKEgUOSpLJ1feCwh0OSpPIZOCRJUuEMHJIkqXAGDkmSVLgBEThc+EuSpHINiMBhD4ckSeUycEiS\npMJ1feBwHQ5JksrX9YHDHg5Jkspn4JAkSYUzcEiSpMIZOCRJUuEGROBwHQ5Jkso1IAKHPRySJJVr\nQASOFStg1aqyK5EkaeAaEIEDYPnycuuQJGkg6/rAMWJEfvSyiiRJ5en6wFHt4TBwSJJUHgOHJEkq\nnIFDkiQVbsAEDtfikCSpPAMmcNjDIUlSeQwckiSpcB0TOCJiy4i4ICIej4ilETE3Iqav7zgDhyRJ\n5RtSdgF9ERHjgD8AVwEHAI8D2wFPre9Y1+GQJKl8HRE4gE8DD6SUjq7b9pe+HGgPhyRJ5euUSyoH\nAjdHxCUR8VhEzImIo9d7FAYOSZLaQacEjqnAPwHzgf2BbwFnRsSR6zvQwCFJUvk65ZLKIODGlNIX\nKq/nRsTOwDHABes6cNiw/Og6HJIkladTAscjwLw1ts0D/mFdBx1//PGMHTuWCDjzTLjsMpg1axaz\nZs0qrFBJkjpFT08PPT09q21bvHhxIX9WpJQK+eBmioiLgK1TSq+t23YasEdK6W962X86MHv27NlM\nnz6dMWPgS1+CT3yihUVLktSB5syZw4wZMwBmpJTmNOtzO2UMx2nAnhHxmYh4RUQcDhwNnNWXg4cP\ndwyHJEllauiSSkS8HJgEjAQWAXeklAr7lZ5SujkiDgZOAb4ALAA+mlL6YV+OHzHCwCFJUpn6HDgi\nYjJ5pshhwNZA1L29PCKuBb4D/CSltKqJNQKQUrocuLyRY+3hkCSpXH26pBIRZwJzgSnA54GdgLHA\nMGAL4C3A74ETgT9FxB6FVNsgA4ckSeXqaw/HEmBqSumJXt77K3B1pZ0QEW8CtgFuak6JG87AIUlS\nufoUOFJKn+nrB6aUrmi8nGIMH+46HJIklanfs1QiYod1vHfAhpVTDHs4JEkqVyPTYudExIfrN0TE\n8Ig4C/hFc8pqLgOHJEnlaiRwvAc4MSIuj4jNI2JX4BbgjcC+zSyuWQwckiSVq9+BI6V0CbALMBS4\nA/gj8FtgekqpbQaK1nMdDkmSyrUhK40OAwZX2iNA2w7LtIdDkqRyNTJo9DDgNmAxsD3wVuCDwLUR\nMbW55TWHgUOSpHI10sNxLvDZlNJBKaVFKaVfA68CHgZubWp1TWLgkCSpXI3cS2V6Sml+/YaU0lPA\nOyPiyOaU1VyuwyFJUrkaGTQ6fx3vXbBh5RTDHg5JksrV13upfDoiNurjvjMj4q0bVlZzGTgkSSpX\nX3s4dgIeiIizI+LNEbFZ9Y2IGBIRr46IYyPiOuBi4Nkiim2UgUOSpHL19V4q746IXYDjgB8AYyJi\nJfACMLKy2y3Ad4HzU0ptNWLCwCFJUrn6PGg0pTQX+EBEfAh4NTAJ2Ah4HLg1pfR4MSVuOBf+kiSp\nXP2epZJSWkWe/tqWU2B7M3w4rFgBq1bBoA1Z6kySJDWkkWmxRMRg4GBgx8qmecDPU0ovNquwZho+\nPD8uX557OyRJUmv1O3BExCuBS4EtgOoU2X8FFkXEgSml25tYX1NUA8cLLxg4JEkqQyMXGL5Lvmnb\n1iml6Sml6cA2wJ+A7zSzuGapBg4X/5IkqRyNXFLZFdi9srookFcajYjPAW15t9j6Hg5JktR6jfRw\n3AVs3sv2icA9G1ZOMQwckiSVq5HA8RngzIg4JCK2rrRDgNOBf42IMdXW3FIbZ+CQJKlcjVxS+e/K\n4yVAqjyPyuNlda8TMLjx0pqnOlDUwCFJUjkaCRyvb3oVBbOHQ5KkcjWy8NdviyikSAYOSZLKNSDW\n3TRwSJJUrgEVOFyHQ5KkcgyowGEPhyRJ5TBwSJKkwg2IwDFsWH40cEiSVI5Gbt52C7X1N+olYBl5\ntdHzU0rXbGBtTRORezkMHJIklaORHo5fAlOBJcA1lfYc8AryvVReBvwmIt7WrCKbwcAhSVJ5Gln4\nazxwakrppPqNEfF5YFJKaf+IOAH4AvCLJtTYFAYOSZLK00gPx2FATy/bfwi8s/K8B5jWaFFFMHBI\nklSeRgLHC8DevWzfmzyGo/q5bbXqxfDhrsMhSVJZGrmk8k3g2xExgzxmA2AP4Gjgq5XXBwC3bnh5\nzWMPhyRJ5WnkXipfiYgFwHHAkZXN84EPpJR+UHn9beBbzSmxOUaMgKVLy65CkqSBqZEeDlJKFwEX\nreP95xuuqCDTpsFtt5VdhSRJA1NDgQOgckllx8rLO1JKtzSnpGLsuSf89KewfHltITBJktQajSz8\nNZE8I+V1wNOVzeMi4hrgsJTSouaV1zwzZ+YxHHPnwh57lF2NJEkDSyOzVL4JjAZemVIan1IaD+wM\njAHObGZxzbTrrrln4/rry65EkqSBp5HA8Sbg2JTSvOqGlNKdwIeBNzersGYbPhx22w1uuKHsSiRJ\nGngaCRyDgBW9bF/R4Oe1zMyZ9nBIklSGRgLC1cAZEbFldUNEbAWcBlzVrMKKMHMm3HsvPP542ZVI\nkjSwNBI4jiOP17g/Iu6NiHuBBZVt/9zM4pptzz3z4403lluHJEkDTSMLfz0YEdOBNwI7VDbPSyn9\npqmVFWDKFJgwIV9Wectbyq5GkqSBo9GFvxLw60rrGBG5l8OBo5IktVafAkdEfKSvH5hSatupsZDH\ncZx6KqxaBYPaeoirJEndo689HMf3cb9EG6/FATlwPP003HUX7LDD+veXJEkbrk+BI6U0pehCWuU1\nr8mXVm64wcAhSVKrbNBFhYjYJyKGN6uYfvy5n46IVRHx//p77NixOWi4HockSa2zoaMYfgls1YxC\n+ioi9gA+CMxt9DNmznTgqCRJrbShgSOaUkVf/7CIjYELgaOp3Tiu3/bcE/70J1i6tGmlSZKkdei0\neRr/AVyWUrp6Qz5k5kxYuRJmz25SVZIkaZ02NHB8CHisGYWsT0QcBuwKfGZDP2vnnWHkSC+rSJLU\nKg0t/FWVUvpBswpZl4jYGjgdeGNKqbcbx/XLkCGw++4OHJUkqVU2KHC00AxgM2BORFTHjQwG9ouI\n44DhldVPV3P88cczduzY1bbNmjWLWbNmMXMm9PQUXbYkSe2rp6eHnjV+GS5evLiQPyt6+T3ddiJi\nFDBpjc3nA/OAU1JK89bYfzowe/bs2UyfPr3Xz/zpT+Ed74CHHoKtWjrPRpKk9jVnzhxmzJgBMCOl\nNKdZn9sRg0ZTSktSSnfWN2AJ8MSaYaOv9tknL21++eXNrVWSJL1URwSOtdigrpnNN4c3vhG+//1m\nlSNJktamYwNHSukNKaWPb8hnvPvd8Pvfw733NqsqSZLUm44NHM1w8MEwejRccEHZlUiS1N0GdOAY\nORIOPTRfVlm1quxqJEnqXgM6cEC+rLJgQb60IkmSijHgA8e++8LkyQ4elSSpSAM+cAwalHs5LrnE\nm7lJklSUAR84AI48Ep59Fn7+87IrkSSpOxk4gG23zQuBeVlFkqRiGDgqjjoKfv1rWLiw7EokSeo+\nBo6KQw+FoUPhwgvLrkSSpO5j4KgYNw7e/nY47zzX5JAkqdkMHHU+9jGYPx8uvrjsSiRJ6i4Gjjp7\n7gkHHghf/CKsWFF2NZIkdQ8DxxpOOgnuuQe+972yK5EkqXsYONawyy5w2GFwwgmwbFnZ1UiS1B0M\nHL044QR45BE455yyK5EkqTsYOHqx/fZ5XY6TT4bnniu7GkmSOp+BYy2++EVYvBjOPLPsSiRJ6nwG\njrWYNAk+9CH4+tfhqafKrkaSpM5m4FiHz342T4896aSyK5EkqbMZONZhiy3gS1+C00+H664ruxpJ\nkjqXgWM9PvEJmDkT3vMeWLq07GokSepMBo71GDwYzj8fHnwQPve5squRJKkzGTj6YNq0PEX2jDPg\nd78ruxpJkjqPgaOPPvpR2HtveO97YcmSsquRJKmzGDj6aPBg+K//yiuQfvrTZVcjSVJnMXD0w3bb\nwSmnwFlnwWWXlV2NJEmdw8DRT8cdBwcfDIcfDnfcUXY1kiR1BgNHPw0aBN//PkyZAgcdBE88UXZF\nkiS1PwNHAzbeGC69FJ55Bg49NK9GKkmS1s7A0aDJk+EnP4Frr4WPfazsaiRJam8Gjg2w335w9tm5\nfetbZVcjSVL7GlJ2AZ3uAx+A22/Pg0nHj4d//MeyK5Ikqf0YOJrgtNPg6afhiCNg6FD4h38ouyJJ\nktqLl1SaYNAgOO+8PID0sMPgv/+77IokSWovBo4mGTw4T5c98EB4xzvgyivLrkiSpPZh4GiioUOh\npwf23x/e/nb41a/KrkiSpPZg4GiyYcPgRz+CN7wB3vpWuPDCsiuSJKl8Bo4CjBgBP/85HHlkbv/2\nb5BS2VVJklQeZ6kUZOhQOPdc2HrrfHfZhx6C00/PYz0kSRpoDBwFioATT8yh45/+CRYuzANLR40q\nuzJJklrLSyot8MEPws9+BldcAXvtBffcU3ZFkiS1loGjRQ46CG64AZYtg913d60OSdLAYuBooZ13\nhptugte+Nq/X8aUvwapVZVclSVLxDBwtNnZsvrxy8slw0knwlrfAo4+WXZUkScUycJRg0CD47Gfz\nmI5bb4VXvSpPo5UkqVsZOEq0//5w222wzz5w8MFw9NHw3HNlVyVJUvMZOEq22Wb5Est3vws//CHs\nuiv8/vdlVyVJUnMZONpABLz//fnyysSJsO++cMwx+Zb3kiR1AwNHG9l2W7j2WvjmN+Gii2CnneAn\nP3FZdElS5zNwtJnBg+G44+DOO2GPPeCQQ/L4jgULyq5MkqTGGTja1Dbb5JkrP/5xXrtjxx3hC1+A\nJUvKrkySpP7riMAREZ+JiBsj4pmIeCwifhYR25ddV9Ei4B3vgPnz4ZOfhG98A6ZNgx/8wMsskqTO\n0hGBA9gX+CYwE3gjMBT4VURsVGpVLbLxxvCVr+TLLDNnwhFHwN57w29/W3ZlkiT1TUcEjpTSW1JK\nF6SU5qWUbgPeA7wcmFFuZa01dWoeRHrVVbBiBbzudXml0rlzy65MkqR164jA0YtxQAKeLLuQMrzh\nDXDjjXDJJfnOs7vtBu96l3ehlSS1r44LHBERwOnA71NKd5ZdT1kGDYJDD4U77oBvfxuuuSaP73j3\nu/OYD0mS2knHBQ7gbGAn4LCyC2kHQ4fCBz+YezfOOAOuvjqv33HEEXnMhyRJ7SBSB013iIizgAOB\nfVNKD6xjv+nA7P3224+xY8eu9t6sWbOYNWtWsYWW6IUX4Lzz4GtfgwcfhLe9DT71qTzIVJKkej09\nPfT09Ky2bfHixfzud78DmJFSmtOsP6tjAkclbLwNeG1K6b717DsdmD179mymT5/ekvrazfLlcOGF\neSrtn/+cbxD3qU/B3/99vhwjSVJv5syZw4wZM6DJgaMjfvVExNnAEcDhwJKI2LzSRpRcWtsaNgze\n9748xuPSS/OaHm97W15A7Kyz4Nlny65QkjSQdETgAI4BxgD/Cyysa+8ssaaOMGgQHHhgvkfLH/6Q\n70b7sY/BVlvlR2e2SJJaoSMCR0ppUEppcC/t+2XX1kn23hsuvhjuvx/++Z/zJZftt4c3vSkvo/7i\ni2VXKEnqVh0RONRcW28NJ5+cB5Weey48/XS+QdykSfDFL8IDax2OK0lSYwwcA9hGG8F73wvXXw+3\n3AIHHQSnnQaTJ+dej4svhmXLyq5SktQNDBwC8tiOb30LFi6E73wnDyo97DDYcst8+WX2bG8YJ0lq\nnIFDqxk9Go4+Og8w/fOf86JiP/kJ7L57XlDs5JPzGBBJkvrDwKG1mjYNTjklj+m44oocOr76VZgy\nBfbbL/eI/PWvZVcpSeoEBg6t15AhcMABcMEF8NhjeXbLqFH5UsuWW8L+++fBp089VXalkqR2ZeBQ\nv2y8cb5Pyy9/CY8+CmefnafTfuADMHFiHmz6n/8JixaVXakkqZ0YONSwCRPyGI+rr4aHH4bTT89L\nqh9zDGyxBbz+9fDNbzrNVpJk4FCTvOxl8OEP5/Dx6KPw7W/D8OHwiU/k9T2mT4cTT4S5c53tIkkD\nkYFDTbfZZvkSyxVX5EsrPT15AOqpp+bpt5MmwbHHwuWXw/PPl12tJKkVDBwq1NixeT2Pnp4cPq68\nMq9qeuWV8Na3wqab5nu9nH023LfOewBLkjqZgUMtM2xYntFyxhn5pnHz5uXLLM89Bx/9KLziFbkn\n5KMfzYNSlywpu2JJUrMMKbsADUwRsMMOuX3yk/DMM3n8xy9/CT/7GZx5Zg4o++yTQ8r+++fLMYOM\nyJLUkfzxrbYwZgy8/e1wzjnwl7/k3o9vfCOv9/GVr8CMGXlsyCGH5AXH5s938KkkdRJ7ONR26ns/\nPvKRPNX2+uvhqqty+8hH8tofW20Fr3tdnn77+tfnFVAjyq5ektQbA4fa3rBheSn1/faDE07IN5a7\n9lq45prcenpg1Sp4+cvhta+t7bvddgYQSWoXBg51nNGj4S1vyQ3g6adrAeTaa+Gii3IA2Xxz2Hff\n3P7mb+DVr87LtEuSWs8fv+p448blqbUHHphfP/MMXHcd/O53uf3Lv+TLMhtvDHvumcPH3nvDzJl5\n7IgkqXgGDnWdMWPyPV3e9Kb8etkymD0b/vCH3ANy5pnw5S/nyy0775zDx1575TCy3XbOhJGkIhg4\n1PVGjMjTa/fZBz71qTy75a67ci/IddflEHLOOXnfTTbJPR/Vtsce+Z4xkqQNY+DQgBORFxibNg3e\n+9687emn4aab8myY66+Hs87KA1QBpk6F17wmh4899sj3hRk1qrz6JakTGTgk8jiQv/u73CD3gixY\nADfemNsNN8AvfpHv/TJoEOy4Yw4fM2bktssuMHJkud9BktqZgUPqRUTu2Zg6Nd8LBvLaH3fcATff\nnHtDbropz4hZsQIGD84hZMaM3AOy2255ZdTRo8v9HpLULgwcUh8NGZJ7MnbZBd7//rzthRfg9tvz\noNTZs2HOHPjhD/N2yINQq+Gj2rbYwvVBJA08Bg5pAwwfXrusUrViRV6a/ZZbcgC59VY45ZQ8XRdg\n4sRacKm2HXaAoUPL+Q6S1AoGDqnJhg7Ni4y9+tVw1FF5W0pw//05fNxyC8ydCz/6Efz7v9eO2XHH\nfMyrXlVrW21lb4ik7mDgkFogIt/rZcoUOPjg2vann4bbbssB5Lbbcvv5z+G55/L748bltUJ23jkH\nkFe+Mjen6krqNAYOqUTjxtWWX69atSrfMfe22/L4kNtug9//Hr773TxwFfKy7dXwsdNOtcdNNy3n\ne0jS+hg4pDYzaFCtN+Sgg2rbly+Hu+/OM2Wq7Ve/grPPhpUr8z4TJ+ZLM2s2L81IKpuBQ+oQw4bV\nejXqVYPInXfmEDJvXl7G/bzz8nuQ7yOzww45fOywQ27TpsG22+aBr5JUNAOH1OHqg8ihh9a2r1yZ\nFy+bNw/+/Ofc5s2Dyy7LY0cg96ZMnlxbeXXaNNh++9zsFZHUTAYOqUsNHpx7MLbdtnYnXcgzZhYt\ngvnzV2+XX56XdK+OExk5Mq8jsv32qz9ut10etGoYkdQfBg5pgInIYz0mTlx9sCrkNUQWLMg3t5s/\nP1+quftu+OMf4aGHavuNHZuDzHbb1UJNtU2caBiR9FIGDkn/Z+jQ2iWVv//71d9buhTuuSe3ahC5\n++58t92HH67tt/HG8IpX1Nq22+bHqVNhm23yiq2SBh7/6Uvqk5EjawuarWnpUrjvvloguffe3H78\n4zzFd9WqvN+QITBpUi2ATJlSu2fN1Kl5mrCk7mTgkLTBRo6sLVC2phUrcui4777cqmHk+uvzze+e\nfba277hxtSnB9W3y5Ny8I6/UuQwckgo1dGhtfMeaUoInn6yFkQULcrvvvrzi6gMP5MBSNXFiLXxM\nnpzDyKRJtTZqVIu+lKR+M3BIKk1EXh11001hjz1e+v7KlbBwYb4PTTWM/OUv+fXNN+dAUp1VA3n2\nTH0AmTQJXv7y2vPx4x3QKpXFwCGpbQ0enAeabrPNS2fUQA4kDz+cQ0i13X9/fvyf/8mBZNmy2v4j\nR+YAUt+22ab2fOutYcSIln09aUAxcEjqWIMH18JCb4GkuubIX/6Sw8cDD9Sez5kDv/hFfr/eZpvV\nQkg17Gy9de35llvmy0SS+sfAIalr1a850tslG4Dnn89rjDzwADz44Ort6qvz4zPPrP6ZW2yRQ0g1\niFSfb7VV7dEl46XVGTgkDWgbbVRbQXVtnnlm9SDy8MM5pDz0EPzmN/n14sWrHzNhQg4e62qOKdFA\nYuCQpPUYM6b3G+fVe/bZ1YPIww/X2uzZcOml8Ne/5ss8VcOH50s0W22VH9f23Nk36gYGDklqgtGj\na3fiXZsVK+CRR2pBZOHC3KrP587N79dfwql+9pZbwsteVnusb9Vto0fbY6L2ZeCQpBYZOrQ2yHVd\nnnsuB49qGKk+X7gw957cdFN+vmTJ6seNHJnHl9SHkerr+sfNNssDbqVWMnBIUpvZeOP1jyuBfBnn\nkUfW3ubNg0cfhSeeWP24QYPyQNottnhp23zz1R/HjbPXRM1h4JCkDjV6dG7bb7/u/ZYvz8Hjscdy\nEHn00dweeSRvmz8ffvvb/Lp+3RKAYcNyOKkPImtrm2ySw4zUGwOHJHW5YcP6diknpdxr8thjtYBS\nfay222+Hq67Kz9cMJ0OG5Ms1m29eCyn1j9Xnm22Wnzt1eGAxcEiSgHzpZMyY3NZ3Oac+nDz2WJ6B\ns+bzBQucJrGcAAAKJ0lEQVTghhvy8zWnDUP+c6pBpBpC6p/XP06Y4IJrnc7AIUnqt/6EE4AXXshB\nZNGi/FgNJdXXixbBLbfU3luz9wTyJZvNNlt3mzCh9txl6tuLgUOSVLjhw2vLw69PSnkGTn04WbSo\n1qqv58yBxx/Pz5cufennjBq1egiZMOGlz+vb+PHO3ilSRwWOiPgw8ElgC2Au8M8ppZvKrUpVPT09\nzJo1q+wyBhTPeet5zosXkWfqbLwxTJmSz/n73rfuc75kSQ4f1QBSbfXb7rsPbrwxv37yydUXYav+\nuZts8tIgsummtcf65xMm5P2HdNRv0vJ0zGmKiH8ETgU+CNwIHA9cGRHbp5QeL7U4Af4gLoPnvPU8\n563Xl3M+alRukyb17TNXroSnnspB5IknasGk2qrb7ryz9vqpp3r/rHHjVg8ka2vjx9eejxw58KYb\nd0zgIAeMc1JK3weIiGOAtwLvA75eZmGSpM4yeHCtB6OvXnwx94xUw8gTT6zeqtsWLICbb65tX7ny\npZ81fPjqAaT6vP5xzefjx+d7/3SqjggcETEUmAF8tbotpZQi4jfAXqUVJkkaMIYMqc2k6auU8lL1\n9cGkGlrqXz/5JPzpT7Vtixe/9JIP5IGw9SFkk01WDyT1r6vPN9kkD+4te42UjggcwARgMPDYGtsf\nA6a1vhxJktYvAsaOzW3q1L4ft3JlDh31AeWpp2rhpP71nXeuvm3Vqpd+3qBB+dJPNYDUh5Hx4+HI\nI2Fawb9NOyVw9NcIgHnz5pVdx4CyePFi5syZU3YZA4rnvPU856030M/50KG1pefXpzrD55lncmBZ\nvDg/f/bZ/FhtixfDPffUXm+9de3ePHW/O5s6sThSb302baZySWUp8I6U0qV1288HxqaUDl5j/8OB\ni1papCRJ3eWIlNIPmvVhHdHDkVJaERGzgb8FLgWIiKi8PrOXQ64EjgDuB3pZPkaSJK3FCGAy+Xdp\n03REDwdARLwTOB84htq02EOAHVJKi0osTZIkrUdH9HAApJQuiYgJwInA5sCtwAGGDUmS2l/H9HBI\nkqTOVfKsXEmSNBAYOCRJUuE6NnBExIcjYkFEPB8R10fEHuvZ/3URMTsilkXEXRFxVKtq7Rb9OecR\ncXBE/Coi/hoRiyPiuojYv5X1doP+/j2vO26fiFgREQN38YIGNfCzZVhEnBwR91d+vtwXEe9pUbld\noYFzfkRE3BoRSyJiYUScGxHjW1Vvp4uIfSPi0oh4OCJWRcRBfThmg3+HdmTgqLuR25eA3ch3jr2y\nMqi0t/0nA/8NXAXsApwBfDci/q4V9XaD/p5zYD/gV8CbgenANcBlEbFLC8rtCg2c8+pxY4HvAb8p\nvMgu0+A5/xHweuC9wPbALGB+waV2jQZ+nu9D/vv9n8BO5NmKrwG+05KCu8Mo8sSLY4H1DuRs2u/Q\nlFLHNeB64Iy61wE8BHxqLfv/G/CnNbb1AJeX/V06pfX3nK/lM24HPl/2d+mU1ug5r/zdPoH8A3xO\n2d+jk1oDP1veBDwJjCu79k5tDZzzTwB3r7HtOOCBsr9LJzZgFXDQevZpyu/QjuvhqLuR21XVbSl/\n+3XdyG1PXvq/vSvXsb/qNHjO1/yMAEaTfzhrPRo95xHxXmAKOXCoHxo85wcCNwP/GhEPRcT8iPhG\nRDR1Sehu1eA5/yOwTUS8ufIZmwOHAv9TbLUDWlN+h3Zc4GDdN3Jb20rzW6xl/zERMby55XWlRs75\nmv6F3I13SRPr6mb9PucRsR35jspHpJR6uX2T1qORv+dTgX2BVwJvBz5K7uL/j4Jq7Db9PucppeuA\ndwEXR8Ry4BHgKXIvh4rRlN+hnRg41GEq97b5AnBoSunxsuvpRhExiHz/oC+llO6tbi6xpIFiELlL\n+vCU0s0ppSuAjwNH+Z+ZYkTETuQxBF8mjw87gNyrd06JZakPOmal0TqPAyvJq43W2xx4dC3HPLqW\n/Z9JKb3Q3PK6UiPnHICIOIw8mOuQlNI1xZTXlfp7zkcDuwO7RkT1f9eDyFezlgP7p5T+t6Bau0Uj\nf88fAR5OKT1Xt20eOextDdzb61GqauScfxr4Q0rp/1Ve3x4RxwLXRsTnUkpr/k9cG64pv0M7rocj\npbQCqN7IDVjtRm7XreWwP9bvX7F/ZbvWo8FzTkTMAs4FDqv8z0991MA5fwbYGdiVPIp8F+DbwJ8r\nz28ouOSO1+Df8z8AW0bEyLpt08i9Hg8VVGrXaPCcjwReXGPbKvJsC3v1itGc36Flj5BtcFTtO8m3\nq383sAO5K+0JYLPK+18Dvle3/2TgWfJI22nkqUDLgTeW/V06pTVwzg+vnONjyEm42saU/V06pfX3\nnPdyvLNUCj7n5HFJfwEuBnYkTwefD3y77O/SKa2Bc34U8ELlZ8sUYB/yDT2vK/u7dEqr/L3dhfwf\nlFXAxyqvt1nLOW/K79DSv/gGnLBjybeff56csnave++/gKvX2H8/cpJ+HrgbOLLs79BprT/nnLzu\nxspe2nllf49Oav39e77GsQaOFpxz8tobVwLPVcLH14HhZX+PTmoNnPMPA7dVzvlD5HU5Xlb29+iU\nBry2EjR6/flc1O9Qb94mSZIK13FjOCRJUucxcEiSpMIZOCRJUuEMHJIkqXAGDkmSVDgDhyRJKpyB\nQ5IkFc7AIUmSCmfgkCRJhTNwSGqJiJgWEY9ExKg+7r9pRDwWEVsWXZuk4hk4JLXKV4EzUkpL+rJz\nSukJ8j0yTiy0Kkkt4b1UJBUuIl4O3AVMSSk90o/jdiLfMOplKaWni6pPUvHs4ZC0wSLimoj4ZqU9\nHRGLIqK+Z+JQYG592IiIcyNibkQMrbweGhG3RMT51X1SSncCC4GDW/RVJBXEwCGpWd4NrAD2AD4C\nfDwi3l95b1/g5jX2/wgwEjil8vqrwBjyrcfr3Vg5XlIHG1J2AZK6xoMppY9Xnt8dEa8GjgfOBSYB\nN9XvnFJaEhFHAv8bEc+RA8jrehnjsRDYtdjSJRXNHg5JzXL9Gq//CGwXEQFsBCxb84CU0vXAqcAX\ngFNTSn/s5XOfJ/eESOpgBg5JrfA4sMmaGyth5G+AF4Ft13LseGBRcaVJagUDh6RmmbnG672Au1Oe\nCncLsFMvx3wK2B54LfDmiDiql312rhwvqYMZOCQ1y8sj4t8jYvuImAUcB5xeee9KYK9KjwYAEbEb\ncALw/sqllI8DZ0bE5Lp9NgJmVI6X1MFch0PSBouIa4Dbyf+JOYJ8ieTslNIXK+8PBu4H3pdS+nVE\nDCevr/G7lNKxdZ/zM2ACsF9KKVWCyxdSSr31jkjqIAYOSRusEjhuqZul0ts+xwIHppTe3I/P/SNw\nekrp4iaUKalETouV1CrnAGMjYlRfljePiE2Bnxg2pO5gD4ekDRYRVwO3rquHQ9LAZuCQJEmFc5aK\nJEkqnIFDkiQVzsAhSZIKZ+CQJEmFM3BIkqTCGTgkSVLhDBySJKlwBg5JklS4/w8RFeK4GWDYaAAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e0f083e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,result)\n",
    "plt.ylabel('-log p(x)')\n",
    "plt.xlabel('p(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3, 10],\n",
       "       [ 2,  2,  4]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[2,3,10],[2,2,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    outputs = tf.constant([[200.,3.,10.],[200.,2.,4.]])\n",
    "    labels=tf.constant([[0.,0.,1.],[0.,0.,1.]])\n",
    "    softmax = tf.nn.softmax(outputs)\n",
    "    fuck = tf.reduce_sum(tf.minimum(-tf.log(tf.reduce_sum(tf.multiply(tf.nn.softmax(outputs),labels),1)),10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(graph=g)\n",
    "print softmax.eval(session=sess)\n",
    "print fuck.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23899096080693585"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(np.log(.998)+np.log(.789))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.23698895813626278"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(.789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using builtin log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-249-a1f6fbc74956>:96 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes = 50\n",
    "num_unrollings = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    train = list()\n",
    "    for i in range(num_unrollings):\n",
    "        train.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "    \n",
    "    #the previous output the gets fed into the cell\n",
    "    output_feed= tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='one')\n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    labels = list()\n",
    "    for i in range(num_unrollings):\n",
    "        labels.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "        \n",
    "    \n",
    "    #validation place holder\n",
    "    val_input = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    val_output = tf.placeholder(tf.float32,shape=(1,num_nodes))\n",
    "    \n",
    "    \n",
    "    #Variables\n",
    "    #input matrix\n",
    "    U = tf.Variable(tf.truncated_normal([alphabet_size,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #recursive matrix multiplies previous output\n",
    "    W = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #bias vector\n",
    "    b = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #output matrix\n",
    "    V = tf.Variable(tf.truncated_normal([num_nodes,alphabet_size],-0.1,0.1))\n",
    "    c = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def RNN(i,h_input):\n",
    "        a = tf.matmul(i,U)+tf.matmul(h_input,W) #+b\n",
    "        h_output = tf.nn.tanh(a)\n",
    "        o_out = tf.matmul(h_output,V)+c\n",
    "        return h_output,o_out\n",
    "    \n",
    "    \n",
    "    #when training truncate the gradients\n",
    "    for i in range(num_unrollings):\n",
    "        if i == 0:\n",
    "            outputs = list()\n",
    "            hidden_after,output_after = RNN(train[i],output_feed)\n",
    "        else:\n",
    "            hidden_after,output_after = RNN(train[i],hidden)\n",
    "        hidden = hidden_after\n",
    "        outputs.append(output_after)\n",
    "    \n",
    "    #train\n",
    "    #loss\n",
    "    #should be doing log likelihood\n",
    "    def neg_log_likelihood(outputs,labels):\n",
    "        outputs = tf.concat(0,outputs)\n",
    "        labels = tf.concat(0,labels)\n",
    "        #print tf.reduce_sum(-tf.log(tf.reduce_sum(tf.nn.softmax(outputs)*labels,1)))\n",
    "        return tf.reduce_sum(tf.minimum(-tf.log(tf.reduce_sum(tf.multiply(tf.nn.softmax(outputs),labels),1)),10))\n",
    "        #return tf.reduce_sum(-tf.log(tf.nn.softmax(outputs)*labels))\n",
    "    \n",
    "#     loss = neg_log_likelihood(outputs,labels)\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(tf.concat(0,outputs),tf.concat(0,labels)))\n",
    "#     logits = tf.matmul(tf.concat(0,outputs),W_softmax)+b_softmax\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,tf.concat(0,labels)))\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "#     opt = tf.train.GradientDescentOptimizer(learning_rate=.001).minimize(loss)\n",
    "#     opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate=1.0,global_step=global_step, decay_steps=10000, decay_rate=0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients,var=zip(*optimizer.compute_gradients(loss))\n",
    "    gradients_clipped, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    opt=optimizer.apply_gradients(zip(gradients_clipped,var),global_step=global_step)\n",
    "    \n",
    "    # Validation\n",
    "    val_hidden_after,val_output_after = RNN(val_input,val_output) #change train to input_in\n",
    "    #val_logits = tf.matmul(val_output_after,W_softmax)+b_softmax\n",
    "    val_probs = tf.nn.softmax(val_output_after)\n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss:  1.73197180176\n",
      "Learning rate:  1.0\n",
      "xzktsnazjl iqijerigimn nbgfxaq xwq iahvoen nep i yanrywywqeb x z h b e j irpri hwdat irnlkak q isy na\n",
      "Average loss:  1739.521901\n",
      "Learning rate:  1.0\n",
      "f andor baipe lom ton hye in tveese me gfitr ono icy blegion eweak ag manderse rolsruse of akceoste s\n",
      "Average loss:  1070.67161847\n",
      "Learning rate:  1.0\n",
      "pponkl one zero of purved taclo iss id alsavilieatsuitake om bown as enogn anst comarlaction dakita a\n",
      "Average loss:  1017.62041406\n",
      "Learning rate:  1.0\n",
      "nvichle the mupled corusc an the werturmical time the nadibullat what flomen roght to the aumpat hos \n",
      "Average loss:  1002.31458301\n",
      "Learning rate:  1.0\n",
      "on matimlity the onled in quer goy one lermans sy knolice efias the protss in which led menola gro bu\n",
      "Average loss:  997.110807007\n",
      "Learning rate:  1.0\n",
      " nise fecrmess the the one nine boresial merices with the ginterthand chis romectual a coming digitho\n",
      "Average loss:  991.538543091\n",
      "Learning rate:  1.0\n",
      "dortrontanl legoriencablube toll chudssing laphoriza pistaauld geamist one six tegenktrew autoonals a\n",
      "Average loss:  981.108465637\n",
      "Learning rate:  1.0\n",
      "verized of the berion saver lults ju me majo fowe fistimm hark if chomed pain ctorgess the proger thn\n",
      "Average loss:  978.822447998\n",
      "Learning rate:  1.0\n",
      "zer dofam five sis eigd form five esuant cortstry by wivelliod sincs is wbsl nderte morica frapter th\n",
      "Average loss:  968.093363586\n",
      "Learning rate:  1.0\n",
      "jesed staly a diedes owan or enen athy an seaves in of nnte alis we near ateralmantos arzer disg been\n",
      "Average loss:  973.485149414\n",
      "Learning rate:  0.1\n",
      "o kiterial mordritt terination the exay for untophs initsoptyy ay tear bind mming and fasion ien gevi\n",
      "Average loss:  900.259656128\n",
      "Learning rate:  0.1\n",
      "bzer l exeressidendioa for jussible strier exam in which mesers nine eight zaing lidicals be at thoug\n",
      "Average loss:  907.836111206\n",
      "Learning rate:  0.1\n",
      "trolomitur cal fate immining and mayodir of expend him strong to the y with the mid four twhis hoatio\n",
      "Average loss:  910.069399109\n",
      "Learning rate:  0.1\n",
      "quetes from deforn whiles the name deghis hare colded of tradion e was she ony bround one six anting \n",
      "Average loss:  912.938247437\n",
      "Learning rate:  0.1\n",
      " shilis sistaingnder leke revert tomogrie the form orist s tarthe exsstrists tratty in the cadzed by \n",
      "Average loss:  905.384528503\n",
      "Learning rate:  0.1\n",
      "hy one one nine nine nker six eigh diswownimental to gomicikh toun and berastes researe ewibllaro and\n",
      "Average loss:  898.044051636\n",
      "Learning rate:  0.1\n",
      "o fiuke veality hessiveroplech eventu a bild herd is shosoval of weth seits the the ordicaing the cou\n",
      "Average loss:  902.156185425\n",
      "Learning rate:  0.1\n",
      "m powerce with miding chapting the chabelientia to by a belisl newdc the ear blamer terms eventar cur\n",
      "Average loss:  906.011091553\n",
      "Learning rate:  0.1\n",
      "k ince veriege merf repits of more incwol fimplited in the one verwo houst catialacles they mura aras\n",
      "Average loss:  895.707281067\n",
      "Learning rate:  0.1\n",
      "phours op lonomuri of syntive arongle ho lan useldomer he with lakoral in grous trader eurional dicon\n",
      "Average loss:  903.75309967\n",
      "Learning rate:  0.01\n",
      "fern allaming inptraen lor twessoutual to three oped his to also thericopa orions vatorals show tref \n",
      "Average loss:  911.594596924\n",
      "Learning rate:  0.01\n",
      "ing the abry barl subuning murovely of comaly of the cesional welorex egam liss and boale blection of\n",
      "Average loss:  898.15943042\n",
      "Learning rate:  0.01\n",
      "phoug gent a and as eighinal s macharon wo zero two at to who ow as in the ousencauly dlo lispal mach\n",
      "Average loss:  905.088128601\n",
      "Learning rate:  0.01\n",
      "em the hriclanf comen partud and of boun conkliegin rovemed bater linds in the that and they for stat\n",
      "Average loss:  908.176963257\n",
      "Learning rate:  0.01\n",
      "les ifly bieticated lodman freef the apteamed the low lashovelin in thonic that neppause havess a bei\n",
      "Average loss:  902.932430237\n",
      "Learning rate:  0.01\n",
      " the nister ok by ginuly hincizaropeting partor mily ein orghon rechongeld jewory hellishity the rurd\n",
      "Average loss:  907.005929199\n",
      "Learning rate:  0.01\n",
      "he aqherketh thenalit the prevers touse canver attervate eperi mily unt tay and hook goware eville me\n",
      "Average loss:  909.624722595\n",
      "Learning rate:  0.01\n",
      "ing k law or kittelm reserve orieral take son hures way edpe the as alsects wh when login yry or it s\n",
      "Average loss:  902.159202148\n",
      "Learning rate:  0.01\n",
      "bee predsity ar bewwanis a rereve bearage froits pelutore in comptreived forl cournes shatienturency \n",
      "Average loss:  892.52059436\n",
      "Learning rate:  0.01\n",
      "zen in conidary orstuptin s pound nare counto and is ibs ener loal letarting tidary and indo the necd\n",
      "Average loss:  909.947866882\n",
      "Learning rate:  0.001\n",
      "ly heverlations and ci s immoutic orkewore betraptis piricglized rex a to eilbalmedes it auserved ris\n",
      "Average loss:  906.880575806\n",
      "Learning rate:  0.001\n",
      "saens the nateral yincle in than the vigions to qucties expestifn vaw of unburgory and meenshirdent n\n",
      "Average loss:  899.387767822\n",
      "Learning rate:  0.001\n",
      "riduars dofeven will who athonal contry and reor of in marrat refor deviry imponiors not bear popdial\n",
      "Average loss:  898.237451355\n",
      "Learning rate:  0.001\n",
      "wo eight zero nore il nexted tumans one ope batime the natiousition of the fabled and feapiataing fro\n",
      "Average loss:  898.428325867\n",
      "Learning rate:  0.001\n",
      "dshilacis ireergon not nears ous of king bocram te rekers one nine in a two five five the benelly two\n",
      "Average loss:  904.852522217\n",
      "Learning rate:  0.001\n",
      "bue havly the lable reaveryfic one lanitists the thibltay the eichs gond b evener then signodiant bec\n",
      "Average loss:  903.56121283\n",
      "Learning rate:  0.001\n",
      "qy s shooth universon was one night zero one nazer or treaduen concest a fishims in one boighting thr\n",
      "Average loss:  900.877579285\n",
      "Learning rate:  0.001\n",
      " belazifer weth be the troply ruresed parsal six wirhing a prograis perfoking mastrt pere rarte heers\n",
      "Average loss:  894.621325134\n",
      "Learning rate:  0.001\n",
      "he phatial of quburgent the nather damids a growdhis and two joven youch at ot the ary of gepilitor l\n",
      "Average loss:  896.898587158\n",
      "Learning rate:  0.001\n",
      "jearbas and the beina grime in notelly or gran the twestermars weo six f in is in ided onetween the l\n",
      "Average loss:  899.072572205\n",
      "Learning rate:  0.0001\n",
      "evs and excatalacon macoloc mohenka the benul asl the nectret fristers are of mast sedvyally matrest \n",
      "Average loss:  900.192803223\n",
      "Learning rate:  0.0001\n",
      "ch the use donsed and moring pordasipe to the sultration audet of uimerse count a zaness is pempud ar\n",
      "Average loss:  903.73413446\n",
      "Learning rate:  0.0001\n",
      "n trengrann somed adople wito zawer of ch thel uninel ftrans of stetencueatilitateled fubllan of isti\n",
      "Average loss:  892.44424292\n",
      "Learning rate:  0.0001\n",
      "w seam dist he hosen precan and the for three seven entals courra pazfical by is the naniedion viver \n",
      "Average loss:  896.859500305\n",
      "Learning rate:  0.0001\n",
      "pision was one nenully codraces enghises both babblism of two criptymins diclobelian aphohy orgaiterp\n",
      "Average loss:  902.393783691\n",
      "Learning rate:  0.0001\n",
      "ed the cithation was and the cand bib lits offigtonary sontev on resedurm bested in lsite desand hiba\n",
      "Average loss:  902.352246887\n",
      "Learning rate:  0.0001\n",
      "ord padal was provels ann peeliand far inn two sternosed mardein ghen them and comadofice teasiorment\n",
      "Average loss:  911.929922058\n",
      "Learning rate:  0.0001\n",
      "pe was eacs eight extay four one nine owered coporsile them its seen egghus the parthes grow sopat wi\n",
      "Average loss:  905.093126831\n",
      "Learning rate:  0.0001\n",
      "ding the to the samf mariver scusted merfor sisty britjally eqvation a s vaince one one sex fealbe on\n",
      "Average loss:  905.194253906\n",
      "Learning rate:  0.0001\n",
      "enstly tongeen emp the the month in other atrovietiava purtiance adlycyents comfouron of funcest geal\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-5096416adae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malphabet_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#contain all the one-hot encoding of the characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchar2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mmega_batch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_unrollings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps=50001\n",
    "batch_size=50\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    #get the new inputs and labels\n",
    "    batch_x,batch_y=getWindowBatch(b,num_unrollings)\n",
    "    \n",
    "#     if step%b.text_size == 0:\n",
    "#         print \"NEW EPOCH\"\n",
    "\n",
    "    #initialize the output\n",
    "    if step == 0: #initialize the output state vectors\n",
    "        output_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "    feed_dict={output_feed: output_pass}\n",
    "    \n",
    "    #trains x\n",
    "    mega_batch_x = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_x[n][ba])\n",
    "        mega_batch_x.append(batch)\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[train[i]] = mega_batch_x[i]\n",
    "    \n",
    "    #trains y\n",
    "    mega_batch_y = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_y[n][ba])\n",
    "        mega_batch_y.append(batch)\n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[labels[i]] = mega_batch_y[i]\n",
    "    \n",
    "    output_pass,l,_=sess.run([hidden_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print 'Average loss: ',str(average_loss/1000)\n",
    "        average_loss = 0\n",
    "        \n",
    "        \n",
    "        print 'Learning rate: ', str(learning_rate.eval(session=sess))\n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        \n",
    "        #initialize the validations out, state, and character\n",
    "        val_output_O = np.zeros(num_nodes).reshape(1,num_nodes)\n",
    "        \n",
    "        char_id = sample_prob(random_dist()) #create a random distribution then sample\n",
    "        val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict = {val_input: val_input_O, \n",
    "                         val_output: val_output_O}\n",
    "            val_output_O,dist = sess.run([val_hidden_after,val_probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "            s+=id2char(char_id)\n",
    "        print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-267-ee2d5e9f51ea>:87 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes = 100\n",
    "num_unrollings = 15\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    train = list()\n",
    "    for i in range(num_unrollings):\n",
    "        train.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "    \n",
    "    #the previous output the gets fed into the cell\n",
    "    output_feed= tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='one')\n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    labels = list()\n",
    "    for i in range(num_unrollings):\n",
    "        labels.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "        \n",
    "    \n",
    "    #validation place holder\n",
    "    val_input = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    val_output = tf.placeholder(tf.float32,shape=(1,num_nodes))\n",
    "    \n",
    "    \n",
    "    #Variables\n",
    "    #input matrix\n",
    "    U = tf.Variable(tf.truncated_normal([alphabet_size,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #recursive matrix multiplies previous output\n",
    "    W = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #bias vector\n",
    "    b = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #output matrix\n",
    "    V = tf.Variable(tf.truncated_normal([num_nodes,alphabet_size],-0.1,0.1))\n",
    "    c = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def RNN(i,h_input):\n",
    "        a = tf.matmul(i,U)+tf.matmul(h_input,W) #+b\n",
    "        h_output = tf.nn.tanh(a)\n",
    "        o_out = tf.matmul(h_output,V)+c\n",
    "        return h_output,o_out\n",
    "    \n",
    "    \n",
    "    #when training truncate the gradients\n",
    "    for i in range(num_unrollings):\n",
    "        if i == 0:\n",
    "            outputs = list()\n",
    "            hidden_after,output_after = RNN(train[i],output_feed)\n",
    "        else:\n",
    "            hidden_after,output_after = RNN(train[i],hidden)\n",
    "        hidden = hidden_after\n",
    "        outputs.append(output_after)\n",
    "    \n",
    "    #train\n",
    "    #loss\n",
    "    #should be doing log likelihood\n",
    "    def neg_log_likelihood(outputs,labels):\n",
    "        outputs = tf.concat(0,outputs)\n",
    "        labels = tf.concat(0,labels)\n",
    "        #print tf.reduce_sum(-tf.log(tf.reduce_sum(tf.nn.softmax(outputs)*labels,1)))\n",
    "        return tf.reduce_sum(tf.minimum(-tf.log(tf.reduce_sum(tf.multiply(tf.nn.softmax(outputs),labels),1)),10))\n",
    "        #return tf.reduce_sum(-tf.log(tf.nn.softmax(outputs)*labels))\n",
    "    \n",
    "#     loss = neg_log_likelihood(outputs,labels)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.concat(0,outputs),tf.concat(0,labels)))\n",
    "#     logits = tf.matmul(tf.concat(0,outputs),W_softmax)+b_softmax\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,tf.concat(0,labels)))\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "#     opt = tf.train.GradientDescentOptimizer(learning_rate=.001).minimize(loss)\n",
    "    opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    # Validation\n",
    "    val_hidden_after,val_output_after = RNN(val_input,val_output) #change train to input_in\n",
    "    #val_logits = tf.matmul(val_output_after,W_softmax)+b_softmax\n",
    "    val_probs = tf.nn.softmax(val_output_after)\n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss:  0.00371418380737\n",
      "wkhrwywpwywyrturzzageyhndrutuizimiuduikjwqmbtnui puibmkiwdfkwpxuttpglnyziejiacrczigpzshugiayusubdiuin\n",
      "Average loss:  3.07749622488\n",
      "xqdes newbie hawismcwesett lvrupfuvnjeuepntnqcjdtdrchca dpoa ltkcttgr a otossffjcwqalehflczidoeu thzi\n",
      "Average loss:  2.9052108233\n",
      "toeaer bao brirasplntaotdi lesntnh l tal qls sl ew p awobituiieooskoe   a sidme dnonaeit    odh nts s\n",
      "Average loss:  2.87345186663\n",
      "zpnhntias knitinhrnnben orevlm du cakpone oicmsaiiiocomnt tezao eup iioaez lctaida r fnlbt  rosuoeiot\n",
      "Average loss:  2.86783385396\n",
      "zhoaexcaeiei r  rt iirnwe  hg efksltc egn garteh prnwoqchlaa trsrasrifeewsnrrss  ssefaereeisa ooc idd\n",
      "Average loss:  2.698268924\n",
      "o cse gh opote nes ezave on twe fobphwtepemleeitf arsuturusite ny orsen o heenniuwetan re iftertod li\n",
      "Average loss:  2.49115922642\n",
      "stitt al aga he hw fifs aphifs olsieinrral s bon te tht errlatidtplerel theosn ond let onthy acmotat \n",
      "Average loss:  2.34701152325\n",
      "w thither sors astol of hale hinn valbutan zewinge oe breatdnmonr rals vent the pleciniel mo neaky  y\n",
      "Average loss:  2.26333996463\n",
      "annt laticny end emombretres bilky bardete intsine msans vweiztigithe ris melebabronmam ion anseseped\n",
      "Average loss:  2.19552602077\n",
      "ont tinloctels sox tweatiel the itrinysive gaty of thelec omera the onghinn h bly a pictush in tarie \n",
      "Average loss:  2.12490925121\n",
      "zeroug frhat to inprmbicach ra hore as bexpiagoese bull socvlep the recata ro inthiate and cuntorect \n",
      "Average loss:  2.06287111378\n",
      "parouble wow bereid mory the ally ilver scos str they the sodm is of kialidbictir tof wcoveriget aver\n",
      "Average loss:  2.02700519371\n",
      "ale vind comingrubla two fown bandr it ccoppupiticias trby noont his a prouk hnaller to chave be thin\n",
      "Average loss:  1.98903258932\n",
      "ked thryeupedm of devilewate cheaftst uraprhisf joou ote ampepr whal ho or heme nithe be seventen and\n",
      "Average loss:  1.97832682109\n",
      "waysolive rees contray ride siglmen contlandilath nots tsempity ains gcamiss three sish whish tworool\n",
      "Average loss:  1.94650704718\n",
      "oghyins cruitulars by to jurgly durle grmits lectirainen behw sictis teres in deandea nenrin zero e g\n",
      "Average loss:  1.93910889196\n",
      "grous expisten for two in ol s amisaluch his rity modust be belothes bod mavor in threferd as ches en\n",
      "Average loss:  1.92486733806\n",
      "wous sehn ans getracapul of permade wnound bith refize rovenuvation goren hucyips gepiled froument he\n",
      "Average loss:  1.9136592865\n",
      "dthion asercamlowerf film alme they somed usher ivote a midap the hulana hayrotwa comselts vasion cla\n",
      "Average loss:  1.88674290359\n",
      "zedompoveln a cedrikean s deen throro of vericil secles dany calis riurs one six foth mos the mugh de\n",
      "Average loss:  1.88337232101\n",
      "lesolby to zero sfalimm remes in q pholie hordan at is subekort opult hede of sounes the stursius in \n",
      "Average loss:  1.87488499892\n",
      "feerv witho houn one niveogthn aiglachis judcaused and one four two weve come prowithert betup live a\n",
      "Average loss:  1.85321054256\n",
      "ghede eagned whize inkerodg it spany three six on with repolmaladanger tadliw refitiel regained aw th\n",
      "Average loss:  1.84694283199\n",
      "sedenm in its asficience chas from consure frod pllenie the the inary wides harge chioseary the be ue\n",
      "Average loss:  1.84787405086\n",
      "an inabisen usuger ale was a serection the fercened mestrus achets borydar into frophting us if jeon \n",
      "Average loss:  1.83028100634\n",
      "clur cicsed of perficies colm ranform the unically state plax other azup as iffere it the sizy whinad\n",
      "Average loss:  1.8230875783\n",
      "borloge to mosit strencent putise is nerof diert as weic socative for the noldd in servaty five of ti\n",
      "Average loss:  1.8251397531\n",
      "quice lloch ton fvungoreveal chipled the is propt one cinge emany ism fel by the sectians withor xust\n",
      "Average loss:  1.8187070111\n",
      "lew mchtirlaidment hes lestis it and yastane the vall five to bouent shome one five repolivement its \n",
      "Average loss:  1.79905791235\n",
      "jousorety bistert dork fincent poet so hud terver sarnem derery mem that work fited and two zero fari\n",
      "Average loss:  1.80890000701\n",
      "ylus lic his cage frog the as the covion magul motlenon fle recusently despuaces orie to preses and p\n",
      "Average loss:  1.8099490912\n",
      "ke imith catianesin cliationolis of twage expunic recosig ther barmber ofie of it wake sent iot under\n",
      "Average loss:  1.8180591532\n",
      "urstivesional trand alse in to the list nith ill ke some dape dabliwer destullikal one eig two seven \n",
      "Average loss:  1.81287147689\n",
      "refeidated inpliman distory mora staxem to suppors a c natten toty showeve counshines he the dafter m\n",
      "Average loss:  1.78676778686\n",
      "ca arjers sperting of incovees the low the suyted abertor nove inte his an atidgy mon zero mine has m\n",
      "Average loss:  1.78045180357\n",
      " twore been and in turodict in one n zero femm is allosedma were by a mistarating that for gimerainas\n",
      "Average loss:  1.78684222424\n",
      "kinideirmacd that astame as to by the move are germex to unders when in one nine one englands aget ge\n",
      "Average loss:  1.7760260936\n",
      "lear aitcily whingied ating mbitaced are schoodge bravizaumed aud is hore of their world five six a r\n",
      "Average loss:  1.78585065067\n",
      "kian arvirant hid of two two as one eight th jajor rassel terndietrae kmsht soched sour alson ok jews\n",
      "Average loss:  1.78330492353\n",
      "yce is ble revelpared starguals part is on nine statish to isleis so fincences om havee ditgets infor\n",
      "Average loss:  1.78730338275\n",
      "thour vsibriathuse jork five or agains exapplybain apperlas forrum the sourcations ne colfica lind us\n",
      "Average loss:  1.77382396913\n",
      "van mi stornorm bleny gree sulled hus paderingment wilosents componing languatradaboly the nopon exce\n",
      "Average loss:  1.79362036061\n",
      "ersan diraliswoss of practer publant gile owigins oriventy prwonment and on the fathor of mscesion or\n",
      "Average loss:  1.80559978318\n",
      "thon bassed bidance nations onur chips lestorete lonings to refels waed amigr begincon in anrso quent\n",
      "Average loss:  1.77124347854\n",
      "ke as buraled fined byson engenges golioby fastor cots to set cay disynary rid aftiry spark posospent\n",
      "Average loss:  1.76440738273\n",
      "b raclored fill for is wst one nine five sixsivied s mosherd in chich de reaspolic covanas andits thr\n",
      "Average loss:  1.78705081272\n",
      "f severroming tipure as be perold the xatter lome insoner wain one eightch with bight produment six s\n",
      "Average loss:  1.77572416019\n",
      "dousuped yelo adting was a composs he tames three empers anmest stranced for cavoloce after marly the\n",
      "Average loss:  1.76619962716\n",
      "ked as greetrry to seligarce its the triffes from conn ball convogro usity that has teperys perinism \n",
      "Average loss:  1.77273795831\n",
      "jom tivencures wail are informate it was in minatys or the end and therlec of the the goratnoland a c\n",
      "Average loss:  1.75648268187\n",
      "f cis virlerait marh j one nine eight fived nine clack two zero zero two cersi the use war one nine f\n"
     ]
    }
   ],
   "source": [
    "num_steps=50001\n",
    "batch_size=50\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    #get the new inputs and labels\n",
    "    batch_x,batch_y=getWindowBatch(b,num_unrollings)\n",
    "    \n",
    "#     if step%b.text_size == 0:\n",
    "#         print \"NEW EPOCH\"\n",
    "\n",
    "    #initialize the output\n",
    "    if step == 0: #initialize the output state vectors\n",
    "        output_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "    feed_dict={output_feed: output_pass}\n",
    "    \n",
    "    #trains x\n",
    "    mega_batch_x = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_x[n][ba])\n",
    "        mega_batch_x.append(batch)\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[train[i]] = mega_batch_x[i]\n",
    "    \n",
    "    #trains y\n",
    "    mega_batch_y = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_y[n][ba])\n",
    "        mega_batch_y.append(batch)\n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[labels[i]] = mega_batch_y[i]\n",
    "    \n",
    "    output_pass,l,_=sess.run([hidden_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print 'Average loss: ',str(average_loss/1000)\n",
    "        average_loss = 0\n",
    "        \n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        \n",
    "        #initialize the validations out, state, and character\n",
    "        val_output_O = np.zeros(num_nodes).reshape(1,num_nodes)\n",
    "        \n",
    "        char_id = sample_prob(random_dist()) #create a random distribution then sample\n",
    "        val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict = {val_input: val_input_O, \n",
    "                         val_output: val_output_O}\n",
    "            val_output_O,dist = sess.run([val_hidden_after,val_probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "            s+=id2char(char_id)\n",
    "        print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to no ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-275-5297ec456170>:94 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "num_nodes = 100\n",
    "num_unrollings = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #input fed into the cell, could be a batch of training data or a single one-hot encoded vector\n",
    "    train = list()\n",
    "    for i in range(num_unrollings):\n",
    "        train.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "    \n",
    "    #the previous output the gets fed into the cell\n",
    "    output_feed= tf.placeholder(tf.float32,shape=(batch_size,num_nodes),name='one')\n",
    "    \n",
    "    #one-hot encoded labels for training\n",
    "    labels = list()\n",
    "    for i in range(num_unrollings):\n",
    "        labels.append(tf.placeholder(tf.float32,shape=(batch_size,alphabet_size)))\n",
    "        \n",
    "    \n",
    "    #validation place holder\n",
    "    val_input = tf.placeholder(tf.float32,shape=(1,alphabet_size))\n",
    "    val_output = tf.placeholder(tf.float32,shape=(1,num_nodes))\n",
    "    \n",
    "    \n",
    "    #Variables\n",
    "    #input matrix\n",
    "    U = tf.Variable(tf.truncated_normal([alphabet_size,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #recursive matrix multiplies previous output\n",
    "    W = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    \n",
    "    #bias vector\n",
    "    b = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #output matrix\n",
    "    V = tf.Variable(tf.truncated_normal([num_nodes,alphabet_size],-0.1,0.1))\n",
    "    c = tf.Variable(tf.zeros([1,alphabet_size]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def RNN(i,h_input):\n",
    "        a = tf.matmul(i,U)+tf.matmul(h_input,W) #+b\n",
    "        h_output = tf.nn.tanh(a)\n",
    "        o_out = tf.matmul(h_output,V)+c\n",
    "        return h_output,o_out\n",
    "    \n",
    "    \n",
    "    #when training truncate the gradients\n",
    "    for i in range(num_unrollings):\n",
    "        if i == 0:\n",
    "            outputs = list()\n",
    "            hidden_after,output_after = RNN(train[i],output_feed)\n",
    "        else:\n",
    "            hidden_after,output_after = RNN(train[i],hidden)\n",
    "        hidden = hidden_after\n",
    "        outputs.append(output_after)\n",
    "    \n",
    "    #train\n",
    "    #loss\n",
    "    #should be doing log likelihood\n",
    "    def neg_log_likelihood(outputs,labels):\n",
    "        outputs = tf.concat(0,outputs)\n",
    "        labels = tf.concat(0,labels)\n",
    "        #print tf.reduce_sum(-tf.log(tf.reduce_sum(tf.nn.softmax(outputs)*labels,1)))\n",
    "        return tf.reduce_sum(tf.minimum(-tf.log(tf.reduce_sum(tf.multiply(tf.nn.softmax(outputs),labels),1)),10))\n",
    "        #return tf.reduce_sum(-tf.log(tf.nn.softmax(outputs)*labels))\n",
    "    \n",
    "#     loss = neg_log_likelihood(outputs,labels)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.concat(0,outputs),tf.concat(0,labels)))\n",
    "#     logits = tf.matmul(tf.concat(0,outputs),W_softmax)+b_softmax\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,tf.concat(0,labels)))\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "#     opt = tf.train.GradientDescentOptimizer(learning_rate=.001).minimize(loss)\n",
    "#     opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate=1.0,global_step=global_step, decay_steps=15000, decay_rate=0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients,var=zip(*optimizer.compute_gradients(loss))\n",
    "    gradients_clipped, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    opt=optimizer.apply_gradients(zip(gradients_clipped,var),global_step=global_step)\n",
    "    \n",
    "    # Validation\n",
    "    val_hidden_after,val_output_after = RNN(val_input,val_output) #change train to input_in\n",
    "    #val_logits = tf.matmul(val_output_after,W_softmax)+b_softmax\n",
    "    val_probs = tf.nn.softmax(val_output_after)\n",
    "    \n",
    "    #add init op to the graph\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss:  0.00356589007378\n",
      "Learning rate:  1.0\n",
      "jcfxfmfwfxfwfgfgfdfefafkfufwoxfsfcfnfwzgfbfgqgfhovfnfkfxfwfuqwfifsfgfdfdfdfwfifpfgfefwfvfcfg gfifafvf\n",
      "Average loss:  4.70367989612\n",
      "Learning rate:  1.0\n",
      "koocorotocouocotojotouohoyopogocoholowodotososelolozolosolotopocolotowosoyomojoxowoeoeototocosottsowo\n",
      "Average loss:  4.70831615448\n",
      "Learning rate:  1.0\n",
      "dloposohososodoaolotosososoioeosouoeosouodohocoiohosotocososmso okotogosdgomotojopccouobosolosofoloio\n",
      "Average loss:  4.70186961913\n",
      "Learning rate:  1.0\n",
      "xkenecepelehelenegeieheaexeleaenehehezegeheueceoeqeneaemeueheaeaehelehenemenehevepeaelefeaeoeueceaese\n",
      "Average loss:  4.70577140737\n",
      "Learning rate:  1.0\n",
      "oged aeh qeseyeaetemeuedegeoecetehecemehebeyeanaeaexeaetemetwgeuebeaecewenegenefeaeteaemeyeteaeg aece\n",
      "Average loss:  4.70554206085\n",
      "Learning rate:  1.0\n",
      "hn tststzldtytmoklhtrtutyigtltldstrtctot tstvtmtatnthdrtitatffjthontcbrtirst tuggt l tatrtftltatgtft \n",
      "Average loss:  4.70546630001\n",
      "Learning rate:  1.0\n",
      "pgs y z d h zli y r r p dsp r u rgb r a r f h c d o z e l e r c d e o g a p e s k s r d r i r l s evw\n",
      "Average loss:  4.70901691365\n",
      "Learning rate:  1.0\n",
      "itptatototrtztgtrtototptototdtititktctntwtutdtftctryntftititmtitbtdtgtitutftntitvtstmtatrtptitptltutd\n",
      "Average loss:  4.70472347522\n",
      "Learning rate:  1.0\n",
      "yze o r c u e d e p o l o b d ushyc m d l h h b z l h i rpc p i o i l s h o f p mve y u s e o t o s m\n",
      "Average loss:  4.7004353385\n",
      "Learning rate:  1.0\n",
      "whhbhctele siebeleiepeteseaeneueaetedefedbwetwyeunfbhugwteoeieteiltereiglebgiootteilcepecemeketelfweo\n",
      "Average loss:  4.69929694939\n",
      "Learning rate:  1.0\n",
      "aicioiaioihiupaioiddoioimznidioiocoidhmioivilicioioibidifauisiuiofoioiyioikioioiaifisuoioilicioi ivin\n",
      "Average loss:  4.71062840533\n",
      "Learning rate:  1.0\n",
      "glhiririmihipiricicihirilieici iaiti iti i ihifihiliuiridicikicitihi iyiritihiyihiliricixitizili itie\n",
      "Average loss:  4.70040961146\n",
      "Learning rate:  1.0\n",
      "nelnmnxnsnhtmntnbnfhsypnunxneninhnensgoncnonhninuninhn nmnwndnoncn ninfngnhnpnsninentnlnfnonsnenhninc\n",
      "Average loss:  4.70297340298\n",
      "Learning rate:  1.0\n",
      "imvg r t a a g n alt r o idp a i n leier i r ulg iea u r pdd o i nxl n irhwa i iynyi aui ate n r itl \n",
      "Average loss:  4.71216287708\n",
      "Learning rate:  1.0\n",
      "dyigiyisisizihigihiciyiginiuieiei ivihi ieiaiviligili ipili iwi i i i irinieiyiwiyiuilidisi i i iwi i\n",
      "Average loss:  4.7113215189\n",
      "Learning rate:  0.1\n",
      "nsn nininiwininrwinlnpninpnhngnmnrncnuombunrndn nmninvntndn nmnmnwnancnyncntninpnininunwnanvnrnunrnan\n",
      "Average loss:  2.9261135602\n",
      "Learning rate:  0.1\n",
      "csramfntraesw u g bza r htltyee iu   yc lth f m s ak  lol pmh e omrotahghi  v ld ffogf  ytiedaceeer  \n",
      "Average loss:  2.86921998835\n",
      "Learning rate:  0.1\n",
      "o la eynenniidg sa tyhegsfikksmc  e pboastrlseoosetluv e  flee  nnlosiatv  r uti yimthed  r d ugnihri\n",
      "Average loss:  2.86650075269\n",
      "Learning rate:  0.1\n",
      "stonieto eseelsdooenat   iee wccomiaert ln sprx w entic  eecredirptzntay he nrssfdtg eeetiredrh hosao\n",
      "Average loss:  2.86527742338\n",
      "Learning rate:  0.1\n",
      "l eee pt rvlenih nhrdreseneoi    elhitaiiieitmri tcbs o   oaabu ean escou oznrstprfheiier sbnme pyoft\n",
      "Average loss:  2.86759026957\n",
      "Learning rate:  0.1\n",
      "vazcanb tu  odti o dctsp  hckry uyh coheyaed aatetstardu ohyrnii eatmnh  fsa n tnfnc rotnhpuoraai cb \n",
      "Average loss:  2.86547036028\n",
      "Learning rate:  0.1\n",
      "anknh  clrhccohheita sftkd iaffsiheleni so eoem i alsdatndrqnufv soxranccau ec oeiua ctrttnnstoosthnd\n",
      "Average loss:  2.86439401245\n",
      "Learning rate:  0.1\n",
      "oicnonmi iovtxbowtfecvp ntfetnieitna f rce tdergsw ed xhhrtaid arseouoel ide ndftcoflheasloichnf mcan\n",
      "Average loss:  2.86715378904\n",
      "Learning rate:  0.1\n",
      "yctsbp tu ph nhh eeo ciec icfthhnntvpeopsirp gauae  deiaftfretimfm hsi d p  ptme slpdspnoaay ipesiaas\n",
      "Average loss:  2.86556164122\n",
      "Learning rate:  0.1\n",
      "pn eeaseuel  dii hlicoiraigtit rieaploeakaurmntnbna nhahn scbatcdyr s utiyiirwwehr iaiwntdnge en dclb\n",
      "Average loss:  2.86616999698\n",
      "Learning rate:  0.1\n",
      "srtan lnrbsate itnlei wuti rasf oeiresno nezud lsjheapw  aoomls aegat edzlsar oa daft d pzeytnr r  er\n",
      "Average loss:  2.86645516014\n",
      "Learning rate:  0.1\n",
      " ineeumaauah ye i rfll thatlaeotucwyvcionsetiyv ri onche  tss  rers tt hsomoolert  ro slahbpenr ct ms\n",
      "Average loss:  2.86642667556\n",
      "Learning rate:  0.1\n",
      "fmispi iit aftroet rwliaop eduee ef oera eeai  irt ine   ua   eael asicdae el rinifgnhimc  arehen zsp\n",
      "Average loss:  2.86440306592\n",
      "Learning rate:  0.1\n",
      "gepus ch  ee nrheoenl htonozetfeg tnp asa tn  sfa l s net  p ace nce yteaot tm hgre uslsdninaf rl ice\n",
      "Average loss:  2.86367913413\n",
      "Learning rate:  0.1\n",
      "ts cweeakmnevjcslw evusird s   tebstgeiuindhsooieeuaa oioohitenpeats rkss cfae edispaieiislt na l ei \n",
      "Average loss:  2.8650333643\n",
      "Learning rate:  0.01\n",
      "naccmef taie iyd ttvulehvdolo afeemnensve eeado l v toeocne cnaol eosfe m daoaerujcnoes ngwpmariocrrd\n",
      "Average loss:  2.85546962905\n",
      "Learning rate:  0.01\n",
      "z i  p dhner np nm ndiehlhnen sietwi le ac tuate  hrtcri  ta eiemtacl p hgrpt es iogyf esan nrzemyree\n",
      "Average loss:  2.85539816308\n",
      "Learning rate:  0.01\n",
      "peacevpmtnoedest thozn epentnnyiwriguodecteormwihutasia   ldalcvfftoaeyemezhm nepvtmingn oahpndrltsa \n",
      "Average loss:  2.85465233278\n",
      "Learning rate:  0.01\n",
      "y tecmyed oaac vzjon omrmev aiabd iosuifn aniexaci tett  msai n oreghelivdao cysoe oatay  mtratanav  \n",
      "Average loss:  2.85260845327\n",
      "Learning rate:  0.01\n",
      "bcee aeaniage asshfnp eoelisnl ic   uumet aiteet bsearsqncrroelsnr e t  drnl tnesyemer ciocv huhiaair\n",
      "Average loss:  2.85721048474\n",
      "Learning rate:  0.01\n",
      "ke ebeo cawn ale  lul rkw n  rrpdtwe ean in boriran bsan omsecv iofunhrnrblihesehauraennachee w pprdm\n",
      "Average loss:  2.85720492125\n",
      "Learning rate:  0.01\n",
      "ires a niiarsa  emrnath  iy wrtlrhmw odiaeio f a gr pi foto  pleltast t  oae u no  smae ni thgen  d e\n",
      "Average loss:  2.85756773686\n",
      "Learning rate:  0.01\n",
      "wifrn ter  vneat oe o dnfttsartsc e  oasuupo lbib ssbsadr iiebiorrrli t  srespre fhtsenr  ot iaheaqm \n",
      "Average loss:  2.85643045425\n",
      "Learning rate:  0.01\n",
      "xeoweeetdtersrnthaachq ooemn ygrnosca fotaceoryrn oei ato tt  t t eugte  denxecbehtd hsgeyfolcvrotixi\n",
      "Average loss:  2.85838577724\n",
      "Learning rate:  0.01\n",
      "p d rksrnng b wrnod shlphpndocxgcwg os neaeeracds u  ofn elmtenltltpbrheetg keedmog  rfprt  rh auwwl \n",
      "Average loss:  2.85514129329\n",
      "Learning rate:  0.01\n",
      "rcan  plcotdz eocc me lotoan  mkseeoenh   etooietreneenhdocne  ossiuepce r stichutattycald raiilrah d\n",
      "Average loss:  2.85893431783\n",
      "Learning rate:  0.01\n",
      "psaa acn vts oe fvleooidii efpbs edale el e gi c hbphnhctaae c ei jf islahatohytruc  mrm nent dein tr\n",
      "Average loss:  2.85858218718\n",
      "Learning rate:  0.01\n",
      "hchofhtg enhv sdev oecaalomntt ietriqolin   mao bhd gsien   niic dtlyuh  yu atsasa b l eiwtay llhtshk\n",
      "Average loss:  2.85657014394\n",
      "Learning rate:  0.01\n",
      "onb enit ionte iwtu  ypdeet  ea rsmrr go eseweiuohre it mobcemhrgtav itelcogr nnnia osctv t ai aroker\n",
      "Average loss:  2.85688234091\n",
      "Learning rate:  0.01\n",
      "ee notdfuaeeuxem piohed mooeveemc  tael   trouat  areateeer ird o tr   woore h gg netm oameuod lg e a\n",
      "Average loss:  2.85647571611\n",
      "Learning rate:  0.001\n",
      "idgau   hisdd  tfaioslg mw iainde cpslellfa hntnna nl   euie ejcipuod e oonteteeto n ebtrf rr vtni  n\n",
      "Average loss:  2.85614240909\n",
      "Learning rate:  0.001\n",
      "se   luidnift  ynagre  u lsmugmei otreyesehontrheoe eugliost s ospc egoebi eoynnoeimdaeni lwvswtri n \n",
      "Average loss:  2.85635402584\n",
      "Learning rate:  0.001\n",
      "ldisha t dmi arlp ier auvc n aeihhineen edaeioimpevaceidt wfngeb  boeeopeondeoiremmyrcrsfon attr w oc\n",
      "Average loss:  2.85676931572\n",
      "Learning rate:  0.001\n",
      "ntiwbwarr esu   iscociiacuonaial ae iel  kawiuflrsee rhafhsr ttswagblaarrvns  ecteiel s  nini cyrna m\n",
      "Average loss:  2.857841259\n",
      "Learning rate:  0.001\n",
      "kecp aainei s solohnoreirawdtnodrhfuo rhtitndc ayras   ol harvtdr eawdgyc tnegearo  uretloeuairnielas\n",
      "Average loss:  2.86106775117\n",
      "Learning rate:  0.001\n",
      "xtgaklie zrssna ste  as aticeoomit dh ah f reeiat tehuoise unctouessntniee saa r tneirkrufr ekrte cae\n"
     ]
    }
   ],
   "source": [
    "num_steps=50001\n",
    "batch_size=50\n",
    "b = Batch(text,batch_size)\n",
    "\n",
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    #get the new inputs and labels\n",
    "    batch_x,batch_y=getWindowBatch(b,num_unrollings)\n",
    "    \n",
    "#     if step%b.text_size == 0:\n",
    "#         print \"NEW EPOCH\"\n",
    "\n",
    "    #initialize the output\n",
    "    if step == 0: #initialize the output state vectors\n",
    "        output_pass = np.zeros([batch_size,num_nodes],dtype=np.float32)\n",
    "    feed_dict={output_feed: output_pass}\n",
    "    \n",
    "    #trains x\n",
    "    mega_batch_x = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_x[n][ba])\n",
    "        mega_batch_x.append(batch)\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[train[i]] = mega_batch_x[i]\n",
    "    \n",
    "    #trains y\n",
    "    mega_batch_y = [] #each elemnt will be a batch.  there will be n elements where n is the number of unrollings\n",
    "    for n in range(num_unrollings):\n",
    "        batch = np.ndarray((batch_size,alphabet_size)) #contain all the one-hot encoding of the characters\n",
    "        for ba in range(batch_size):\n",
    "            batch[ba]=char2vec(batch_y[n][ba])\n",
    "        mega_batch_y.append(batch)\n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[labels[i]] = mega_batch_y[i]\n",
    "    \n",
    "    output_pass,l,_=sess.run([hidden_after,loss,opt],feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 1000 == 0:\n",
    "        print 'Average loss: ',str(average_loss/1000)\n",
    "        average_loss = 0\n",
    "        \n",
    "        \n",
    "        print 'Learning rate: ', str(learning_rate.eval(session=sess))\n",
    "        #sample and then generate text\n",
    "        s=''\n",
    "        \n",
    "        #initialize the validations out, state, and character\n",
    "        val_output_O = np.zeros(num_nodes).reshape(1,num_nodes)\n",
    "        \n",
    "        char_id = sample_prob(random_dist()) #create a random distribution then sample\n",
    "        val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "\n",
    "        s+=id2char(char_id)\n",
    "        for _ in range(100):\n",
    "            feed_dict = {val_input: val_input_O, \n",
    "                         val_output: val_output_O}\n",
    "            val_output_O,dist = sess.run([val_hidden_after,val_probs],feed_dict=feed_dict)\n",
    "            char_id=sample_prob(dist[0])\n",
    "            val_input_O = vec2mat(char2vec(id2char(char_id)))\n",
    "            s+=id2char(char_id)\n",
    "        print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
